## Note
nid: 1629179682686
model: Basic-d7a3e
tags: 10_neural_networks
markdown: false

### Front
Give a definition of the <b>ReLU activation</b> function.

### Back
\(f(x)=\max (0, x)\)<div>
</div><div><img src="paste-52f6048ff6aa60354ea06922a0ea519729e66bbd.jpg">
<div>
</div><div>
</div></div>
