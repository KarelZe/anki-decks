## Note
nid: 1629195827467
model: Basic-d7a3e
tags: 10_neural_networks
markdown: false

### Front
Why are mini batches commonly used for gradient descent?

### Back
<div>
<div><ul>
<li>Mini batch is an intermediate version of stochastic and batch gradient descent</li>
<li>Gives less noisy estimates than stochastic gradient descent</li>
<li>More efficient than batch gradient descent</li>
<li>Preferable for GPU implementations</li>
</ul>
</div></div>
