{
    "__type__": "Deck",
    "children": [],
    "crowdanki_uuid": "14eb92d8-5f06-11ec-8e0f-cb9b2d7b92c1",
    "deck_config_uuid": "9b3009cf-5c21-11ec-8e7e-15cf58b20f1f",
    "deck_configurations": [
        {
            "__type__": "DeckConfig",
            "autoplay": true,
            "crowdanki_uuid": "9b3009cf-5c21-11ec-8e7e-15cf58b20f1f",
            "dyn": false,
            "interdayLearningMix": 0,
            "lapse": {
                "delays": [
                    10.0
                ],
                "leechAction": 1,
                "leechFails": 8,
                "minInt": 1,
                "mult": 0.0
            },
            "maxTaken": 60,
            "name": "Default",
            "new": {
                "bury": false,
                "delays": [
                    1.0,
                    10.0
                ],
                "initialFactor": 2500,
                "ints": [
                    1,
                    4,
                    0
                ],
                "order": 1,
                "perDay": 20
            },
            "newGatherPriority": 0,
            "newMix": 0,
            "newPerDayMinimum": 0,
            "newSortOrder": 0,
            "replayq": true,
            "rev": {
                "bury": false,
                "ease4": 1.3,
                "hardFactor": 1.2,
                "ivlFct": 1.0,
                "maxIvl": 36500,
                "perDay": 200
            },
            "reviewOrder": 0,
            "timer": 0
        }
    ],
    "desc": "",
    "dyn": 0,
    "extendNew": 0,
    "extendRev": 0,
    "media_files": [
        "112zztZseYCyAaRDGLfW.png",
        "121BJuB1U5Rr3kxCytJF.png",
        "126kztJPnHSRVJHmPTgW.png",
        "12DBdgEh2YzWxo7kiWr8.png",
        "12HXSSmLeVk4cvtpWDmv.png",
        "12MoDNYM9Q5xw6J9e1Pi.png",
        "12QJBBdwQesZH2dFmSZj.png",
        "12UNbg829E4AMsrifd22.png",
        "12Xy8cBT94UkXamrbgfh.png",
        "12YdUtaFq7aXifiMVDhb.png",
        "12ax1YiR86R9S275oioW.png",
        "12h2gEdoa2bG2JSfLgoP.png",
        "12oHRhRvhjDvvME2nhGB.png",
        "12owimmsTSoAU8u6a1ux.png",
        "12stsZR4gedZw7MfGRgz.png",
        "12tM8TE8zecMfzU5NQWF.png",
        "12vaXC2wWmZx5UBJL2mb.png",
        "12wEbPAvWMvR4QxaRYe8.png",
        "12wgBdw7BJtcqzLcw4vx.png",
        "15KyazAyakKyrocKqiPs.png",
        "18qn4BtFpqsFeKs8yJES.png",
        "19BXgeYgmC4DofcMbj4y.png",
        "1BJnf2FaxpjmTUHwv4oX.png",
        "1CVoqiVLK6idiaz6MDrP.png",
        "1FHPUtGrVP6fRmVHDn3A7Rw.png",
        "1JEJcBEUBcZ4UuwNAiJW.png",
        "1L7FTx9NcP9ATodQukyE.png",
        "1LEFTh6DNySf4khfAXLd.png",
        "1Mhk16usuKxW4ZD6Yoai.png",
        "1NzzqVpYwXJgiEYG46yC.png",
        "1QC3fD7UxQPSDdaRCRN6.png",
        "1SLbEBB1nftnQiWXph7u.png",
        "1T6zjvHinfrkW3UWr2wK.png",
        "1UpqzXQwkj9UxwvNJwpS.png",
        "1V6n5SdvMa7opP2FNFsb.png",
        "1WVxqxhuSnvQ5eHTUVzd.png",
        "1YsS6YcFep97q99Yjw1a.png",
        "1ZUYZsiN4VW6SEgTJ1YX.png",
        "1ePizQdkj8zdyerpkugL.png",
        "1fLjd2BUnBQdpwVMFghR.png",
        "1nKZJny8gHLUrpastXHy.png",
        "1p6XbyHBTKLTmGY18Fyk.png",
        "1sAFbpoMmj5GEfW6seHD.png",
        "1skohGEQx4R82tiGP3D3.png",
        "1uzLhQK96GY3apYKyMV8.png",
        "1x2JriRywDQMenpFzMvN.png",
        "1xh71nP4RmuG2nt9FvEX.png",
        "1yxWobtRk1AxCDps1beK.png",
        "1zeVvHW3XQHy8ELPRV9q.png",
        "28441879.png",
        "3017990105-68a50cb98b8cc286_fix732.png",
        "SGD.jpg",
        "XD2O4.png",
        "cart_classify_tree.JPG",
        "curseofdimensionality.png",
        "gru.png",
        "main-qimg-9e3419cfcd8535fb289bb1b710920d2f.png",
        "paste-00f20df6a86ed0946abbfe58c92c82872f849780.jpg",
        "paste-01d904854611ff65e1ec4aa224c552e774baaf73.jpg",
        "paste-0547953726d47f9767f7c17559c163c8bab8ee26.jpg",
        "paste-0794fed3b91b29540d61d65276d9b20b14458a0b.jpg",
        "paste-088e228eef7bcb8b8392a02cdd5c05b14be347ff.jpg",
        "paste-0a471cfeb5cd8be7cacb461aca4b6e016b03a8aa.jpg",
        "paste-0bb0d6856d32a0b2dcf81b4459ce1ed966334f08.jpg",
        "paste-1597c02f411761888d2c9db1a2990c40f01b3a8a.jpg",
        "paste-1769a1d2b3eb91d2bcaf68d843fe5208b1741ccd.jpg",
        "paste-19ffca0ed79c17d915f34cb0fc243b76ff585c9a.jpg",
        "paste-1cfbf98ff69c968908418a79f6c634639893036d.jpg",
        "paste-1d7f4a32b9f1db4e99019c65ca65b5b3a272eb44.jpg",
        "paste-1f08bdc45f900c733ad728cff19fb90a391f1e5f.jpg",
        "paste-221678bdd29f30a141e99f77ac12e3a84bbe54fe.jpg",
        "paste-246b22e3e7cfacb271b5b82ab2637864d058bdb4.jpg",
        "paste-270aa5690d9324d79fc0cc4a0355bfa77404bdac.jpg",
        "paste-285137c0cc0671bc438ec55a9c68b56c519173ba.jpg",
        "paste-29b2fd376c05db4436ec961dad39e9e19b59e447.jpg",
        "paste-2cb62211eede3fe03134d1b9fd9eabcc0a4d3938.jpg",
        "paste-2d8ea6871f44599f7199bc4da534d0e79241cdfd.jpg",
        "paste-2e717330e5c31f2144754ac4fbf890829f2f1ce3.jpg",
        "paste-2f55ffd1e6783f3cea8c652bd54e3e0fb7672e33.jpg",
        "paste-319cebfe22365c7b22818945d6b3cf790251a701.jpg",
        "paste-32f91c27bffa66c78bba9a3f73283b74dea8518d.jpg",
        "paste-34def274df302ae886b95f6bb9cf90cf9f317069.jpg",
        "paste-39842fc139de6141d2ecc902cc9326fa3e04220c.jpg",
        "paste-39b9827d44a2cbcf60ec14ec968f8946ac49b3fc.jpg",
        "paste-39cdad62a7dc76d4494ccd17fdd16fbc80476eeb.jpg",
        "paste-3ab198070b33cde8ab5967d6ae100f4a8379d4dc.jpg",
        "paste-435c27b7deceeb7bfcdffff568c470a041d52aa1.jpg",
        "paste-45943dd0e0e24e5cd3ceac41704eb7d264cb730b.jpg",
        "paste-466385bdbf325602a55808365f8f98792a218407.jpg",
        "paste-46b5fa791349cc46f838d19f8c38d546138eda84.jpg",
        "paste-4853cc17679d81491132eceda746d9c1e84eb049.jpg",
        "paste-4ae50fc23f62c713b16a5ccb3f0cafa27c17441d.jpg",
        "paste-4aea5717d369f2f35231f2e2d629b380297df76d.jpg",
        "paste-4e3cbac6da72820b542f141e4203608393035c4b.jpg",
        "paste-4e6a2de0523f3a0224bf4d08b835eec6926c1257.jpg",
        "paste-5010ad84a5a9e22bd18b0de803fb4412a7521bb6.jpg",
        "paste-52eafc401f607994446b9de11e9ab7b881b61801.jpg",
        "paste-52f6048ff6aa60354ea06922a0ea519729e66bbd.jpg",
        "paste-5b0cf9d2d13ce4b605af6ad161ad868e3e04a5c2.jpg",
        "paste-60d96d5d77835e58f4c539424ef47b3d256e5cda.jpg",
        "paste-6469b5aec1cc4804472264da56e4d27baf9d616e.jpg",
        "paste-66318042237c3a0b3c2267d485ee08f4c41db114.jpg",
        "paste-67c83c3991a1d93a7b0a6f96f28c216b133dd723.jpg",
        "paste-6e883ccb5efda948f8beac055d1221ac08ef10f4.jpg",
        "paste-6fd93cf457a811c4f11916b6f814266eb94ad44d.jpg",
        "paste-78a885aa7419fa81a808101ecd3d522d8cc921a1.jpg",
        "paste-7a18a60ed0a74c8b2ff0271621f6e62ba6061bb7.jpg",
        "paste-7e84c5cc800736ec791cd4676a6ae9c683419e87.jpg",
        "paste-8405ec9aba8b229a8d397ccc0a6285ad26ccd855.jpg",
        "paste-848510cf51999a08809cc308fab0cecaface5a47.jpg",
        "paste-8edcac52a727502b1537dcdec121493d93691d94.jpg",
        "paste-91e0d265856f3757aaf9d8d87f19b82bfb26fb42.jpg",
        "paste-92e1e1454b1da41205a4be67ece285003f386301.jpg",
        "paste-93eac9aaa150734c89f4411f959426685ac4911a.jpg",
        "paste-976fda0679c3ce7eff570a48ef8bde4ee4e178af.jpg",
        "paste-9e375eedd4394eb696c28905b4dc118751d18a7f.jpg",
        "paste-a4df8b815501d43b8d5d499f351ce6a6315c8989.jpg",
        "paste-a795048d2bef92bdbe249d3a19081382d6fa2a52.jpg",
        "paste-a847484cbd5bc6aedfd254238e395c7ac295386d.jpg",
        "paste-a8504c5f8f92e40fb4c84c8c02d1ce0584b95769.jpg",
        "paste-ace22c55511538845e267e883707b20a9bba3b4f.jpg",
        "paste-ace4f49525fc841b9e02ba6054f73313373b8ebf.jpg",
        "paste-b25c84dba80be779a94954f124f451382d05c4fd.jpg",
        "paste-b5b85fd9853b59c76953a26ca1100700dd7c43c3.jpg",
        "paste-b6595a9cffefe3291a8dfd031e14c93f7dccd13b.jpg",
        "paste-b93f9590b30e8fe0d52e35a06477d728e200df28.jpg",
        "paste-ba9ffad3dd33c9b1f3543ae75ddd2c8e7beae597.jpg",
        "paste-bc5d93bf53cbdc86d9f1454b6c296f79b8620cb3.jpg",
        "paste-beb37c5b09d86aaf7ccd4b921b5fc2319cf4da54.jpg",
        "paste-c0368e2f12364c4b2f4628f6c39483c989496ce5.jpg",
        "paste-c14aae6b9bd97fc81ea48fde4fc70b7e605c982d.jpg",
        "paste-c399ce6fe7b5c7a6a71cc35807bc955872ac6908.jpg",
        "paste-c4a07ca06c398d074be73efe9d7d884176769b03.jpg",
        "paste-c854a6ff664136ecc1cf68d6e93038d2bba4ee4c.jpg",
        "paste-c87c880af9b2c681ac553087d463600beaba4361.jpg",
        "paste-c8bee1571b288a2d7088ba6c2f7cafd453932458.jpg",
        "paste-c8ebb5806f43f725bd542b66b71cdcacf753d5b6.jpg",
        "paste-ca14b0f5025cead0f2a6e1404457c5832f33fa92.jpg",
        "paste-cae52d4bc93e36739c26476b7256984ff7e4c919.jpg",
        "paste-cb47415c1a6b36e4f26a7612dcb65b9ea4171540.jpg",
        "paste-cc573063e4172324c94f74334aa8b150d2e734cb.jpg",
        "paste-cd53484264b2c55c813ec0a0fc6c5d8030d6c586.jpg",
        "paste-d08f51da828bd4e295b9886a97a9be9599bf9ec5.jpg",
        "paste-daa333c2468ef7552b696b25c85305e1645f4503.jpg",
        "paste-dc0452c2c0468c3ed1ffd7bcbaa651da2a215f5a.jpg",
        "paste-dff10d7cc1878d3387d5b43c2dfd465f6f6f93df.jpg",
        "paste-e1dd8c4cb6b5298053e89ee19e0448f8fb3389fa.jpg",
        "paste-e2f42c11feb690d750e38e5a51e69153d11399d6.jpg",
        "paste-e86f1f621250a98764d596d3210e22fa319185cf.jpg",
        "paste-efadd0be79a8d3ba8a9aaf743727d97cd743dac6.jpg",
        "paste-efd5b89295384446131675ddee1c066c481369f3.jpg",
        "paste-f06366296c0279ade3fd9fc59d4ad9a419d17c28.jpg",
        "paste-f067be98ce82c7721b9ede76260c52388ff5515e.jpg",
        "paste-f27af5f1ab8a8555bd4c4b683aa575bb094e0533.jpg",
        "paste-f2d8b2017eb2b715e04c819da1590444c2a1f3e1.jpg",
        "paste-f4ea162621a06aece55728cbef89f943b1591026.jpg",
        "paste-f7a7e28c275857b6e61b09184aa1c2970a62ea9e.jpg",
        "paste-f9adfdfac33981ec2b25857eb9ddd712300f5a20.jpg",
        "paste-fc59ce0439c2e5dee9b265e7904b20e95f272a94.jpg"
    ],
    "name": "🧠Machine_Learning",
    "note_models": [
        {
            "__type__": "NoteModel",
            "crowdanki_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "css": ".card {\n font-family: 'Comic Sans MS';\n font-size: 24px;\n text-align: left;\n color: black;\n background-color: white;\n}\n\nu { font-family: courier new; text-decoration: none; }\n",
            "flds": [
                {
                    "font": "Arial",
                    "media": [],
                    "name": "Front",
                    "ord": 0,
                    "rtl": false,
                    "size": 20,
                    "sticky": false
                },
                {
                    "font": "Arial",
                    "media": [],
                    "name": "Back",
                    "ord": 1,
                    "rtl": false,
                    "size": 20,
                    "sticky": false
                }
            ],
            "latexPost": "\\end{document}",
            "latexPre": "\\documentclass[12pt]{article}\n\\special{papersize=3in,5in}\n\\usepackage{amssymb,amsmath}\n\\pagestyle{empty}\n\\setlength{\\parindent}{0in}\n\\begin{document}\n",
            "latexsvg": false,
            "name": "Basic-5bd3e",
            "req": [
                [
                    0,
                    "any",
                    [
                        0
                    ]
                ]
            ],
            "sortf": 0,
            "tags": [],
            "tmpls": [
                {
                    "afmt": "{{FrontSide}}\n\n<hr id=answer>\n\n{{Back}}",
                    "bafmt": "",
                    "bfont": "Arial",
                    "bqfmt": "",
                    "bsize": 12,
                    "did": null,
                    "name": "Card 1",
                    "ord": 0,
                    "qfmt": " {{Front}}"
                }
            ],
            "type": 0,
            "vers": []
        }
    ],
    "notes": [
        {
            "__type__": "Note",
            "fields": [
                "<p>\n\nWas ist <b>Maschinelles Lernen</b>?\n\n<br></p>",
                "<div>\n<div><ul>\n<li>Algorithmen, die ihre Performance verbesseren, indem sie Trainingsdaten verwenden.</li>\n<li>Haben allgemein eine große Zahl an Parameter</li>\n<li>Gelernt aus Daten</li>\n</ul>\n</div></div><div><div><ul>\n</ul>\n</div></div>"
            ],
            "guid": "NX~W1eM2J6",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "00_introduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>When is Machine Learning useful?</p>",
                "<div><div><div><div></div><div></div></div></div>\n</div><p>\n</p><div>\n<div><ul>\n<li><b>No expert knowledge available</b>: e. g. industrial / manufacturing control</li>\n<li><b>Black-box expert knowledge</b>: e. g. face / handwriting / speech recognition</li>\n<li><b>Fast changing phenomena</b>: e. g. credit scoring</li>\n<li><b>Customization / Personalization</b>: e. g. personalized news reader</li>\n</ul>\n</div></div>"
            ],
            "guid": "ntf*7Qx9U+",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "00_introduction",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Why is there such an <b>explosion</b> in Machine Learning right now?</p>",
                "<div><div><div><div></div><div></div></div></div>\n</div><p>\n</p><div>\n<div><ul>\n<li>More training data available</li>\n<li>More computation power available e. g. GPUs</li>\n<li>New algorithms e. g. Deep Learning</li>\n</ul>\n</div></div>"
            ],
            "guid": "cU&zj(ADp2",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "00_introduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Which three types of Learning does one differentiate?",
                "<div><div><div><div></div><div></div></div></div>\n</div><p>\n</p><div>\n<div><ol>\n<li><b>Supervised Learning: </b>Training data includes target values</li>\n<li><b>Unsupervised Learning:</b> Training data does not include target values.&nbsp;</li>\n<li><b>Reinforcement Learning:</b> No target values, but evaluation (reward) of the output</li>\n</ol>\n</div></div>"
            ],
            "guid": "NM~&T|i1|0",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "00_introduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Name two examples for <b>Supervised Learning</b>.</p>",
                "<p><img src=\"1sAFbpoMmj5GEfW6seHD.png\" style=\"width: 366px;\"><br></p>"
            ],
            "guid": "zeGqX(QapY",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "00_introduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Name two examples for <b>Unsupervised Learning.</b></p>",
                "<p><img src=\"12QJBBdwQesZH2dFmSZj.png\" style=\"width: 366px;\"></p>"
            ],
            "guid": "F&q~NT6f}W",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "00_introduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Out of which parts (3 p) does a ML algorithm consist?</p>",
                "<p>1. <b>Representation: </b><i>How does the model look like?</i> e. g. Decision Trees, Neural Networks.</p><p>2. <b>Evaluation: </b><i>What are we optimizing for?</i> e. g. Accuracy, Squared error, Precision and recall</p><p>3. <b>Optimization:</b> <i>How do we optimize?</i> e. g. Least square solution, Gradient descent, random search</p>"
            ],
            "guid": "uWPt;qA|tX",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "00_introduction",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was versteht man unter einer <b>random variable</b> im Kontext von <b>Wahrscheinlichkeiten</b>?</p>",
                "<p>A <b>random variable</b>&nbsp;\\(X\\)&nbsp;repräsentiert ungewisse Zustände oder Ergebnisse der Welt.</p><p>Man schreibt \\(p(x)\\)&nbsp;und meint damit die Wahrscheinlichkeit mit der \\(X\\) den Wert \\(x\\) annimmt.</p>"
            ],
            "guid": "v3-=9,D{%n",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>\n\nWas versteht man unter \\(p(x)\\)&nbsp;bei der <b>Wahrscheinlichkeitsrechnung</b>?<br></p>",
                "<p>\\(p(x)\\) ist die <b>probability mass (density) function</b></p><div>\n<div><ul>\n<li>Weißt jeder Zahl einen Wert im <b>Ereignisraum </b>(sample space) zu</li>\n<li>Zwischen 0 und 1</li>\n<li>z. B. Wie oft tritt \\(x\\) auf?</li>\n</ul>\n</div></div>"
            ],
            "guid": "kKgjv*4,yR",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was unterscheidet <b>joint distributions</b> von <b>conditionial distributions</b> in der Wahrscheinlichkeitsrechnung?</p>",
                "<p><img src=\"18qn4BtFpqsFeKs8yJES.png\" style=\"width: 366px;\"><br></p><p><img src=\"12MoDNYM9Q5xw6J9e1Pi.png\" style=\"width: 287px;\"><br></p><p><img src=\"12stsZR4gedZw7MfGRgz.png\" style=\"width: 366px;\"><br></p><p><img src=\"1UpqzXQwkj9UxwvNJwpS.png\" style=\"width: 325px;\"><br></p>"
            ],
            "guid": "y,r^*G^0n6",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie lautet die <b>Kettenregel</b>&nbsp;<i>(chain rule)</i> in der <b>Wahrscheinlichkeitsrechnung</b>?</p>",
                "<p>\\(p(x, y)=p(x \\mid y) p(y)\\)<br></p><p>\\(p\\left(x_{1}, \\cdots, x_{D}\\right)=p\\left(x_{1}\\right) p\\left(x_{2} \\mid x_{1}\\right) \\ldots p\\left(x_{D} \\mid x_{1}, \\ldots, x_{D-1}\\right)\\)</p>"
            ],
            "guid": "C6r_;Y$li#",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was besagt der <b>Satz von Bayes </b>(<i>Bayes rule</i>) und warum ist er so wichtig?</p>",
                "<p>\\[p(x \\mid y)=\\frac{p(y \\mid x) p(x)}{p(y)}=\\frac{p(y \\mid x) p(x)}{\\sum_{x^{\\prime}} p(y \\mid x) p(x)}\\]<br></p><p>Einfacher weg um bedingte Wahrscheinlichkeiten \"umzudrehen\". Nützlich, weil häufig eine einfach zu berechnen ist, nicht aber die andere.</p>"
            ],
            "guid": "Plz,FG>}hW",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie lässt sich der Erwartungswert \\(\\mathbb{E}_p\\) einer Funktion \\(f(x)\\) berechnen?</p>",
                "<p>Der Erwartungswert einer Funktion \\(f(x)\\) mit Berücksichtigung der Verteilung \\(p(x)\\) ist gegeben durch:<br></p><p>\\(\\mathbb{E}_{p}[f(x)]=\\int p(x) f(x) d x\\)<br></p><p>Ein bedingter Erwartungswert ist gegeben durch:</p><p>\\(\\mathbb{E}_{p}[f(x) \\mid Y=y]=\\int p(x \\mid y) f(x) d x\\)<br></p><p>Kettenregel für Erwartungswerte:</p><p>\\(\\mathbb{E}_{p}[f(x)]=\\int p(y) \\mathbb{E}[f(x) \\mid Y=y] d y\\)<br></p>"
            ],
            "guid": "Dg@#!:K~Br",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was ist die zentrale Idee der <b>Monte-Carlo Schätzung</b>?</p>",
                "<p>\\[\\mathbb{E}_{p}[f(x)]=\\int p(x) f(x) d x \\approx \\frac{1}{N} \\sum_{x_{i} \\sim p(x)} f\\left(x_{i}\\right)\\]<br></p><p>Notwendig, wenn keine analytische Lösung existiert, um das Integral zu lösen.</p>"
            ],
            "guid": "Q/Gn>PKc%$",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was ist die&nbsp;<b>Bernoulli Verteilung</b>?</p>",
                "<p>Binary random variable \\(X \\in\\{0,1\\}\\)</p><p>One parameter \\(p(X=1)=\\mu\\)</p><p>Probability distribution \\(p(x)=\\mu^{x}(1-\\mu)^{(1-x)}\\)</p><p><b>Intuition:</b></p><p>\\(X\\)<span>&nbsp;ist entweder 1 oder 0. Mathematische Art eine if-Clause zuschreiben. Anders als if-Clause kann man diesen Ausdruck differenzieren.\n\n</span><br></p>"
            ],
            "guid": "GSjKdbG`K?",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was ist die <b>Multinomial-Verteilung</b>?</p>",
                "<p>\\(K\\) different events: \\(\\quad C \\in\\{1, \\ldots, K\\}\\).<br></p><p>Directly specifies probabilities: \\(p(C=k)=\\mu_{k}, \\quad \\mu_{k} \\geq 0, \\quad \\sum_{k=1}^{K} \\mu_{k}=1\\).<br></p><p>Or written with 1-hot-encoding (without an \"if\" clause)<br>\\[<br>p(c)=\\prod_{k=1}^{K} \\mu_{k}^{h_{c, k}}<br>\\]<br><br></p><p>where \\(h_{x}\\) is a mask or a \\(K\\)-dimensional 1-hot encoding vector e.g. \\([0,0,1,0, \\ldots]\\).<br></p><p><span>\n\n\n\n</span><span>\n\n\n\n</span></p>"
            ],
            "guid": "boS/]^eZKW",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie ist die <b>Gaußsche Normalverteilung</b> definiert? Wodurch ist sie <b>vollständig parametrisiert</b>?</p>",
                "<p>\\[p(x)=\\mathcal{N}(x \\mid \\mu, \\sigma)=\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} \\exp \\left\\{-\\frac{(x-\\mu)^{2}}{2 \\sigma^{2}}\\right\\}\\]<br></p><p>Durch Erwartungswert \\(\\mu\\) und Varianz \\(\\sigma^2\\)</p>"
            ],
            "guid": "6fr#stQ%X",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification",
                "formula_sheet"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie ist eine <b>Multivariate Gaußverteilung</b> definiert?</p>",
                "<p>\\[p(\\boldsymbol{x})=\\mathcal{N}(\\boldsymbol{x} \\mid \\boldsymbol{\\mu}, \\mathbf{\\Sigma})=\\frac{1}{\\sqrt{\\mid 2 \\pi \\mathbf{\\Sigma}} \\mid} \\exp \\left\\{-\\frac{(\\boldsymbol{x}-\\boldsymbol{\\mu})^{T} \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu})}{2}\\right\\},\\]<br></p><p>wobei \\(\\boldsymbol{\\mu}\\) der Mittelwert-Vektor ist und \\(\\boldsymbol{\\Sigma}\\) die Kovarianzmatrix.</p>"
            ],
            "guid": "j*A:65cXiZ",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification",
                "formula_sheet"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was sind wichtige <b>Eigenschaften</b> einer <b>Gauss-Verteilung</b>?</p>",
                "<div><div><div><div></div><div></div></div></div>\n</div><p>\n</p><div>\n<div><ul>\n<li>Die Summe zweiter Gauß-Normal-Verteilungen ist eine Gauß-Verteilung</li>\n<li>Das Produkt zweiter Gaus-Normal-Verteilungen ist wieder eine Gauß-Verteilung.</li><li>Jede bedingte Gauß-Normal-Verteilung ist wieder eine Gaußverteilung</li><li>Die Summe zweier normalverteilter Zufallsvariablen ist wieder normalverteilt</li>\n</ul>\n</div></div>"
            ],
            "guid": "tl|a,KY=T(",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was ist die <b>zentrale Idee</b> von <b>Regression</b>?</p>",
                "<p>Eine stetige Funktion erlernen:&nbsp;</p><p>\\(y=f(x)+\\epsilon\\)<br></p><p>Epsilon ist dabei ein Fehlerterm, von dem ausgegangen wird, dass er <b>normalverteilt </b>ist.</p><p><b>Skizze</b></p><p><img src=\"1NzzqVpYwXJgiEYG46yC.png\" style=\"width: 366px;\"><br></p><p><br></p>"
            ],
            "guid": "yDmElsXZHp",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "01_linear_regression"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was ist die zentrale Idee von <b>linearer Regression</b>?</p>",
                "<p>- Man gleicht <i>(fit)</i> eine Gerade <i>(line)</i> an:</p><p>\\(y=f(x)+\\epsilon=w_{0}+w_{1} x+\\epsilon\\)<br></p><p>Man geht davon aus, dass der Fehlerterm Epsilon normalverteilt ist.</p><p><b>Skizze:</b></p><p><img src=\"12Xy8cBT94UkXamrbgfh.png\" style=\"width: 366px;\"><br></p><p><br></p>"
            ],
            "guid": "nBQVVhEz,a",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "00_introduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Nach was wird bei der Regression hin optimiert?</p>",
                "Minimieren des <b>Summed (oder mean) Squared Error</b>"
            ],
            "guid": "Dq$-e=-qFh",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "01_linear_regression"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was sind wichtige Eigenschaften des Summed Squared Error?</p>",
                "\n<div><ul>\n<li>vollständig differenzierbar</li>\n<li>einfach zu optimieren (1. Ableitung &amp; gleich 0 setzen)</li>\n<li>Mathematisch sinnvoll:\\(f^{*}(\\boldsymbol{x})=\\operatorname{argmin}_{f(\\boldsymbol{x})} \\mathrm{SSE} \\Rightarrow f^{*}(\\boldsymbol{x})=\\mathbb{E}[y \\mid \\boldsymbol{x}]\\)</li></ul></div>"
            ],
            "guid": "xA(:?iGQ8",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "01_linear_regression"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie ist der <b>Summed Squared Error</b> definiert?</p>",
                "<p>\\[\\operatorname{SSE}=\\sum_{i=1}^{N}\\left(y_{i}-f\\left(\\boldsymbol{x}_{i}\\right)\\right)^{2}\\]<br></p><p>\\(x\\) ist dabei der Inputvektor und \\(y\\)&nbsp;der Outputvektor.<br></p><p>Allgemeiner Fall mit <b>mehreren Inputs</b>:</p><p>\\[\\operatorname{SSE}=\\sum_{i=1}^{N}\\left(y_{i}-\\left(w_{0}+\\sum_{j} w_{j} x_{i, j}\\right)\\right)^{2}\\]</p>"
            ],
            "guid": "gbRpSpeEsb",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "01_linear_regression",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Lineare Regression modelliert den Output \\(y\\) als lineare Funktion zwischen Input \\(x_i\\)<b>&nbsp;</b>folgendermaßen:</p><p>\\[y=f(x)+\\epsilon=w_{0}+w_{1} x+\\epsilon\\]<br></p><p>Was ist dabei die Bedeutung von \\(w_0\\) und von \\(w_1\\)? Wie wirken sich beide geometrisch aus?</p>",
                "<p>\\(w_0\\) verschiebt Gerade entlang der y-Achse</p><p>\\(w_1\\) verändert Steigung der Geraden</p>"
            ],
            "guid": "GLx}7=@9.2",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "01_linear_regression"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie lautet die<b> lineare Funktion</b> für <b>mehrere Inputs</b>?</p>",
                "<p>\\[f\\left(\\boldsymbol{x}_{i}\\right)=w_{0}+\\sum_{j} w_{j} x_{i, j}\\]<br></p><p>Das Fehlermaß lautet dann:</p><p>\\[\\operatorname{SSE}=\\sum_{i=1}^{N}\\left(y_{i}-\\left(w_{0}+\\sum_{j} w_{j} x_{i, j}\\right)\\right)^{2}\\]<br></p>"
            ],
            "guid": "BH*}@|I2-l",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "01_linear_regression",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie lassen sich <b>lineare Regressionsmodelle</b> in <b>Matrixform</b> darstellen?</p>",
                "<p><span>\n\n</span></p><p>\\[Y_{i}=\\beta_{0}+\\beta_{1} X_{i}+\\epsilon_{i} \\quad \\text { where } \\quad \\epsilon_{i} \\sim^{i i d} N\\left(0, \\sigma^{2}\\right)\\]<br></p><p>\\[\\begin{gathered}<br>{\\left[\\begin{array}{c}<br>Y_{1} \\\\<br>Y_{2} \\\\<br>\\vdots \\\\<br>Y_{n}<br>\\end{array}\\right]} &amp; =\\left[\\begin{array}{c}<br>\\beta_{0}+\\beta_{1} X_{1} \\\\<br>\\beta_{0}+\\beta_{1} X_{2} \\\\<br>\\vdots \\\\<br>\\beta_{0}+\\beta_{1} X_{n}<br>\\end{array}\\right]+\\left[\\begin{array}{c}<br>\\epsilon_{1} \\\\<br>\\epsilon_{2} \\\\<br>\\vdots \\\\<br>\\epsilon_{n}<br>\\end{array}\\right] \\\\<br>{\\left[\\begin{array}{c}<br>Y_{1} \\\\<br>Y_{2} \\\\<br>\\vdots \\\\<br>Y_{n}<br>\\end{array}\\right]} &amp; =\\left[\\begin{array}{cc}<br>1 &amp; X_{1} \\\\<br>1 &amp; X_{2} \\\\<br>\\vdots &amp; \\vdots \\\\<br>1 &amp; X_{n}<br>\\end{array}\\right]\\left[\\begin{array}{l}<br>\\beta_{0} \\\\<br>\\beta_{1}<br>\\end{array}\\right]+\\left[\\begin{array}{c}<br>\\epsilon_{1} \\\\<br>\\epsilon_{2} \\\\<br>\\vdots \\\\<br>\\epsilon_{n}<br>\\end{array}\\right]<br>\\end{gathered}\\]<br></p><p>Design-Matrix</p><p>\\[\\mathbf{X}_{n \\times 2}=\\left[\\begin{array}{cc}<br>1 &amp; X_{1} \\\\<br>1 &amp; X_{2} \\\\<br>\\vdots &amp; \\vdots \\\\<br>1 &amp; X_{n}<br>\\end{array}\\right]\\]<br></p><p>Vector of Parameters</p><p>\\[\\beta_{2 \\times 1}=\\left[\\begin{array}{l}<br>\\beta_{0} \\\\<br>\\beta_{1}<br>\\end{array}\\right]\\]<br></p><p>Vector of Error Terms</p><p>\\[\\epsilon_{n \\times 1}=\\left[\\begin{array}{c}<br>\\epsilon_{1} \\\\<br>\\epsilon_{2} \\\\<br>\\vdots \\\\<br>\\epsilon_{n}<br>\\end{array}\\right]\\]<br></p><p>Vector of Responses</p><p>\\[\\mathbf{Y}_{n \\times 1}=\\left[\\begin{array}{c}<br>Y_{1} \\\\<br>Y_{2} \\\\<br>\\vdots \\\\<br>Y_{n}<br>\\end{array}\\right]\\]<br></p><p>\\[\\begin{aligned}<br>\\mathbf{Y} &amp;=\\mathbf{X} \\beta+\\epsilon \\\\<br>\\mathbf{Y}_{n \\times 1} &amp;=\\mathbf{X}_{n \\times 2} \\beta_{2 \\times 1}+\\epsilon_{n \\times 1}<br>\\end{aligned}\\]</p>\n<br><p></p>"
            ],
            "guid": "QYo|**,X=L",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "01_linear_regression"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie lässt sich die&nbsp;<b>Sum of squared errors (SSE)</b> in Matrixform darstellen?</p>",
                "<p>\\[\\operatorname{SSE}=\\sum_{i}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}=\\sum_{i} e_{i}^{2}=e^{T} e=(\\boldsymbol{y}-\\boldsymbol{X} \\boldsymbol{w})^{T}(\\boldsymbol{y}-\\boldsymbol{X} \\boldsymbol{w})\\]<br></p><p>Alternative <b>visuelle Darstellung</b>:</p><p>\\[\\sum \\epsilon_{i}^{2}=\\left[\\epsilon_{1} \\epsilon_{2} \\cdots \\epsilon_{n}\\right]\\left[\\begin{array}{c}<br>\\epsilon_{1} \\\\<br>\\epsilon_{2} \\\\<br>\\vdots \\\\<br>\\epsilon_{n}<br>\\end{array}\\right]=\\epsilon^{\\prime} \\epsilon\\]</p>"
            ],
            "guid": "I.Li!c/[wF",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "01_linear_regression",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie findet man die <b>optimalen Gewichte</b> \\(w\\) bei Linearer Regression, welche den SSE minimieren?</p>",
                "Indem man die Ableitung bildet und die Nullstelle findet."
            ],
            "guid": "PybwSyprd$",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "01_linear_regression"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Wie lautet die Gleichung für optimalen Gewichte? (d. h. Ableitung des SSE + SSE = 0)",
                "<p>\\[\\boldsymbol{w}^{*}=\\left(\\boldsymbol{X}^{T} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{T} \\boldsymbol{y}\\]<br></p>"
            ],
            "guid": "g[pbB*{RS%",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "01_linear_regression",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie lassen sich Regressionsmodelle evaluieren? Warum ist dabei die <b>Sum of Squared Errors</b> eher ungeeignet?</p>",
                "<div>\n<div><ul>\n<li>SSE kann beliebig große Werte abhängig des Outputs annehmen.</li>\n<li>Auswertung soll unabhängig der Varianz von \\(y\\) sein.</li>\n</ul>\n</div></div>"
            ],
            "guid": "CZ=VQccL3!",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "01_linear_regression",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p><span>\n\nWelches Maß eignet sich zur Evaluation von Regressionmodellen?\n\n</span><br></p>",
                "<p>R - Square:</p><p>\\[R^{2}=1-\\frac{\\text { Regression sum of squares }}{\\text { Total sum of squares }}=1-\\frac{\\sum_{n=1}^{N}\\left(\\hat{y}_{n}-y_{n}\\right)^{2}}{\\sum_{n=1}^{N}\\left(y_{n}-\\bar{y}\\right)^{2}}\\]<br></p><p><b style=\"letter-spacing: 0.01071em;\">Interpretation:</b></p><p> wie viel der Variation in \\(y\\)&nbsp;durch \\(x\\)&nbsp;erklärt wird.</p><p><span>\n\n</span></p><p style=\"font-weight:400;letter-spacing:0.12852px;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\">\\(R^2 = 1\\) Bedeutet, dass gesamte Fehler erklärt wäre</p><p style=\"font-weight:400;letter-spacing:0.12852px;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\">\\(R^2 = 0\\) bedeutet, dass der Regressor schlecht funktioniert wird.</p><p></p>"
            ],
            "guid": "F-wcO5oev_",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "01_linear_regression",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wann heißt ein Datenset <b>linear separierbar</b>?</p>",
                "<p><img src=\"112zztZseYCyAaRDGLfW.png\" style=\"width: 366px;\"></p>"
            ],
            "guid": "sDr2zP-/5C",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was ist der <b>\"0-1-loss\"</b> in der <b>linearen Klassifikation</b> und wie lässt sich dieser <b>optimieren</b>?</p>",
                "<p><b>Prediction:</b>&nbsp;</p><p>\\(y=\\operatorname{step}(f(\\boldsymbol{x}))=\\operatorname{step}\\left(\\boldsymbol{w}^{T} \\boldsymbol{x}+b\\right)\\)<br>Predict class 1 for \\(f(x)&gt;0\\) else predict class 0<br><br><b>Optimization:</b>&nbsp;</p><p>Find \\(\\boldsymbol{w}\\) such that<br>\\[<br>L_{0}(\\boldsymbol{w})=\\sum_{i} \\mathbb{I}\\left(\\operatorname{step}\\left(\\boldsymbol{w}^{T} \\boldsymbol{x}+b\\right) \\neq y_{i}\\right)<br>\\]<br>where \\(\\mathbb{I}\\) returns 1 if the argument is true and \\(\\sum\\) counts the number of misclassifications<br></p>"
            ],
            "guid": "Oz0WAM}]A6",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was unterscheidet <b>Regressionsverfahren</b> von <b>Klassifikationsverfahren</b>?</p>",
                "<p><span>\n</span></p><div>\n<div><p><strong>Regression:</strong> Continuous output labels</p>\n<ul>\n<li>Linear regression, Polynomial Regression, kNN, Regression Trees, GaussianProcesses, NeuralNets</li>\n</ul>\n<p><strong>Classification:</strong> Discrete/ Nominal output labels</p>\n<ul>\n<li>Logistic Regression, DecisionTrees, NeuralNets, SVMs, kNN</li>\n</ul>\n</div>\n</div>\n\n<p></p>"
            ],
            "guid": "BeJ}D/2-2V",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was unterscheidet <b>true risk</b> von <b>empirical risk</b>?</p><p>Gehen Sie dabei auch auf Klassifikation und Regression ein.</p>",
                "<p><b>True risk: </b>performance on a random test point \\((x,y)\\)<br></p><div>\n<div><ul>\n<li>Classification: probability of misclassification&nbsp;\\(p(y \\neq f(x))\\)</li>\n<li>Regression: expected squared error&nbsp;\\(\\mathbb{E}_{\\boldsymbol{x}, y}\\left[(f(\\boldsymbol{x})-y)^{2}\\right]\\)</li><li><b>True risk is unknown!</b></li>\n</ul>\n</div></div><p><b>Empirical risk:</b> performance on the training set</p><div>\n<div><ul>\n<li>Classification: proportion of misclassified samples&nbsp;\\(\\frac{1}{n} \\sum_{i} \\mathbb{I}\\left(f\\left(\\boldsymbol{x}_{i}\\right) \\neq y_{i}\\right)\\)</li>\n<li>Regression: average squared error&nbsp;\\(\\frac{1}{n} \\sum_{i}\\left(f\\left(\\boldsymbol{x}_{i}\\right)-y_{i}\\right)^{2}\\)</li><li><b>Empirical risk can be evaluated!</b></li></ul></div></div>"
            ],
            "guid": "oh[d{Z{VV2",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "03_model_selection",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was versteht man unter <b>Overfitting</b> und <b>Underfitting</b> bei der Modellauswahl?</p>",
                "<p><span>\n<div>\n<div><p><strong>Overfitting</strong></p>\n<ul>\n<li>Man “fittet” noise in den Daten</li>\n<li>Kein spezfiziertes Verhalten zwischen Datenpunkten</li>\n<li>Nicht genug Daten</li>\n</ul>\n<p><strong>Underfitting</strong></p>\n<ul>\n<li>Man repräsentiert nicht die zugrundeliegende Funktion</li>\n</ul>\n</div>\n</div>\n\n</span></p>"
            ],
            "guid": "H&3}|`i3y5",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "03_model_selection"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie steht <b>Overfitting</b> / <b>Underfitting </b>mit <b>empiricial risk </b>und <b>true risk</b> in Verbindung?</p>",
                "<p><img src=\"1p6XbyHBTKLTmGY18Fyk.png\" style=\"width: 366px;\"></p>"
            ],
            "guid": "z1es3kZ*3+",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "03_model_selection"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Welche zwei Ansätze existieren, um ein Dataset in Trainings- und Testdaten aufzuspalten und dabei auf \"neuen\" Trainingsdaten zu trainieren?</p>",
                "<p><span>\n<div>\n<div><ul>\n<li>hold-out method</li>\n<li>cross-validation</li>\n</ul>\n</div>\n</div>\n\n</span></p>"
            ],
            "guid": "zB3mj:L;DG",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "03_model_selection"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie funktioniert die <b>Hold-out-Methode</b> im Kontext von Trainings- / Testdatensets?</p>",
                "\n<div><div>Hold-out procedure: \\(n\\) datapoints available \\(D=\\left\\{\\left(\\boldsymbol{x}_{i}, y_{i}\\right\\}_{i=1}^{n}\\right.\\)</div>\n<ol>\n<li>Split into 2 datasets:\\[D_{T}=\\left\\{\\left(\\boldsymbol{x}_{i}, y_{i}\\right\\}_{i=1}^{m} \\quad D_{V}=\\left\\{\\left(\\boldsymbol{x}_{i}, y_{i}\\right\\}_{i=m+1}^{n}\\right.\\right.\\]</li><li>Train on training data to obtain \\(\\hat{f}_{D{T}}(x)\\) for each model class \\(M\\)</li>\n<li>Evaluate resulting estimators on validation data, e.g: \\(\\operatorname{MSE}\\left(D_{V}, \\hat{f}_{D{T}}\\right)=\\frac{1}{n-m} \\sum_{i=m+1}{n}\\left(\\hat{f}_{D_{T}}\\left(\\boldsymbol{x}_{i}\\right)-y_{i}\\right)^{2}\\)</li>\n<li>Pick model with best validation loss</li>\n</ol>\n</div>"
            ],
            "guid": "lt2~7]F8uJ",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "03_model_selection",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was sind Nachteile der <b>Hold-out Methode</b> im Kontext von Trainings- und Testdaten?</p>",
                "<p><span>\n<div>\n<div><ul>\n<li>“teuer” im Sinne von Daten</li>\n<li>Unglückliche Verteilung der Daten kann zu irreführenden Ergebnissen führen</li>\n</ul>\n</div>\n</div>\n\n</span></p>"
            ],
            "guid": "tdigq7kyyA",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "03_model_selection"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie funktioniert \\(k\\)<b>-fold cross validation</b>?</p>",
                "<p><span>\n</span></p><div>\n<div><ol>\n<li>Partioniere das Datenset in \\(k\\) Partionen</li>\n<li>Schätze \\(k\\) hold-out Prädikator mit 1 Partition Validation und \\(k−1\\) Partitionen als Trainingsset</li>\n</ol>\n</div>\n</div>\n\n<img src=\"15KyazAyakKyrocKqiPs.png\" style=\"width: 347px;\"><br><p></p>"
            ],
            "guid": "J5-,-D+gXP",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "03_model_selection"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was funktioniert die <b>Leave-One-Out</b> <b>cross validation</b>?</p>",
                "<div>\n<div><ol>\n<li>Special case with \\(k=n\\)</li>\n<li>Consequently, estimate (\\(n\\)) hold-out predictors using 1 sample as validation and (\\(n-1)\\) samples as training set</li>\n</ol>\n</div></div><p><img src=\"paste-c87c880af9b2c681ac553087d463600beaba4361.jpg\"><br></p>"
            ],
            "guid": "Edr,gO%SP_",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "03_model_selection"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie funktioniert <b>cross validation</b> mittels <b>random sub-sampling</b>?</p>",
                "<ol>\n<li>Randomly sample a fraction of \\(\\alpha \\times n\\), with \\((0 \\leq \\alpha \\leq 1)\\) data points for validation.</li>\n<li>Train on remaining points and validate, repeat \\(K\\) times</li></ol><p><img src=\"paste-2f55ffd1e6783f3cea8c652bd54e3e0fb7672e33.jpg\"><br></p>"
            ],
            "guid": "x{ml]xB*nk",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "03_model_selection"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was sind gängige Techniken, um Overfitting zu vermeiden?</p>",
                "<p><span>\n<div>\n<div><ul>\n<li>Begrenze die Komplexität des Modells</li>\n<li>Regularisierung mittels Strafmaß</li>\n<li>“Early Stopping”</li>\n<li>Verstärkung von Rauschen in Daten</li>\n</ul>\n</div>\n</div>\n\n</span></p>"
            ],
            "guid": "QR4>>4kq6E",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "03_model_selection",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was besagt <b>Occam's Razor</b>?</p>",
                "Ziehe einfache Erklärungen Komplexeren vor."
            ],
            "guid": "Dc+{cy%cv{",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "03_model_selection"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie wirkt sich <b>Over- und Underfitting</b> auf <b>Test- und Trainingsfehler </b>aus?</p>",
                "<p><span>\n</span></p><div>\n<div><p><strong>Overfitting</strong></p>\n<ul>\n<li>Trainingfehler geht nach unten</li>\n<li>Validationfehler geht nach oben</li>\n</ul>\n<p><strong>Underfitting</strong></p>\n<ul>\n<li>Training- und Validationfehler sind hoch.</li></ul></div></div><p></p>"
            ],
            "guid": "D]8&oE)/&*",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "03_model_selection"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie funktioniert die <b>Regularization Penalty</b>?</p>",
                "<p>Man optimiert die Kostenfunktion erweitert um einen Strafterm (<i>regularization penalty</i>)</p><p>\\(\\underset{\\text { parameters } \\boldsymbol{\\theta}}{\\arg \\min } \\sum_{i=1}^{N} l\\left(\\boldsymbol{x}_{i}, \\boldsymbol{\\theta}\\right)+\\lambda \\text { penalty }(\\boldsymbol{\\theta})\\)<br></p>"
            ],
            "guid": "E%kGI#qIjp",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "03_model_selection",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was ist das Ziel der sogenannten <b>Regularization penalty</b>?</p>",
                "<p><span>\n<div>\n<div><ul>\n<li>Strafmaß hält Parameter klein</li>\n<li>Kleinere Parameter bedeuten <strong>glattere Funktionen</strong></li>\n<li>Limitiert implizit die <strong>Komplexität</strong> des Models (größeres Lambda → kleinere Komplexität)</li>\n</ul>\n</div>\n</div>\n\n</span></p>"
            ],
            "guid": "xm;g`Vhc/u",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "03_model_selection"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was unterscheidet die \\(\\ell_1\\) von der \\(\\ell_2\\) Penalty?</p>",
                "<p>\\(\\ell_2\\)<b> penalty (Ridge):</b></p><p>\\(\\operatorname{penalty}(\\boldsymbol{\\theta})=\\|\\boldsymbol{\\theta}\\|_{2}=\\sum_{d} \\theta_{d}^{2}\\)</p><ul>\n<li>einfach zu optimieren, weil konvex</li>\n<li>Geschlossene Lösungen vorhanden</li>\n<li>Parameter sind nahe Null, aber niemals Null</li></ul><p><img src=\"paste-f7a7e28c275857b6e61b09184aa1c2970a62ea9e.jpg\"><br></p><p>\\(\\ell_1\\)<b>&nbsp;penalty (LASSO):</b><br></p><p>\\(\\operatorname{penalty}(\\boldsymbol{\\theta})=\\|\\boldsymbol{\\theta}\\|_{1}=\\sum_{d}\\left|\\theta_{d}\\right|\\)<br></p><p>Sparse solution bedeutet, dass einige Parameter dann 0 sind.</p><ul>\n<li>Führt dünnbesetzte Lösungen ein</li>\n<li>Schwieriger zu optimieren</li></ul><div><img src=\"paste-270aa5690d9324d79fc0cc4a0355bfa77404bdac.jpg\"><br></div>"
            ],
            "guid": "L&R?CVJNbf",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "03_model_selection"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie lässt sich mit <b>Ridge Regression</b> <b>Over- und Underfitting</b> vermeiden? Welche Rolle spielt dabei die Wahl von \\(\\lambda\\)?</p>",
                "<p><span>\n</span></p><div>\n<div><ul>\n<li><strong>Hohes Lambda</strong> → Hoher Trainings- und Validierungsfehler</li>\n<li><strong>genau richtig</strong> → Validierungsfehler ist minimal</li>\n<li><strong>Kleines Lambda</strong> → Kleiner Trainingsfehler, aber hoher Validierungsfehler</li>\n</ul>\n</div>\n</div>\n\n<p></p><p><img src=\"12ax1YiR86R9S275oioW.png\" style=\"width: 366px;\"><span><br></span></p>"
            ],
            "guid": "f)9=Iboy*{",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "03_model_selection"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was ist die Idee des <b>early stopping</b>?</p>",
                "<p><span>\n</span></p><div>\n<div><ul>\n<li>Trainiere nicht bis zu kleinen Trainingsfehlern</li>\n<li>Verwendet bei inkrementellen learning rules z. B. gradient descent</li>\n<li>Man verwendet den Validationfehler um zu entscheiden, wann man beendet</li>\n<li>Beschränkt implizit auch die Komplexität des Modells!</li><li>Gewichte werden zwischengespeichert und dann zurückgegeben.</li>\n</ul>\n<p><strong>Illustration</strong></p>\n</div>\n</div>\n\n<p></p><p><img src=\"1ePizQdkj8zdyerpkugL.png\" style=\"letter-spacing: 0.01071em; width: 366px;\"></p>"
            ],
            "guid": "B*t%t1OR]s",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "03_model_selection"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was sind Vor- und Nachteile des <b>early stopping</b> Ansatzes?</p>",
                "<p><span>\n</span></p><div>\n<div><p><strong>Vorteil</strong></p>\n<ul>\n<li>Effizient: es müssen nur Kopien der (vorherigen) Gewichte gespeichert werden</li>\n<li>Einfach: Kein Wechsel des Algorithmus / Modells</li>\n<li>Keine Hyperparameter wie z. B. \\(\\lambda\\) zu bestimmen</li>\n</ul>\n<p><strong>Nachteil</strong></p>\n<ul>\n<li>braucht Datenvalidierung</li></ul></div></div><p></p>"
            ],
            "guid": "u)B`%zB,e=",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "03_model_selection",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie funktioniert das <b>Hinzufügen von Rauschen</b> für <b>linear regression model</b>?</p>",
                "<p>Man fügt Rauschen (noise) hinzu, um wenig robuste Lösungen herauszufiltern.</p><p><img src=\"1SLbEBB1nftnQiWXph7u.png\" style=\"width: 366px;\"><br></p><p>Für ein LR model Rauschen ist gegeben durch:</p><p>\\(f(\\boldsymbol{x}+\\boldsymbol{\\epsilon})=\\boldsymbol{w}^{T}(\\boldsymbol{x}+\\boldsymbol{\\epsilon}), \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\lambda \\boldsymbol{I})\\)<br></p><p>Damit ergibt sich für den MSE:</p><p>\\(\\begin{aligned}<br>\\operatorname{MSE}(\\boldsymbol{w}) &amp;=\\mathbb{E}_{\\boldsymbol{x}, y, \\boldsymbol{\\epsilon}}\\left[\\left(\\boldsymbol{w}^{T} \\boldsymbol{x}-y+\\boldsymbol{w}^{T} \\boldsymbol{\\epsilon}\\right)^{2}\\right] \\\\<br>&amp;=\\underbrace{\\mathbb{E}_{\\boldsymbol{x}, y}\\left[\\left(\\boldsymbol{w}^{T} \\boldsymbol{x}-y\\right)^{2}\\right]}_{n \\mathbf{S S E}(\\boldsymbol{w})}+2 \\underbrace{\\mathbb{E}_{\\boldsymbol{x}, y, \\epsilon}\\left[\\left(\\boldsymbol{w}^{T} \\boldsymbol{x}-y\\right) \\boldsymbol{w}^{T} \\boldsymbol{\\epsilon}\\right]}_{=0, \\text { zero mean, i.i.d. noise }}+\\underbrace{\\mathbb{E}_{\\boldsymbol{x}, y, \\boldsymbol{w}}\\left[\\left(\\boldsymbol{w}^{T} \\boldsymbol{\\epsilon}\\right)^{2}\\right]}_{\\lambda \\boldsymbol{w}^{T} \\boldsymbol{w}} \\\\<br>&amp;=n \\operatorname{SSE}(\\boldsymbol{w})+\\lambda\\|\\boldsymbol{w}\\|_{2}^{2}<br>\\end{aligned}\\)<br></p><p>Das Hinzufügen von Rauschen entspricht also einer Regularisierung mit \\(\\ell_2\\) <b>Norm</b>.</p>"
            ],
            "guid": "F-5,Ao-JNE",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "03_model_selection",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was versteht man unter <b>Data Augmentation</b> (dt. Datenvermehrung)?</p>",
                "<p>Erzeugung zusätzlicher künstlicher Stichproben.</p><p><b>Beispiel:</b></p><p><img src=\"1YsS6YcFep97q99Yjw1a.png\" style=\"width: 366px;\"><br></p>"
            ],
            "guid": "lB_P:GlCrv",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "03_model_selection"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was zeichnet <b>nicht-parametrische Methoden</b> aus?</p>",
                "<p><b>Nicht parametrische Methoden</b> speichern ihre gesamten Trainingsdaten und nutzen die Trainingsdaten für Vorhersagen.&nbsp;</p><p>Sie passen keine Parameter an eine <b>parametrisches Model</b> an.</p>"
            ],
            "guid": "eD-Qt`kXvn",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was sind <b>Vor- und Nachteile nicht-parametrischer Methoden?</b></p>",
                "<p><span>\n<div>\n<div><p><strong>Vorteile</strong></p>\n<ul>\n<li>Komplexität passt sich Trainingsdaten an</li>\n<li>Schnell beim Training</li>\n</ul>\n<p><strong>Nachteile</strong></p>\n<ul>\n<li>Langsam für Vorhersagen</li>\n<li>Problematisch bei hoch-dimensionalen Daten</li>\n</ul>\n</div>\n</div>\n\n</span></p>"
            ],
            "guid": "bF|[6mYZqH",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie funktioniert der <b>k-Nearest Neighbour Classifier</b>?</p>",
                "<p>Um einen neuen Inputvektor \\(x\\) zu klassifizieren, müssen die \\(k\\)-nächsten Trainingsdatenpunkte zu \\(x\\) untersucht werden. Das Objekt wird dann der Klasse mit den häufigsten Objekten zugeordnet.</p><p><img src=\"1uzLhQK96GY3apYKyMV8.png\" style=\"width: 366px;\"><br></p>"
            ],
            "guid": "EVc.6oT-#6",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wann eignet sich der <b>k-Nearest Neighbour Classifier </b>besonders für die <b>Anwendung</b>?</p>",
                "<p><span>\n<div>\n<div><ul>\n<li>kann die Distanz zwischen Datenpunkten messen</li>\n<li>Weniger als 20 Attribute pro Instanz</li>\n<li>viele Trainingsdaten</li>\n</ul>\n</div>\n</div>\n\n</span></p>"
            ],
            "guid": "zHHK`5vT>G",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was sind sogenannte <b>Decision Boundaries</b>?</p>",
                "<p><span>\n</span></p><div>\n<div><ul>\n<li><em>knn</em> ermittelt keine expliziten <strong>decision boundaries</strong></li>\n<li>Voronoidiagramme sind jedoch eine Teilmenge der Trainingsdaten</li>\n<li>Je mehr Datenpunkte man hat, imso komplizierter kann die <em>decision boundary</em> werden.</li>\n</ul>\n</div>\n</div>\n\n<p></p><p><img src=\"paste-efadd0be79a8d3ba8a9aaf743727d97cd743dac6.jpg\"><span><br></span></p>"
            ],
            "guid": "nu1<fsb!d7",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie ist der <b>Euklidische Abstand</b> definiert?&nbsp;</p><p>Unterscheiden Sie den Fall, bei dem Features in derselben Einheit sind und Features, die erst normalisiert werden müssen.&nbsp;</p>",
                "<p><b>Fall 1:</b> Daten haben dieselbe Einheit:</p><p>\\[\\left.d(\\boldsymbol{x}, \\boldsymbol{y})=\\|\\boldsymbol{x}-\\boldsymbol{y}\\|=\\sqrt{\\left(\\sum_{k=1}^{d}\\left(\\boldsymbol{x}_{k}-\\boldsymbol{y}_{k}\\right)^{2}\\right.}\\right)\\]<br></p><p><b>Fall 2:</b> Daten haben unterschiedliche Einheit, müssen also normalisiert werden indem:</p><p>\\(\\tilde{\\boldsymbol{x}}=(\\boldsymbol{x}-\\boldsymbol{\\mu}) \\oslash \\boldsymbol{\\sigma}\\)<br></p><p>Mean \\(\\mu\\), standard deviation \\(\\sigma\\), element-wise divison (similar to Hardamard product).&nbsp;</p><p>\\(\\tilde{\\boldsymbol{x}}\\) is standardized with zero mean and unit variance.</p>"
            ],
            "guid": "L{$B~#b/BB",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Nennen Sie beliebte <b>Distanzmaße</b> bei <b>kNN</b></p>",
                "<p><span>\n</span></p><div>\n<div><ul>\n<li>Euclidean distance</li>\n<li>Cosine distance</li>\n<li>Hamming distance</li>\n<li>Manhattan distance</li><li>Mahlanobis distance</li></ul></div></div><p></p>"
            ],
            "guid": "b.Bwa7-`Rh",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wofür sind jeweils folgende Distanzmaße gut geeignet?</p><p><span>\n</span></p><div>\n<div><ul>\n<li>Euclidean distance</li>\n<li>Cosine distance</li>\n<li>Hamming distance</li>\n<li>Manhattan distance</li></ul></div></div><p></p>",
                "<ul>\n<li>Euclidean distance → Features sind im selben Verhältnis</li>\n<li class=\"has-line-data\" data-line-end=\"2\" data-line-start=\"1\">Cosine distance → Dokumente, Bilder etc.</li>\n<li class=\"has-line-data\" data-line-end=\"3\" data-line-start=\"2\">Hamming distance → Strings / kategorische Features</li>\n<li class=\"has-line-data\" data-line-end=\"4\" data-line-start=\"3\">Manhattan distance → Koordinatenabstand</li>\n</ul>"
            ],
            "guid": "H)6Njs<)H2",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was ist das Problem mit irrelevanten Features bei k-NN?</p>",
                "<p>Typischerweise verschlechtert sich die Performanz des k-NN mit mehr (unrelevanten) Dimensionen.</p><p><img src=\"1nKZJny8gHLUrpastXHy.png\" style=\"width: 366px;\"><br></p>"
            ],
            "guid": "HX~g+8JV3d",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was versteht man unter dem <b>Fluch der Dimensionalität</b> <i>(Curse of dimensionality)</i>?</p>",
                "<p>Fluch der Dimensionalität ist ein Begriff, um den rapiden Anstieg im Volumen beim Hinzufügen weiterer Dimensionen in einen mathematischen Raum zu beschreiben.&nbsp;</p><p>In einem 2 oder 3-dimensionalen Raum sind die meisten Punkte nahe des Ursprungs. In einem hochdimensionalen Raum hingegen nicht mehr.</p><p>Man braucht also deutlich mehr Punkte um den Raum auszufüllen.</p><p><img src=\"curseofdimensionality.png\"><br></p>"
            ],
            "guid": "jN{L)4/jd5",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Warum funktioniert die nearest neighbor classification schlecht auf hoch-dimensionalen Daten?</p>",
                "<p>Selbst wenn fast alle Features relevant sind in einem hochdimensionalen Raum, sind dennoch die meisten Punkte gleich weit entfernt (insb. weit weg vom Ursprung).</p><p>Die \"Neighborhood\" wird sehr groß.</p>"
            ],
            "guid": "cTO0Jeq.n3",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Wie erfolgt der <b>Aufbau</b> eines <b>KD-Baums</b>?",
                "<p>For each non-leaf node<br></p><div>\n<div><ul>\n<li>choose dimension (e. g. longest hyperrectangle)</li>\n<li>choose median as pivot</li>\n<li>split node according to (pivot, dimension)</li></ul></div></div><p><img src=\"paste-c4a07ca06c398d074be73efe9d7d884176769b03.jpg\"><br></p>"
            ],
            "guid": "Ds^XcHRNZ9",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie erfolgt die Suche des <b>nächsten Nachbarn</b> in einem <b>KD-Baum</b>?</p>",
                "<p><img src=\"19BXgeYgmC4DofcMbj4y.png\" style=\"width: 366px;\"></p>"
            ],
            "guid": "sJbHm`iG0b",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was ist eine beliebte Datenstruktur für den k-NN-Algorithmus?</p>",
                "KD-Trees"
            ],
            "guid": "z5eYrC}6H6",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Welche Information enthält ein <b>Regression Tree</b> und welche ein <b>Classification Tree</b>?</p>",
                "<p><span>\n\n</span></p><p style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><strong>Regression</strong></p><ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li>der prognostizierte Wert eines Knotens ist die&nbsp;<strong>durchschnittliche Antwortvariable</strong>&nbsp;für alle Beobachtungen im Knoten</li></ul><p><img src=\"12oHRhRvhjDvvME2nhGB.png\" style=\"width: 366px;\"><br></p><p style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><strong>Klassifikation</strong></p><ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li>die prognostizierte Klasse ist die&nbsp;<strong>häufigste Klasse</strong>&nbsp;einer Node (Mehrheitsvotum).</li><li>Man kann dadurch die erwartete Wahrscheinlichkeit für die Klassenzugehörigkeit erhalten.</li></ul><div><img src=\"cart_classify_tree.JPG\"><br></div>"
            ],
            "guid": "K9h?#JJc_Q",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was ist gängiges <b>Splitkriterium</b> für <b>Regression Trees</b>?</p>",
                "<p>\\[\\operatorname{RSS}=\\sum_{\\text {left }}\\left(y_{i}-\\bar{y}_{L}\\right)^{2}+\\sum_{\\text {right }}\\left(y_{i}-\\bar{y}_{R}\\right)^{2}\\]<br></p><p>wobei&nbsp;\\(\\bar{y}_{L}\\) und&nbsp; \\(\\bar{y}_{R}\\) die Durchschnittswerte im linken und rechten Teilbaum sind.</p>"
            ],
            "guid": "m%D!)?%-6&",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was ist ein <b>gängiges Splitkriterium</b> für <b>Classification Trees</b>?</p>",
                "<p><b>Minimum Entropy</b></p><p>\\[\\text { score }=N_{L} H\\left(p_{\\mathrm{L}}\\right)+N_{R} H\\left(p_{\\mathrm{R}}\\right),\\]<br></p><p>wobei \\(H\\left(p_{L}\\right)=-\\sum_{k} p_{L}(k) \\log p_{L}(k)\\) die Entropie im linken Teilbaum ist. \\(N_{L}\\) sind die Samples im linken Teilbaum.</p>"
            ],
            "guid": "pM7Vzw7c%K",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was sind zwei gängige <b>Stop-Kriterien</b> <i>(stopping criteria) </i>beim Trainieren von Bäumen?</p>",
                "<ul><li>Minimale Anzahl Einträgen pro Node wurde erreicht</li><li>maximale Tiefe wurde erreicht</li></ul>"
            ],
            "guid": "OTMa3%5k/|",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Vergleichen Sie einen Baum mit <b>vielen Einträgen je Knoten</b> mit einem mit <b>wenig Einträgen je Knoten</b>.</p>",
                "<p><b>Geringe Zahl an Einträgen pro Blatt</b></p><p>Baum modelliert auch Rauschen (Overfitting!)</p><p><img src=\"1T6zjvHinfrkW3UWr2wK.png\" style=\"width: 366px;\"><br></p><p><b>Hohe Zahl an Einträgen pro Blatt</b></p><p><img src=\"1BJnf2FaxpjmTUHwv4oX.png\" style=\"width: 366px;\"><br></p><p><span style=\"letter-spacing: 0.01071em;\">Baum ist wenig aussagekräftig.</span></p>"
            ],
            "guid": "L;@Y};b+^q",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p><span>\n\nWas sind die&nbsp;<strong>Vorteile</strong>&nbsp;von&nbsp;<em>Classification and Regression Trees</em>&nbsp;(CART)?\n\n</span><br></p>",
                "<p><span>\n\n<ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li>Anwendbar für Regression und Klassifikation</li><li>\"Natürliche\" Kategorievorhersage</li><li>Rechnerisch wenig aufwendig</li><li>Keine Verteilungsannahmen</li><li>Kann nicht-lineare Zusammenhänge und&nbsp;<em>classification boundaries</em>&nbsp;abbilden</li><li>automatische Variablenauswahl</li><li>Kleine Bäume sind einfach zu interpretieren.</li></ul>\n\n</span></p>"
            ],
            "guid": ")(7D97e1U",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p><span>\n\nWas sind die&nbsp;<strong>Nachteile</strong>&nbsp;von&nbsp;<em>Classification and Regression Trees</em>&nbsp;(CART)?\n\n</span><br></p>",
                "<p><span>\n\n</span></p><ul style=\"letter-spacing: normal; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\"><li style=\"\"><b>Genauigkeit</b>: Heutige Methoden wie Neuronale Netze haben eine 30 % geringere Fehlerrate</li><li style=\"\"><b>Instabilität</b><span style=\"font-weight: 400;\">: Verändert man die Daten nur geringfügig, so verändert sich der Baum. Schwierig dann zu interpretieren.</span></li></ul>\n\n<p></p>"
            ],
            "guid": "hdy?2AX33r",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was ist die<b> zentrale Idee</b> von <b>Random Forests</b>?</p>",
                "<p>Benutze viele Bäume um die Performanz zu verbessern.</p><p><b>Beispiel Regression:</b></p><p><img src=\"12HXSSmLeVk4cvtpWDmv.png\" style=\"width: 366px;\"><br></p><p><b>Beispiel Klassifikation:</b></p><p><img src=\"paste-1cfbf98ff69c968908418a79f6c634639893036d.jpg\"><br></p>"
            ],
            "guid": "BQQ7R-^_@0",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Für was steht <b>Bagging</b>?</p>",
                "Bagging = Bootstrap Aggregating"
            ],
            "guid": "bQ$dI!OqkX",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was versteht man unter <b>Bootstrap Aggregating</b> <i style=\"\">(Bagging)</i>?</p>",
                "<p><span>\n\n</span></p><p style=\"font-weight:400;letter-spacing:0.12852px;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\">Eine Ensemble Methode, die Vorhersagen von mehreren ML Algorithmen kombiniert um eine genauere Vorhersage als die Einzelmodelle zu machen.</p><p style=\"font-weight:400;letter-spacing:0.12852px;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\">Man fittet ein Modell zu sogenannten Bootstrap samples aus den Daten und kombiniert die Modelle mittels&nbsp;<b style=\"font-weight:700\">Votum&nbsp;</b>(Klassifikation) oder&nbsp;<b style=\"font-weight:700\">Durchschnitt&nbsp;</b>(Regression)</p><p></p><p><img src=\"121BJuB1U5Rr3kxCytJF.png\" style=\"width: 366px;\"><br></p>"
            ],
            "guid": "G|eK3;+hC6",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was versteht man unter einem <b>Bootstrap sample</b>?</p>",
                "Ein bootstrap sample wird zufällig mit Ersetzen aus den Daten gezogen. Manche Beobachtungen landen dadurch häufiger im bootstrap sample als andere. Andere landen auch gar nicht im Bag. Man spricht dann von \"out of bag\"."
            ],
            "guid": "Pl*cOZsKGh",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie funktioniert die <b>Variance Reduction</b> bei <b>Random Forests</b>?</p>",
                "<p><b>Im Allgemeinen:</b></p><p>\\(\\operatorname{Var}\\left[\\frac{1}{M} \\sum_{i=1}^{M} X_{i}\\right]=\\frac{1}{M^{2}} \\operatorname{Var}\\left[\\sum_{i=1}^{M} X_{i}\\right]=\\frac{1}{M} \\operatorname{Var}[X], \\quad \\text { if } X \\text { i.i.d. }\\)<br></p><p>Idealerweise würde die Varianz linear mit der Zahl der Bäume sinken.</p><p><b>In der Praxis:</b></p><p>\\(\\operatorname{Var}\\left[\\frac{1}{M} \\sum_{i=1}^{M} \\operatorname{Tree}_{i}\\right]&gt;\\frac{1}{M} \\text { Var [Tree], as trees are still correlated }\\)</p>"
            ],
            "guid": "F3~l4KMvd)",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie ist funktioniert <b>Randomization</b> bei <b>Random Forests</b>?</p>",
                "<p>\n\n</p><p style=\"letter-spacing: normal; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\"><span style=\"font-weight: 400;\"><strong>Baum-Ebene:</strong>&nbsp;Lasse einen&nbsp;<em>forest</em>&nbsp;mit mehreren&nbsp;<em>trees</em>&nbsp;wachsen z. B.&nbsp;</span>\\(R = 500\\)</p><ul style=\"letter-spacing: normal; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\"><li style=\"\">Jeder Baum wird anhand eines&nbsp;<em style=\"font-weight: 400;\">bootstrap samples</em>&nbsp;(Größe \\(N\\)) trainiert</li></ul><p style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><strong>Für jede Node:</strong></p><ul style=\"letter-spacing: normal; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\"><li style=\"\">Wähle \\(m\\) Variablen zufällig aus aus allen \\(M\\)&nbsp;möglichen Variablen</li><li style=\"font-weight: 400;\">Finde den besten Split für alle \\(m\\) Kriterien</li></ul><p style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\">Lasse die Bäume dann bis zur maximalen Tiefe wachsen (Klassifikation) Votum / Durchschnitt der Bäume um Vorhersagen für neue Daten zu erhalten.</p>\n\n<p></p>"
            ],
            "guid": "rQ0L=`8$Uh",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Warum funktioniert <b>Randomization</b> bei <b>Random Forests</b>?</p>",
                "<p><span>\n\n<ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li>Höhere Variabilität der Einzelbäume</li><li>Ein einzelner Baum wird sich eher nicht überspezialisieren</li><li>Einzelne Bäume leiden weniger unter&nbsp;<em>Overfitting</em></li></ul>\n\n</span></p>"
            ],
            "guid": "MWR:8xpoe{",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Hinsichtlich welcher beiden Eigenschaften<b> Random Forests</b> <b>CARTs</b> verbessern?</p>",
                "<p><span>\n\n<strong>Genauigkeit</strong>: Random Forests sind ebenbürdig mit modernen ML Methoden&nbsp;</span></p><p><span><strong>Instabilität</strong>: Veränderungen an den Daten führen zwar zu Veränderungen an den einzelnen Bäumen, der Forest bleibt aber als Kombination mehrerer Bäume stabil</span></p>"
            ],
            "guid": "C,h`.g^IXl",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was sind die <b>Vor- und Nachteile</b> von <b>Random Forests</b>?</p>",
                "<p><span>\n\n</span></p><div style=\"line-height: 19px; white-space: pre;\"><div style=\"\"><b>Vorteile</b>\n<ul style=\"font-weight: 400; letter-spacing: normal; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\"><li>Anwendbar für Regression und Klassifikation</li><li>\"Natürliche\" Kategorievorhersage</li><li>Rechnerisch wenig aufwendig</li><li>Keine Verteilungsannahmen</li><li>Kann nicht-lineare Zusammenhänge und&nbsp;<em>classification boundaries</em>&nbsp;abbilden</li><li>automatische Variablenauswahl</li></ul><p><b>Nachteile</b></p><ul style=\"font-weight: 400; letter-spacing: normal; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\"><li>Schwierig zu interpretieren</li></ul></div></div>\n\n<p></p>"
            ],
            "guid": "v0KR0+h:w5",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>How is a <b>Classifier </b>defined?</p>",
                "<p>Given the dataset \\(\\mathcal{D}=\\left\\{\\left(\\boldsymbol{x}_{i}, c_{i}\\right)\\right\\}_{i=1 \\ldots N}\\), where \\(\\boldsymbol{x}_{i} \\in \\mathbb{R}^{d}\\) are the input samples and \\(c \\in\\{1 \\ldots K\\}\\) are the class labels, we want to learn a classifier \\(f(x)\\) that predicts the class label for unseen samples.<br></p><p>For \\(K=2\\) its binary classification and for \\(K &gt; 2\\) its multi-class classification.</p>"
            ],
            "guid": "Qnviduxlsn",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie unterscheidet sich <b>generative</b> und <b>discriminative modelling</b>?</p>",
                "<p><img src=\"1V6n5SdvMa7opP2FNFsb.png\" style=\"width: 366px;\"><br></p><p><img src=\"12owimmsTSoAU8u6a1ux.png\" style=\"width: 366px;\"><br></p><p><br></p>"
            ],
            "guid": "KoWYA.1~vI",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie funktioniert ein <b>discriminative binary classifier</b>?</p>",
                "<p>Given the training data \\(\\left(\\boldsymbol{x}_{i}, y_{i}\\right), \\mathrm{i}=1 \\ldots \\mathrm{N}\\), with \\(\\boldsymbol{x}_{i} \\in \\mathbb{R}^{d}\\) and \\(y_{i} \\in\\{0,1\\}\\), learn a classifier \\(f(\\boldsymbol{x})\\)<br>such that:<br>\\[<br>f\\left(\\boldsymbol{x}_{i}\\right)= \\begin{cases}&gt;0, &amp; \\text { if } y_{i}=1 \\\\ &lt;0, &amp; \\text { if } y_{i}=0\\end{cases}<br>\\]<br></p><p><img src=\"paste-93eac9aaa150734c89f4411f959426685ac4911a.jpg\"><br></p>"
            ],
            "guid": "D]0R)]-i**",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie ist ein <b>Linear Classifier</b> definiert?</p>",
                "<p>A linear classifier is given in the form:<br></p><p>\\[f(\\mathbf{x})=\\mathbf{w}^{T} \\mathbf{x}+b,\\]<br></p><p>where \\(\\boldsymbol{w}\\) is the normal to the line and \\(b\\) is the bias.</p><p><img src=\"paste-66318042237c3a0b3c2267d485ee08f4c41db114.jpg\"><br></p>"
            ],
            "guid": "@SLeDPk%9",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was ist ein <b>Linear Discriminator</b>?</p>",
                "<p>Funktion die zwei Klassen trennt. In</p><p><span>\n</span></p><ul><li><p>2D: eine Gerade</p></li><li><p>3D: eine Ebene</p></li><li><p>ND: eine Hyperplane</p></li></ul><p></p>"
            ],
            "guid": "g7!:v[KDA_",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Warum ist der <b>SSE </b>(regression loss) nicht als Verlustfunktion für Lineare Klassifikation geeignet?</p>",
                "<p><span>\n\n</span><span>\n\n</span></p><ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li>Man würde übersehen, dass \\(y_i\\) nur aus den Klassen \\(\\{0,1\\}\\) kommt.</li><li>Außerdem ist der regression loss nicht robust bei Ausreisern:</li><li><img src=\"126kztJPnHSRVJHmPTgW.png\" style=\"width: 274px;\"><br></li></ul>\n\n<span>\n\n</span><p></p>"
            ],
            "guid": "f+Y|NP312:",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was ist die <b>Sigmoid Funktion</b>? Warum ist Sie&nbsp;gut für <b>Verlustfunktionen </b>bei <b>Linearen Klassifiziereren</b> geeignet?</p>",
                "<p>\\(\\sigma(a)=\\frac{1}{1+\\exp (-a)}\\)<br></p><div>\n<div><ul>\n<li>Output ist beschränkt zwischen 0 und 1.</li>\n<li>Funktion is glatt.</li>\n</ul>\n</div></div><p><img src=\"paste-3ab198070b33cde8ab5967d6ae100f4a8379d4dc.jpg\"><br></p>"
            ],
            "guid": "l*$y2>`p0]",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was ist der Vorteil von <b>Generalized Logistic Models</b>?</p>",
                "<p><img src=\"paste-285137c0cc0671bc438ec55a9c68b56c519173ba.jpg\"><br></p><p><span>By transforming&nbsp;</span>\\(x_i\\) with<span>&nbsp;</span>\\(\\phi\\left(\\boldsymbol{x}_{i}\\right)\\) one can transform data that is not linearily seperable in the input space, but linear seperable in the feature space.<br></p>"
            ],
            "guid": "x^hC9H>mQc",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie funktioniert <b>Regularisierung</b> <i>(Regularization)</i> bei <b>Logistischer Regression</b>?</p>",
                "<p>On can add a <b>regularization penalty</b>:</p><p>\\(L(\\tilde{\\boldsymbol{w}}, D)=\\log \\operatorname{lik}(\\tilde{\\boldsymbol{w}}, D)-\\lambda \\operatorname{penalty}(\\tilde{\\boldsymbol{w}})\\)<br></p><p>\\(\\ell_2\\) is commonly used:</p><p>\\(\\operatorname{penalty}(\\tilde{\\boldsymbol{w}})=\\|\\tilde{\\boldsymbol{w}}\\|^{2}\\)<br></p>"
            ],
            "guid": "DBn$@k~mRd",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was sind die jeweils gängigen Ansätze für die Regularisierung von Linearen Klassifizierern und Linearen Regressionsmodellen?</p>",
                "<p><b>Least squares solution:</b><br></p><p>\\(\\operatorname{argmin}_{\\boldsymbol{w}} \\operatorname{SSE}(\\boldsymbol{w}, D)+\\lambda \\operatorname{penalty}(\\boldsymbol{w})\\)<br></p><p><b>Maximum likelihood solution:</b></p><p>\\(\\operatorname{argmax}_{\\boldsymbol{w}} \\log \\operatorname{lik}(\\boldsymbol{w}, D)-\\lambda \\operatorname{penalty}(\\boldsymbol{w})\\)<br></p><p>(Recall that&nbsp;\\(\\underset{\\boldsymbol{x}}{\\arg \\min } f(\\boldsymbol{x})=\\underset{\\boldsymbol{x}}{\\arg \\max }-f(\\boldsymbol{x})\\). Therefore, penalty is subtracted.)</p>"
            ],
            "guid": "MoPjFm7sok",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wann lassen sich globale Extrema finden, wann nur lokale?</p>",
                "<p><span>\n\n</span></p><ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li>Konvexe Funktion → Globale Extrema d. h. Minimum / Maximum</li><li>Nicht-Konvexe Funktion → Lokale Extrema</li></ul><div><img src=\"paste-4ae50fc23f62c713b16a5ccb3f0cafa27c17441d.jpg\"><br></div>\n\n<p></p>"
            ],
            "guid": "i]!EFHOSqO",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was ist ein <u>großer Vorteil</u> hinsichtlich Ridge Regression bei der Bestimmung des Minimums?</p>",
                "<p>\\(L_{\\text {ridge }}=(\\boldsymbol{y}-\\boldsymbol{\\Phi} \\boldsymbol{w})^{T}(\\boldsymbol{y}-\\boldsymbol{\\Phi} \\boldsymbol{w})+\\lambda \\boldsymbol{w}^{T} \\boldsymbol{w}\\)<br></p><p><span>\n\n</span></p><ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li>Funktion ist konvex</li><li>quadratische Funktion von&nbsp;\\(w\\)</li><li>Minimum kann damit ausnahmsweise analytisch bestimmt werden</li></ul><p></p>"
            ],
            "guid": "EkA}0mr`c4",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p><span>\n\nWas lässt sich mit Gradient Descent finden?\n\n</span><br></p>",
                "<p><span>\n\n<ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li><strong>Globale Minima</strong>&nbsp;bei&nbsp;<strong>konvexen</strong>&nbsp;Funktionen</li><li><strong>Lokale Minima</strong>&nbsp;bei&nbsp;<strong>nicht-konvexen</strong>&nbsp;Funktionen</li></ul>\n\n</span></p>"
            ],
            "guid": "5dLB{2aBu",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie ist der ungefähre Ablauf von <b>Gradient Descent</b>?</p>",
                "<p>Starte an einem Punkt und folge dem Gradienten zu einem Minimum.</p><p><img src=\"paste-5010ad84a5a9e22bd18b0de803fb4412a7521bb6.jpg\"><br></p><p>\\(\\eta\\) ist dabei die Lernrate.<br></p><p>Abbruchkriterium für Gradient Descent ist:<br></p><ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li>Änderungen im Gradient sind klein</li><li>Veränderungen im Funktionswert sind klein</li><li>Zeitbudget ist aufgebraucht</li></ul>"
            ],
            "guid": "E07Swm#>;d",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Führen Sie sich vor Augen, was passiert, wenn bei Gradient Descent die <b>step size</b> / <b>Learning Rate</b> zu groß / zu klein ist.</p>",
                "<p>\\(\\eta_{t}=t\\), it is too big<br></p><p><img src=\"paste-ace4f49525fc841b9e02ba6054f73313373b8ebf.jpg\"><br></p><p>too small \\(\\eta_{t}\\), after 100 iterations<br></p><p><img src=\"paste-4aea5717d369f2f35231f2e2d629b380297df76d.jpg\"><br></p>"
            ],
            "guid": "HaoJ5<EN_m",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was ist die Idee von&nbsp;<b>Stochastic Gradient Descent</b>?</p>",
                "<p>Benutze nur <b>eine Stichprobe</b> (<i>sample</i>) um die Aktualisierung zu berechnen.</p>"
            ],
            "guid": "vNqDl+bNt+",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>\n\nWas sind <b>Vor- und Nachteile</b> von&nbsp;<strong>Stochastic Gradient Descent</strong>?\n\n<br></p>",
                "<p><span>\n\n</span></p><div><div><b>Vorteile:</b></div><div><ul><li>Iterationen sind viel günstiger</li></ul></div></div><div><b><br></b></div><div><b>Nachteile:</b></div><ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li>Steigt nicht immer ab</li><li>Benötigt kleine Iterationen</li></ul><div><img src=\"paste-7a18a60ed0a74c8b2ff0271621f6e62ba6061bb7.jpg\"><br></div>\n\n<p></p>"
            ],
            "guid": "xkbfn,#?@v",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>How should the&nbsp;<b>Learning Rate</b> / <b>Step size</b>&nbsp;for&nbsp;<b>Stochastic Gradient Descent </b>be chosen?</p>",
                "<div>\n<div><ul>\n<li>Assymptotically approach the optimum</li>\n<li>Instead of “wiggling” around optimum</li>\n</ul>\n</div></div><p>Standard in <b>SGD </b>is to use diminishing step sizes, e.g., \\(\\eta_{t}=\\frac{1}{t}\\)</p>"
            ],
            "guid": "LM9XkDWuJe",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Warum funktioniert <b>stochastic gradient descent</b> oft besser als <b>batch gradient descent</b>?</p>",
                "<br><div>Oftmals enthalten Daten Redundanzen. Berechnung bei <b>batch gradient descent</b> sind dann redundant z. B. Gradienten für ähnliche Samples oder dieselben Paramtervektoren.<br><br>Passiert nicht, wenn wir unmittelbar nach einer Stichprobe aktualisieren (<b>stochastic gradient descent</b>).</div><div><br></div>"
            ],
            "guid": "hU2;JA*)Yw",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was ist die Idee von <b>Mini-Batches</b> bei <b>Gradient Descent</b>?</p>",
                "<p>Take subset of samples \\(I_{t} \\subset\\{1, \\ldots, n\\},\\left|I_{t}\\right|=b, b \\ll n\\) to approximate real gradient:<br></p><p>\\[\\frac{1}{b} \\sum_{i \\in I_{t}} l\\left(\\boldsymbol{x}_{i} ; \\boldsymbol{\\theta}\\right) \\\\ \\boldsymbol{\\theta}_{t+1}=\\boldsymbol{\\theta}_{t}-\\frac{\\eta}{b} \\sum_{i \\in I_{t}} \\nabla_{\\boldsymbol{\\theta}} l\\left(\\boldsymbol{x}_{i} ; \\boldsymbol{\\theta}_{t}\\right)\\]</p><p>where \\(b\\) is the number of samples drawn uniformly at random from the the trainingsets and \\(I_{t}\\) are the samples themselves. As they are drawn randomly and uniformly the expectation of the gradient is unchanged, however variance is much lower.</p>"
            ],
            "guid": "xu:z@+Cj!@",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification",
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was sind wichtige Eigenschaften der Sigmoid-Funktion? (3 Stück)</p>",
                "<p><b>Beschränkt:</b><br></p><p>\\(\\sigma(a)=\\frac{1}{1+\\exp (-a)} \\in(0,1)\\)<br></p><p><b>Symmetrisch:</b></p>\\(-\\sigma(a)=\\frac{\\exp (-a)}{1+\\exp (-a)}=\\frac{1}{1+\\exp (a)}=\\sigma(-a)\\)<p><b>Ableitung existiert:</b><br></p><p>\\(\\sigma^{\\prime}(a)=\\frac{\\exp (-a)}{(1+\\exp (-a))^{2}}=\\sigma(a)(1-\\sigma(a))\\)</p>"
            ],
            "guid": "L+g#[^z|H]",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie sieht die Softmax Likelihood function für einen Multiclass Klassifizierer aus?</p>",
                "<p><b>Softmax Likelihood function:</b><br></p><p>\\(p(c=i \\mid \\boldsymbol{x})=\\frac{\\exp \\left(\\boldsymbol{w}_{i}^{T} \\boldsymbol{\\phi}(\\boldsymbol{x})\\right)}{\\sum_{k=1}^{K} \\exp \\left(\\boldsymbol{w}_{k}^{T} \\boldsymbol{\\phi}(\\boldsymbol{x})\\right)}\\)<br></p><div>\n<div><ul>\n<li>Each class gets a weight vector</li><li>Higher probability for class \\(i\\) if \\(\\boldsymbol{w}_{i}^{T} \\phi(\\boldsymbol{x})\\) is high</li><li>For \\(K=2, \\boldsymbol{w}_{2}\\) is redundant → better to use sigmoid<br></li>\n</ul><div><img src=\"paste-39842fc139de6141d2ecc902cc9326fa3e04220c.jpg\"><br></div>\n</div></div>"
            ],
            "guid": "c6:g2AN=IF",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie sieht der Gradient für die Multiclass Classification aus? Vollziehen Sie die Berechnung nach.</p>",
                "<p>\\[\\begin{aligned}<br>\\frac{\\partial \\operatorname{loss}_{i}}{\\partial \\boldsymbol{w}_{k}} &amp;=\\frac{\\partial}{\\partial \\boldsymbol{w}_{k}}\\left(\\sum_{k=1}^{K} \\boldsymbol{h}_{c_{i}, k} \\boldsymbol{w}_{k}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)-\\log \\left(\\sum_{j=1}^{K} \\exp \\left(\\boldsymbol{w}_{j}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)\\right)\\right)\\right) \\\\<br>&amp;=\\boldsymbol{h}_{c_{i}, k} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)-\\frac{\\partial}{\\partial \\boldsymbol{w}_{k}} \\log \\left(\\sum_{j=1}^{K} \\exp \\left(\\boldsymbol{w}_{j}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)\\right)\\right) \\\\<br>&amp;=\\boldsymbol{h}_{c_{i}, k} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)-\\frac{1}{\\sum_{j=1}^{K} \\exp \\left(\\boldsymbol{w}_{j}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)\\right)} \\exp \\left(\\boldsymbol{w}_{k}^{T} \\phi\\left(\\boldsymbol{x}_{i}\\right)\\right) \\phi\\left(\\boldsymbol{x}_{i}\\right) \\\\<br>&amp;=\\underbrace{\\phi\\left(\\boldsymbol{x}_{i}\\right)}_{\\text {feature vector }} \\underbrace{\\left(\\boldsymbol{h}_{c_{i}, k}-p\\left(k \\mid \\boldsymbol{x}_{k}\\right)\\right)}_{\\text {\"soft-max error\" }}<br>\\end{aligned}\\]<br></p>"
            ],
            "guid": "v`k|pu02m-",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie ist der <b>Data log-likelihood </b>für eine <b>Multiclass Classification</b> definiert?</p>",
                "<p><b>Hintergrund: </b>Umformulierung conditional multinomial distribution:</p><p>\\(\\begin{aligned} p(c \\mid \\boldsymbol{x}) &amp;=\\prod_{k=1}^{K} p(c=k \\mid \\boldsymbol{x})^{\\boldsymbol{h}_{x, k}} \\\\ &amp;=\\prod_{k=1}^{K}\\left(\\frac{\\exp \\left(\\boldsymbol{w}_{k}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)\\right)}{\\sum_{k^{\\prime}=1}^{K} \\exp \\left(\\boldsymbol{w}_{k^{\\prime}}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)\\right)}\\right)^{\\boldsymbol{h}_{c, k}} \\end{aligned}\\)<br></p><p><b>Hintergrund 2:</b></p><p>\\(p(c=i \\mid \\boldsymbol{x})=\\frac{\\exp \\left(\\boldsymbol{w}_{i}^{T} \\phi(\\boldsymbol{x})\\right)}{\\sum_{k=1}^{K} \\exp \\left(\\boldsymbol{w}_{k}^{T} \\boldsymbol{\\phi}(\\boldsymbol{x})\\right)}\\)<br></p><p><b>Berechnung:</b></p><p>\\(\\begin{aligned} \\log \\operatorname{lik}\\left(\\mathcal{D}, \\boldsymbol{w}_{1: K}\\right) &amp;=\\sum_{i=1}^{N} \\log p\\left(c_{i} \\mid \\boldsymbol{x}_{i}\\right)=\\sum_{i=1}^{N} \\underbrace{\\sum_{k=1}^{K} \\boldsymbol{h}_{c_{i}, k} \\log p\\left(k \\mid \\boldsymbol{x}_{i}\\right)}_{\\operatorname{loss}_{i} \\ldots \\text { loss of the ith sample }} \\\\ &amp;=\\sum_{i=1}^{N} \\sum_{k=1}^{K} \\boldsymbol{h}_{c_{i}, k}\\left[\\boldsymbol{w}_{k}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)-\\log \\left(\\sum_{j=1}^{K} \\exp \\left(\\boldsymbol{w}_{j}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)\\right)\\right)\\right] \\\\ &amp;=\\sum_{i=1}^{N} \\sum_{k=1}^{K} \\boldsymbol{h}_{c_{i}, k} \\boldsymbol{w}_{k}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)-\\underbrace{\\log \\left(\\sum_{j=1}^{K} \\exp \\left(\\boldsymbol{w}_{j}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)\\right)\\right)}_{\\text {independent from } \\mathrm{k}} \\underbrace{\\sum_{k} \\boldsymbol{h}_{c_{i}, k}}_{=1} \\end{aligned}\\)<br></p><p><br></p>"
            ],
            "guid": "Km[TzWN,2H",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>\n\nWarum betreibt man Dimensionality Reduction <i>(applications)</i>?\n\n<br></p>",
                "<p><span>\n\n</span></p><ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li>Zur besseren Visualisierung von Daten</li><li>Vorverarbeitung für lernende Algorithmen</li></ul><p style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><strong>Beispiel</strong></p><p style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\">z. B. Verringert sich bei least-squares linear regression der Aufwand für Invertieren der Matrix, sofern man weniger Features hat.</p><p></p>"
            ],
            "guid": "mul*mSUjG",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_dim_reduction",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was ist das Besondere an <b>Linear Dimensionality Reduction</b>?</p>",
                "Ein Datenpunkt ist dann eine <b>Linearkombination</b> von Basisvektoren."
            ],
            "guid": "rfWD2:fP54",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie lässt sich das <b>Problem</b> der <b>Linear Dimensionality Reduction</b> mathematisch formalisieren?</p>",
                "<p>Let \\(\\boldsymbol{x}_{i}\\)&nbsp;be the \\(i\\)-th original data point&nbsp;\\(\\boldsymbol{x}_{i} \\in \\mathbb{R}^{D}\\).</p><p>We try to find a low-dimensional representation of the \\(i\\)-th data point:&nbsp;\\(\\boldsymbol{z}_{i} \\in \\mathbb{R}^{M}\\) with \\(D&gt;&gt;M\\).</p><p>This is equivalent to finding a mapping:</p><p>\\(\\boldsymbol{x}_{i} \\rightarrow \\boldsymbol{z}_{i}\\)<br></p><p>By restricting the mapping to be a linear function one gets:</p><p>\\(\\boldsymbol{z}_{i}=\\boldsymbol{W} \\boldsymbol{x}_{i}\\), with \\(\\boldsymbol{W} \\in \\mathbb{R}^{M \\times D}\\)<br></p>"
            ],
            "guid": "dU}#g-2TGS",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_dim_reduction",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was ist eine <b>Orthonormalbasis</b>?</p>",
                "<p><span>\nDas heißt das Skalarprodukt zweier beliebiger Basisvektoren ergibt Null und jeder Basisvektor besitzt die Norm 1.\n\n</span></p><p><span><b>Beispiel:</b></span></p><p>\\(\\left[\\begin{array}{l}3 \\\\ 7\\end{array}\\right]=3\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]+7\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]\\)<span><br></span></p><p><span>mit&nbsp;</span>\\(e_1\\)<span>&nbsp;und&nbsp;</span>\\(e_2\\)<span>.</span></p>"
            ],
            "guid": "rjqA#O!PI@",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_dim_reduction",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was ist die Grundidee der <b>Zerlegung</b> <i>(Decomposition)</i>?</p>",
                "<p>Use \\(M &lt;&lt; D\\) basis vectors:</p><p>\\(\\boldsymbol{x}=\\underbrace{\\sum_{i=1}^{M} z_{i} \\boldsymbol{u}_{i}}_{\\tilde{\\boldsymbol{x}} \\approx \\boldsymbol{x}}+\\underbrace{\\sum_{j=M+1}^{D} z_{j} \\boldsymbol{u}_{j}}_{\\text {skip }},\\)<br></p><p>where \\(\\boldsymbol{u}_{i}\\) are basis vectors and&nbsp; \\(z_i\\) are scalars.</p><p>We then try to find \\(M\\) basis vectors \\(\\boldsymbol{u}_{i}\\), so that the <b>mean squared reproduction error</b> is minimal:</p><p>\\(\\underset{\\boldsymbol{u}_{1}, \\ldots, \\boldsymbol{u}_{M}}{\\arg \\min } E\\left(\\boldsymbol{u}_{1}, \\ldots, \\boldsymbol{u}_{M}\\right)=\\underset{\\boldsymbol{u}_{1}, \\ldots, \\boldsymbol{u}_{M}}{\\arg \\min } \\sum_{i=1}^{N}\\left\\|\\boldsymbol{x}_{i}-\\tilde{\\boldsymbol{x}}_{i}\\right\\|^{2}\\)<br></p>"
            ],
            "guid": "J84P^tLkI)",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_dim_reduction",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie funktioniert die Minimierung des Fehlers bei der <b>Zerlegung</b> <i>(decomposition)</i>?</p>",
                "<p>Assuming a single basis vector the error can be written as:</p><p>\\(\\begin{aligned} E\\left(\\boldsymbol{u}_{1}\\right) &amp;=\\sum_{i=1}^{N}\\left\\|\\boldsymbol{x}_{i}-\\tilde{\\boldsymbol{x}}_{i}\\right\\|^{2}=\\sum_{i=1}^{N}\\|\\boldsymbol{x}_{i}-\\underbrace{\\left(\\boldsymbol{u}_{1}^{T} \\boldsymbol{x}_{i}\\right)}_{z_{i 1}} \\boldsymbol{u}_{1}\\|^{2} \\\\ &amp;=\\sum_{i=1}^{N} \\boldsymbol{x}_{i}^{T} \\boldsymbol{x}_{i}-2\\left(\\boldsymbol{u}_{1}^{T} \\boldsymbol{x}_{i}\\right)^{2}+\\left(\\boldsymbol{u}_{1}^{T} \\boldsymbol{x}_{i}\\right)^{2} \\boldsymbol{u}_{1}^{T} \\boldsymbol{u}_{1}=\\sum_{i=1}^{N} \\boldsymbol{x}_{i}^{T} \\boldsymbol{x}_{i}-\\left(\\boldsymbol{u}_{1}^{T} \\boldsymbol{x}_{i}\\right)^{2} \\\\ &amp;=\\sum_{i=1}^{N} \\boldsymbol{x}_{i}^{T} \\boldsymbol{x}_{i}-z_{i 1}^{2} \\end{aligned}\\)<br></p><p>\\(\\Rightarrow \\underset{\\boldsymbol{u}_{1}}{\\arg \\min } E\\left(\\boldsymbol{u}_{1}\\right)=\\underset{\\boldsymbol{u}_{1}}{\\arg \\max } \\sum_{i=1}^{N} z_{i 1}^{2}=\\underset{\\boldsymbol{u}_{1}}{\\arg \\max } \\sum_{i=1}^{N}\\left(\\boldsymbol{u}_{1}^{T} \\boldsymbol{x}_{i}\\right)^{2}\\)<br></p><p>Note that:\\(z_{i1}=\\boldsymbol{u}_{1}^{T} \\boldsymbol{x}_{i}\\). Minimizing the error is equivalent to maximizing the varaince in projection (Assuming a zero mean on the data). A zero mean projection can be assured by subtracting the mean from the data</p><p>\\(\\overline{\\boldsymbol{x}}_{i}=\\boldsymbol{x}_{i}-\\boldsymbol{\\mu}\\)<br></p>"
            ],
            "guid": "s}Ddd+gsDt",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_dim_reduction",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was ist das Ziel der <b>principle component analysis (PCA)</b>?</p>",
                "<p>Man versucht sogenannte <b>principal directions</b> und die <b>Varianz der Daten</b> entlang jeder principal direction zu finden.</p><p><b>Illustration:</b></p><p><img src=\"1fLjd2BUnBQdpwVMFghR.png\" style=\"width: 366px;\"><br></p>"
            ],
            "guid": "CVeykKI?E(",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie funktioniert die Berechnung der<b> first principal direction</b> bei der <b>principal component analysis</b>? Wie der zweiten?</p>",
                "<p>The first principal direction \\(\\boldsymbol{u}_{1}\\) is the direction anlong which the variance of the projected data is maximal<br></p><p>\\(\\boldsymbol{u}_{1}=\\underset{\\boldsymbol{u}}{\\arg \\max } \\frac{1}{N} \\sum_{i=1}^{N}(\\boldsymbol{u}^{T} \\underbrace{\\left(\\boldsymbol{x}_{i}-\\boldsymbol{\\mu}\\right)}_{\\overline{\\boldsymbol{x}}_{i}})^{2} \\quad\\) s.t. \\(\\boldsymbol{u}^{T} \\boldsymbol{u}=1\\)<br></p><p>Note that the directions have unit norm implied through the constraint.</p><p>The second principal direction maximizes the variance of the data in the orthogonal complement of the first principal direction.</p><p><img src=\"paste-4853cc17679d81491132eceda746d9c1e84eb049.jpg\"><br></p>"
            ],
            "guid": "o?:;b)v4>W",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_dim_reduction",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Welches Verfahren wird gern für <b>Optimierungsprobleme mit Nebenbedingungen</b> <i>(constrained optimization) </i>genutzt?</p>",
                "<b>Langrange-Multiplikatoren</b> <i>(Lagrangian Multiplier)</i>"
            ],
            "guid": "J-1=7}3s9(",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was ist die Idee des <b>Langrange Multiplikators </b><i>(Lagrangian Multiplier)</i>?</p>",
                "<p><img src=\"12tM8TE8zecMfzU5NQWF.png\" style=\"width: 366px;\"><br></p><p><span style=\"letter-spacing: 0.01071em;\">Man kann dann den Gradient (mit Ableitung nach x und Lambda) berechnen und gleich 0 setzen.</span></p><p><span style=\"letter-spacing: 0.01071em;\">Lambda ist ein beliebiges Vielfaches (eine Konstante).</span></p><p><span style=\"letter-spacing: 0.01071em;\"><b>Beispiel mit mehreren Optimierungsbedingungen</b></span></p><p><img src=\"1skohGEQx4R82tiGP3D3.png\" style=\"width: 351px;\"><span style=\"letter-spacing: 0.01071em;\"><b><br></b></span></p><p><span style=\"letter-spacing: 0.01071em;\">b_i sind konstante Bedingungen z. B. Budget etc...</span></p>"
            ],
            "guid": "hZo()oRyFg",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_dim_reduction",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie lässt sich der Lagrange Multiplier für die Bestimmung der <b>principal component directions</b> verwenden?</p>",
                "<p>Calculating the principal directions / components can be written as:</p><p>\\(\\boldsymbol{u}_{1}=\\underset{\\boldsymbol{u}}{\\arg \\max } \\boldsymbol{u}^{T} \\boldsymbol{\\Sigma} \\boldsymbol{u} \\quad\\) s.t. \\(\\boldsymbol{u}^{T} \\boldsymbol{u}=1\\)<br></p><p>By applying Langranian optimization one gets the Langranian given by:</p><p>\\(L(\\boldsymbol{u}, \\lambda)=\\boldsymbol{u}^{T} \\boldsymbol{\\Sigma} \\boldsymbol{u}+\\lambda\\left(\\boldsymbol{u}^{T} \\boldsymbol{u}-1\\right)\\)<br></p><p>With an optimal solution for \\(u\\):</p><p>\\(\\frac{\\partial L(\\boldsymbol{u}, \\lambda)}{\\partial \\boldsymbol{u}}=2 \\boldsymbol{\\Sigma} \\boldsymbol{u}+2 \\lambda \\boldsymbol{u} \\stackrel{!}{=} \\mathbf{0} \\quad \\Rightarrow \\boldsymbol{\\Sigma} \\boldsymbol{u}=\\lambda \\boldsymbol{u}\\)<br></p><p>Which is an Eigenvalue problem.</p>"
            ],
            "guid": "dRE-P~Br`?",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_dim_reduction",
                "formula_sheet"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie funktioniert die <b>Eigenwert-Zerlegung</b> für <b>positiv-definite, symmetrische Matritzen</b>?</p>",
                "<p>\\(\\boldsymbol{C}=\\boldsymbol{U} \\boldsymbol{\\Lambda} \\boldsymbol{U}^{T}=\\underbrace{\\left[\\begin{array}{lll}\\boldsymbol{u}_{1} &amp; \\ldots &amp; \\boldsymbol{u}_{D}\\end{array}\\right]}_{\\text {Eigenvectors }} \\underbrace{\\left[\\begin{array}{c}\\lambda_{1} \\\\ {\\ddots} \\\\ \\lambda_{D}\\end{array}\\right]}_{\\text {Eigenvalues }}\\left[\\begin{array}{c}\\boldsymbol{u}_{1}^{T} \\\\ \\vdots \\\\ \\boldsymbol{u}_{D}^{T}\\end{array}\\right]\\)<br></p><p>D. h. jede positiv-definite, symmetrische Matrix kann zerlegt werden.</p>"
            ],
            "guid": "L&0;0lGGx|",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie stehen <b>Eigenwerte</b> und <b>Eigenvektoren</b> in Verbindung mit der <b>PCA</b>?</p>",
                "<div>\n<div><ul>\n<li>Die <b>größten Eigenwerte</b> geben die <b>maximale Varianz</b></li>\n<li>Die zugehörigen&nbsp;<b>Eigenvektoren </b>geben die <b>Richtung der maximalen Varianz</b></li>\n</ul>\n</div></div>"
            ],
            "guid": "uXV3.M+Nf7",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was sind die drei Dinge, die PCA macht?</p>",
                "<p><span>\n\n<ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li>PCA projeziert Daten auf einen linearen Unterraum</li><li>PCA maximiert die Varianz der Projektion</li><li>PCA minimiert den Fehler in der Rekonstruktion</li></ul>\n\n</span></p>"
            ],
            "guid": "Eg;^$,pU%%",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_dim_reduction",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was sind <b>Anwendungsgebiete</b> von <b>PCA</b>?</p>",
                "<p><span>\n\n</span></p><ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li><p>PCA erlaubt die Transformation hochdimensionaler Input Räume in niedrigdimensionale Feature Spaces.</p></li><li><p>PCA findet ein natürlicheres Koordinatensystem für die Daten</p></li><li><p>PCA ist ein wichtiger Schritt in der Datenvorverarbeitung für hochdimensionale Daten</p></li></ul><p></p>"
            ],
            "guid": "OpqHZy=>8E",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_dim_reduction",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p><span>\n\nWas sind Ansätze für die Density-Estimation? Welche grundlegenden Ansätze unterscheidet man?\n\n</span><br></p>",
                "<p><span>\n\n<p style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><strong>Parametrisch</strong></p><ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li>Maximum Likelihood</li><li>Bayesian Estimation</li></ul><p style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><strong>Nicht-Parametrische</strong></p><ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li>Histogramme</li><li>Kernel-Density-Methoden</li><li>Nearest Neighbour Density Estimation</li></ul>\n\n</span></p>"
            ],
            "guid": "WY{@9b2<k",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Welche zwei Parameter parametrisieren die <b>Gauss-Verteilung</b> vollständigt.</p>",
                "Mittelwert \\(\\mu\\) und Varianz \\(\\sigma^2\\)&nbsp;"
            ],
            "guid": "NxuH*]r.,T",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p><span>\n\nWas ist der Vorteil von&nbsp;<strong>nicht-parametrischen Modellen</strong>&nbsp;bei der Density-Estimation?\n\n</span><br></p>",
                "<p><span>\n\nMan weiß oftmals gar nicht welche Form z. B. Normalverteilung die class-conditional density annimmt.\n\n</span></p>"
            ],
            "guid": "rqGy8?xsq7",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p><span>\n\nWas sind&nbsp;<strong>Beispiele</strong>&nbsp;für&nbsp;<strong>nicht-parametrische Modelle</strong>&nbsp;zur Density-Estimation?\n\n</span></p>",
                "<p><span>\n\n</span></p><ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li>Histograms</li><li>Kernel density estimation (Parzen Window)</li><li>\\(k\\)-nearest neighbour</li></ul>\n\n<p></p>"
            ],
            "guid": "ocy/Sl?Y^%",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie lässt sich mit <b>Histogrammen </b>eine Dichte-Schätzung machen?</p>",
                "<p><span>\n\n</span></p><div style=\"font-weight:normal;line-height:19px;white-space:pre\"><div><span><ol style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li>Teile den Input-Space in Bins auf</li><li>Zähle die Samples pro Bin</li></ol></span></div></div>"
            ],
            "guid": "tMa%7!F3_r",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p><span>\n\nWas sind Nachteile von Histogrammen bei der Dichte-Schätzung?\n\n</span><br></p>",
                "<p><span>\n\n</span></p><ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li>Problematisch bei hochdimensionalen Feature Spaces</li><li>Exponentieller Anstieg der Bins</li><li>Benötigt exponentiell viele Daten</li><li>Die Wahl der richtigen Größe der Bins ist erneut ein <i>Model-Selection-Problem</i></li></ul>\n\n<p></p>"
            ],
            "guid": "B?y6kT<SI5",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie ist ein Distanzmaß (Distance Measure) formal definiert?</p>",
                "<p><span>\n\n</span></p><div style=\"font-weight:normal;line-height:19px;white-space:pre\"><div><span>Seien </span>\\(O_1\\)<span> und </span>\\(O_2\\)<span> zwei Objekte aus dem Universum </span>der möglichen Objekte. Die Distanz </div><div>(Unähnlichkeit) zwischen \\(O_1\\) und \\(O_2\\) ist dann eine reele Zahl ausgedrückt in \\(D(O_1, O_2)\\).</div></div>\n\n<p></p>"
            ],
            "guid": "Q[UPQ^N%!A",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_clustering"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Welche Eigenschaften sollte ein <b>gutes Distanzmaß</b> haben?</p>",
                "<p><i>Symmetry </i>/ Symmetrie</p><p>\\(D(A, B) = D(B,A)\\)</p><p><i>Constancy of Self-Similiarty / </i>Selbst-Ähnlichkeit</p><p>\\(D(A,A) = 0\\)</p><p><i>Postivity (Separation)</i> / Positivheit</p><p><i>Triangular inequality</i> / Dreiecksungleichung</p><p>\\(D(A, B) \\leq D(A,C) + D(B,C)\\)</p>"
            ],
            "guid": "o_5~l|9]#P",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_clustering",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p><span>\n\nWelche zwei Gruppen an&nbsp;<strong>Clustering Algorithmen</strong>&nbsp;unterscheidet man? Was sind jeweils Vertreter?\n\n</span><br></p>",
                "<p><span>\n\n</span></p><p style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><strong>Hierarchical Clustering Methoden</strong></p><ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li>Bottom-up (merging)</li><li>Top-down (splitting)</li></ul><p style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><strong>Flat clustering Algorithmen</strong></p><ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li>\\(k\\)-Means</li><li>Mixture Models</li></ul>\n\n<p></p>"
            ],
            "guid": "lv`1|?5tbA",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_clustering",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was ist ein Dendrogram?</p>",
                "<p><img src=\"1ZUYZsiN4VW6SEgTJ1YX.png\" style=\"width: 366px;\"><br></p><p><img src=\"1x2JriRywDQMenpFzMvN.png\" style=\"width: 267px;\"><br></p>"
            ],
            "guid": "jZEc&#R%0!",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_clustering",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was ist ein möglicher <b>Anwendungsfall </b>von Dendrogrammen?</p>",
                "<b>Ausreiser-Erkennung:</b> Ein einzelner, isolierter Zweig legt nahe, dass ein Datenpunkt sehr verschieden von den übrigen Punkten ist.<div><br></div><div><img src=\"paste-f9adfdfac33981ec2b25857eb9ddd712300f5a20.jpg\"><br></div>"
            ],
            "guid": "O$)&_7$BE>",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_clustering"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie unterscheiden sich die beiden Hierarchical Clustering Ansätze <b>Bottom-Up</b> und <b>Top-Down</b>?</p>",
                "<p><span>\n\n<ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li><strong>Bottom-Up</strong>&nbsp;(agglomerative): Beginnend mit jedem Item in seinem eigenen Cluster, finde das beste Paar zur Vereinigung in einem neuen Cluster. Wiederhole solange alle Cluster vereinigt sind.</li><li><strong>Top-Down</strong>&nbsp;(divisive): Beginnend mit allen Daten eines Clusters, betrachte alle Möglichkeiten das Cluster in zwei aufzuteilen. Wähle die beste Aufteilung und wiederhole auf beiden Seiten.</li></ul>\n\n</span></p>"
            ],
            "guid": "GOZxtkJ0EE",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_clustering"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie funktioniert <b>Bottom-Up Clustering</b>?</p>",
                "<p><img src=\"12UNbg829E4AMsrifd22.png\" style=\"width: 366px;\"></p>"
            ],
            "guid": "neeH:Ft*=I",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_clustering",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Nennen Sie 4 Methoden, um die Distanz <i>(Cluster Linkage)</i> zwischen Clustern zu beschreiben.</p><p>Was ist jeweils die Intuition?</p>",
                "<p><b>Single Linkage</b></p><p>Minimale Distanz zwischen zwei Punkten zweier Cluster</p><p>\\(d\\left(C_{k}, C_{l}\\right)=\\min _{\\boldsymbol{x}_{i} \\in C_{k}} \\min _{\\boldsymbol{x}_{j} \\in C_{l}} d\\left(\\boldsymbol{x}_{i}, \\boldsymbol{x}_{j}\\right)\\)<br></p><p><img src=\"paste-1769a1d2b3eb91d2bcaf68d843fe5208b1741ccd.jpg\"><b><br></b></p><p><b>Complete Linkage</b></p><p>Maximale Distanz zwischen zwei Datenpunkten zweier Cluster</p><p>\\(d\\left(C_{k}, C_{l}\\right)=\\max _{\\boldsymbol{x}_{i} \\in C_{k}} \\max _{\\boldsymbol{x}_{j} \\in C_{l}} d\\left(\\boldsymbol{x}_{i}, \\boldsymbol{x}_{j}\\right)\\)<br></p><p><img src=\"paste-9e375eedd4394eb696c28905b4dc118751d18a7f.jpg\"><b><br></b></p><p><b>Average Linkage</b></p><p>Durchschnittliche Distanz zwischen zwei beliebigen Paaren</p><p>\\(d\\left(C_{k}, C_{l}\\right)=\\frac{1}{\\left|C_{l}\\right|\\left|C_{k}\\right|} \\sum_{\\boldsymbol{x}_{i} \\in C_{1}} \\sum_{\\boldsymbol{x}_{j} \\in C_{k}} d\\left(\\boldsymbol{x}_{i}, \\boldsymbol{x}_{j}\\right)\\)<br></p><p><img src=\"paste-efd5b89295384446131675ddee1c066c481369f3.jpg\"><b><br></b></p><p><b>Centroid Linkage</b></p><p>Distanz zwischen zwei Centroiden</p><p>\\(d\\left(C_{k}, C_{l}\\right)=d\\left(\\frac{1}{\\left|C_{l}\\right|} \\sum_{\\boldsymbol{x}_{i} \\in C_{l}} \\boldsymbol{x}_{i}, \\frac{1}{\\left|C_{k}\\right|} \\sum_{\\boldsymbol{x}_{j} \\in C_{k}} \\boldsymbol{x}_{j}\\right)\\)</p><p><img src=\"paste-1597c02f411761888d2c9db1a2990c40f01b3a8a.jpg\"><br></p>"
            ],
            "guid": "wcs!Hr[z+x",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_clustering",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>\n\n</p><div style=\"line-height: 19px; white-space: pre;\"><div style=\"\">Was&nbsp;sind&nbsp;die&nbsp;Vorteile&nbsp;/&nbsp;Nachteile&nbsp;von&nbsp;<b>Hierarchical&nbsp;Clustering&nbsp;Methods</b>?</div></div><p></p>",
                "<p><span>\n\n</span></p><p style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><strong>Vorteile</strong></p><ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li>Keine Notwendigkeit die Zahl der Cluster im Vorhinein festzulegen</li><li>Hierarchisches Clustering bildet sich schön auf die menschliche Intuition ab</li></ul><p style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><strong>Nachteile</strong></p><ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li>Skalieren schlecht. Aufwand von \\(\\mathcal{O}(n^2)\\)</li><li>Wie jeder heuristische Algorithmen sind lokale Optima ein Problem</li><li>Interpretation der Ergebnisse ist sehr subjektiv</li></ul>\n\n<p></p>"
            ],
            "guid": "vj_D028;2x",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_clustering",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie funktioniert der \\(k\\)<b>-means Algorithmus?</b></p>",
                "<p><img src=\"1zeVvHW3XQHy8ELPRV9q.png\" style=\"width: 318px;\"><br></p><p><img src=\"1JEJcBEUBcZ4UuwNAiJW.png\" style=\"width: 332px;\"></p><p><img src=\"12DBdgEh2YzWxo7kiWr8.png\" style=\"width: 366px;\"><br></p><p><img src=\"12h2gEdoa2bG2JSfLgoP.png\" style=\"width: 366px;\"></p><p><img src=\"1L7FTx9NcP9ATodQukyE.png\" style=\"width: 366px;\"><br></p><p><img src=\"1xh71nP4RmuG2nt9FvEX.png\" style=\"width: 366px;\"></p><p><img src=\"1Mhk16usuKxW4ZD6Yoai.png\" style=\"width: 271px;\"><br></p>"
            ],
            "guid": "Mv)|!)W7_+",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_clustering",
                "checklater",
                "formula_sheet"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p><span>\n\nIst die Konvergenz von&nbsp;</span>\\(k\\)<span>-Means garantiert?\n\n</span><br></p>",
                "<p><span>\n\n<p style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><strong>lokal:</strong></p><ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li>Ja, denn es gibt nur eine finite Zahl an Zentren.</li><li>Jede Zuweisung oder Anpassung verbessert oder hält den SSD konstant</li></ul><p style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><strong>global:</strong></p><ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li>Nein, NP-schweres Problem.</li><li>Stark abhängig von der Initialisierung der Zentren</li></ul>\n\n</span></p>"
            ],
            "guid": "IvUrTv`..o",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_clustering"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was verbessert <b>K-means++</b> gegenüber <b>K-means</b>?</p>",
                "<p><img src=\"12vaXC2wWmZx5UBJL2mb.png\" style=\"width: 365px;\"><br></p><p><img src=\"12YdUtaFq7aXifiMVDhb.png\" style=\"width: 334px;\"><br></p>"
            ],
            "guid": "v3GNimF8}k",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_clustering",
                "checklater",
                "formula_sheet"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p><span>\n\nWelche Ansätze existieren um das optimale K zu finden beim K-Means Algorithmus?\n\n</span><br></p>",
                "<p><span>\n\n<ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li>Guter Rückgang der Funktionswerts der objective function (SSD) auf \"holdout\" set oder mit cross-validation Methode</li><li>\"Knee-finding Methode\" / \"elbow finding\"</li></ul>\n\n</span></p>"
            ],
            "guid": "|vL&k1^<p",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_clustering"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p><span>\n\nWie funktioniert die&nbsp;<strong>knee-finding Methode</strong>&nbsp;zum Finden der optimalen Cluster-Anzahl bei&nbsp;</span>\\(k\\)<span>-Means?\n\n</span><br></p>",
                "<p><span>\n\n</span></p><ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li>Plotten der objective function (SSD) für \\(k\\) von 1 bis 6.</li><li>SSD geht zurück mit höherem \\(k\\)</li><li>bei abrupter Verbesserung des SSD hört man auf</li></ul><div><img src=\"paste-221678bdd29f30a141e99f77ac12e3a84bbe54fe.jpg\"><br></div>\n\n<p></p>"
            ],
            "guid": "KtKcke+E>B",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_clustering"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was sind die <b>Vor- und Nachteile</b> von<b> </b>\\(k\\)<b>-Means</b>?</p>",
                "<p><span>\n\n<p style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><strong>Vorteile</strong></p><ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li>K-Means konvergiert typischerweise schnell</li><li>K-Means++ garantiert immer noch nicht globale Optima zu finden. → mehrmalige Intialisierung mit leicht unterschiedlichen Werten für bessere Ergebnisse</li></ul><p style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><strong>Nachteile</strong></p><ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li>Nur anwendbar wenn (Cluster-) Mittelpunkt definiert ist → schwierig bei kategorischen Daten</li><li>Kann nicht mit Rauschen und Ausreisern umgehen</li><li>Ungeeignet für nicht-konvexe Mengen</li></ul>\n\n</span></p>"
            ],
            "guid": "b1#daM/=d[",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_clustering",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie bestimmt man die Wahrscheinlichkeit, dass \\(\\boldsymbol{x}\\) in eine Region \\(R\\)&nbsp;fällt?</p>",
                "<p>We can also compute \\(p(x \\in R)\\) from samples (If we have sufficiently large dataset).<br></p><p>\\(p(\\boldsymbol{x} \\in R)=\\int_{R} p(\\boldsymbol{x}) d \\boldsymbol{x} \\approx p(\\boldsymbol{x}) V\\)<br></p><p>\\(V\\) the volume e. g. fixed with Parzen Window and&nbsp;\\(\\boldsymbol{x}\\) is sampled from probability density \\(p(\\boldsymbol{x})\\)</p>"
            ],
            "guid": "gO5Biy$^p^",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>How does the <b>parzen window</b> work for <b>density estimation</b>?</p>",
                "<p><b>Kernel function:</b> Span a hypercube with dimension \\(d\\) and edge length \\(h\\).</p><p>\\(g(\\boldsymbol{u})= \\begin{cases}1, &amp; \\left|u_{j}\\right| \\leq h / 2, j=1 \\ldots d \\\\ 0, &amp; \\text { else }\\end{cases}\\)<br></p><p><b>Volume:</b></p><p>\\(V=\\int g(\\boldsymbol{u}) d \\boldsymbol{u}=h^{d}\\)<br></p><p><b>Estimated density:</b><br></p><p>\\(p\\left(\\boldsymbol{x}_{*}\\right) \\approx \\frac{1}{N h^{d}} \\sum_{i=1}^{N} g\\left(\\boldsymbol{x}_{*}-\\boldsymbol{x}_{i}\\right)\\)</p>"
            ],
            "guid": "t}7]@]1#a!",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie funktioniert ein Gaussian Kernel zur Bestimmung der Dichtefunktion?</p>",
                "<p><img src=\"1LEFTh6DNySf4khfAXLd.png\" style=\"width: 366px;\"></p>"
            ],
            "guid": "CtxKkYN!Ye",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie lässt sich die Wahrscheinlichkeit bestimmen, dass ein Punkt \\(\\boldsymbol{x}\\) gesampled aus \\(p(x)\\) in eine Region \\(R\\) fällt?</p>",
                "<p>Für ein ausreichend großes Datenset kann man&nbsp;\\(p(\\boldsymbol{x} \\in R)\\) berechnen:</p><p>\\(p(\\boldsymbol{x} \\in R) \\approx \\frac{K}{N} \\Rightarrow p(x) \\approx \\frac{K}{N V},\\)</p><p>wobei \\(N\\) die Gesamtzahl der Punkte und \\(K\\) ist die Anzahl der Punkte, die in die Region \\(R\\) fallen.</p><p></p>"
            ],
            "guid": "c<R#F8IpBP",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p><span>\n\nWelche Probleme treten hinsichtlich der Parameterbestimmung bei Histogramen, der Kernel Density Estimation und bei k-nearest neighbour auf?\n\n</span><br></p>",
                "<p><span>\n\n</span></p><ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li>Histogramme: Größe der Bins (zu groß → zu glatt)</li><li>Kernel density estimation: Kernel-Bandbreite (\\(h\\) zu groß → zu glatt)</li><li>K-nearest neighbour: \\(K\\) (\\(K\\) zu groß → zu glatt)</li></ul>\n\n<p></p>"
            ],
            "guid": "NN@^$R=7pn",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was sind die Vor- und Nachteile <b>parametrischer </b>und <b>nicht-parametrischer Modelle</b>?</p>",
                "<p><img src=\"1yxWobtRk1AxCDps1beK.png\" style=\"width: 366px;\"><br></p>"
            ],
            "guid": "krEnROGe?L",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p><span>\n\nWas ist die Grundidee von&nbsp;<strong>Mixture Models</strong>?\n\n</span><br></p>",
                "<p><span>\n\n</span></p><div style=\"line-height: 19px;\"><span style=\"white-space: pre;\">Erzeuge eine komplexe Verteilung, indem man einfache verbindet z. B. Normal-Verteilungen.</span></div><div style=\"line-height: 19px;\"><span style=\"white-space: pre;\"><br></span></div><div style=\"line-height: 19px;\"><span style=\"white-space: pre;\"><b>Beispiel:</b></span></div><div style=\"line-height: 19px;\"><img src=\"1WVxqxhuSnvQ5eHTUVzd.png\" style=\"width: 318px;\"><span style=\"white-space: pre;\"><br></span></div><div style=\"line-height: 19px;\"><br></div><div style=\"line-height: 19px;\"><br></div><div style=\"line-height: 19px;\">\\(p(\\boldsymbol{x})=\\sum_{k=1}^{K} p(k) p(\\boldsymbol{x} \\mid k),\\)</div><div style=\"line-height: 19px;\"><br></div><div style=\"line-height: 19px;\">wher \\(p(k)\\) is the mixture coefficient, \\(K\\) is the number of components and \\(p(\\boldsymbol{x} \\mid k)\\) is the \\(k\\)-th mixture component.</div>\n\n<p></p>"
            ],
            "guid": "D<PC_]2C-j",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie ist Gauss'sche Mischverteilungsmodell <i>(Mixture of Gaussians)</i> definiert?</p>",
                "<p><img src=\"1QC3fD7UxQPSDdaRCRN6.png\" style=\"width: 366px;\"><br></p>"
            ],
            "guid": "vTwMgNC^Mk",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was sagt die Kullback-Leibler-Divergenz <i>(Kullback-Leibler Divergence)</i> aus?</p>",
                "<p>KL-Divergenz ist ein Maß für die <b>Ähnlichkeit von Verteilungen,</b></p>"
            ],
            "guid": "iTbPW]TN$X",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was sind <b>Latente Variablenmodelle</b> <i>(Latent Variable Models)</i>?</p>",
                "<p><span>\nEin latentes Variablenmodell beschreibt den Zusammenhang zwischen \nbeobachtbaren Variablen und dahinter liegenden latenten Variablen.</span></p><p><span><b>Beispiel:</b></span></p><p><span>Beobachtete Variable x, latente Variable z (z. B. Mischfaktor bei Mischfaktoren (mixture components).</span></p><p><span>\n\n</span></p>"
            ],
            "guid": "u@;q{H%j~v",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie lassen sich die Parameter eines <b>Gaussian Mixture Models </b>bestimmen?</p>",
                "Mittels des Expectation-Maximization (EM)-Ansatzes, da eine Optimierung mit Log-Likelihood schwierig ist, weil Summen mit log zu optimieren wären."
            ],
            "guid": "FxuqHmuQHb",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was ist der <b>Expectation-Maximization (EM)</b> Algorithmus?</p>",
                "<p><span>\n\n</span></p><ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li>Ein allgemeiner Algorithmus zur Bestimmung von Modellen mit latenten Variablen</li><li>Gern genutzt bei Gaussian Mixture Models zur Bestimmung der Indizes der Mixture-Components</li></ul>\n\n<p></p>"
            ],
            "guid": "q^,;2PW.C2",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie zerlegt der EM Algorithmus den marginal likelihood?</p>",
                "<p>\\[<br>\\log p(\\boldsymbol{x} \\mid \\boldsymbol{\\theta})=\\sum_{z} q(z) \\log \\frac{p(\\boldsymbol{x}, z \\mid \\boldsymbol{\\theta})}{q(z)}+\\sum_{z} q(z) \\log \\frac{q(z)}{p(z \\mid x)}<br>\\]<br></p>"
            ],
            "guid": "D1Kl-N2Jf4",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>How does the EM algorithm work? What are its main steps?</p>",
                "<p><b>Expectation step</b></p><p>\\(q(z)=\\underset{q}{\\arg \\min } \\mathrm{KL}(q(z) \\| p(z \\mid \\boldsymbol{x}))\\)<b><br></b></p><p>Find \\(q(z)\\) (distribution of latent variable) that minimizes KL.</p><p>Can be done in closed form for discrete \\(\\boldsymbol{z}\\) (e. g. mixtures)</p><p>\\(q(z)=p\\left(z \\mid \\boldsymbol{x}, \\boldsymbol{\\theta}_{\\text {old }}\\right)=\\frac{p\\left(\\boldsymbol{x}, z \\mid \\boldsymbol{\\theta}_{\\text {old }}\\right)}{\\sum_{z} p\\left(\\boldsymbol{x}, z \\mid \\boldsymbol{\\theta}_{\\mathrm{old}}\\right)}\\)</p><p><b>Maximization step:</b></p><p>\\(\\boldsymbol{\\theta}=\\underset{\\boldsymbol{\\theta}}{\\arg \\max } \\mathcal{L}(q, \\boldsymbol{\\theta})=\\underset{\\boldsymbol{\\theta}}{\\arg \\max } \\sum_{z} q(z) \\log p(\\boldsymbol{x}, z \\mid \\boldsymbol{\\theta})+\\text { const }\\)<b><br></b></p><p>Maximize lower bound with respect to \\(\\boldsymbol{\\theta}\\)</p><p>Also called the complete-data likelihood</p><p>Each possible value of the missing dat is weighted by:</p><p>\\(q(z)=p\\left(z \\mid \\boldsymbol{x}, \\boldsymbol{\\theta}_{\\text {old }}\\right)\\)</p>"
            ],
            "guid": "H[#gQE?8oY",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Why does EM always improve the lower bound?</p><p>Why does EM always improve the marginal likelihood?</p>",
                "<p>EM improves the <b>lower bound</b>&nbsp;\\(\\mathcal{L}\\left(q_{\\text {new }}, \\boldsymbol{\\theta}_{\\text {new }}\\right) \\geq \\mathcal{L}\\left(q_{\\text {old }}, \\boldsymbol{\\theta}_{\\text {old }}\\right)\\)</p><p>E-Step: KL is set to 0, lower bound has to go up<br></p><p>M-Step: Lower bound is maximized<br></p><p><br></p><p>EM improves the <b>marginal likelihood</b>&nbsp;\\(\\log p\\left(\\boldsymbol{x} \\mid \\boldsymbol{\\theta}_{\\mathrm{new}}\\right) \\geq \\log p\\left(\\boldsymbol{x} \\mid \\boldsymbol{\\theta}_{\\mathrm{old}}\\right)\\)<br></p><p>E-Step: Marginal likelihood is unaffected</p><p>M-Step: Lower bound increases and KL increases (can't get smaller than 0)</p>"
            ],
            "guid": "Gp@0v2,mt*",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie funktioniert der <b>EM-Algorithmus</b> für <b>Mixture of Gaussians</b>?</p>",
                "<p><b>Initialize: </b>Mixture Components + Mixture coefficinents e. g. with k-means for the component means and some initial covariance</p><p><b>Repeat until covariance:</b></p><p><b>Expecation step:</b>&nbsp;Compute responsibilities (degree to which a component contributes to the model)</p><p>\\(q_{i k}=\\frac{\\pi_{k} \\mathcal{N}\\left(\\boldsymbol{x}_{i} \\mid \\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Sigma}_{k}\\right)}{\\sum_{j=1}^{K} \\pi_{j} \\mathcal{N}\\left(\\boldsymbol{x}_{i} \\mid \\boldsymbol{\\mu}_{j}, \\boldsymbol{\\Sigma}_{j}\\right)}=p\\left(z=k \\mid \\boldsymbol{x}_{i}\\right)\\)<br></p><p><b>Maximization step: </b>Update coefficients, components means and component variance</p><p>\\(\\pi_{k}=\\frac{\\sum_{i} q_{i k}}{N}\\)<br></p><p>\\(\\boldsymbol{\\mu}_{k}=\\frac{\\sum_{i} q_{i k} \\boldsymbol{x}_{i}}{\\sum_{i} q_{i k}}\\)<br></p><p>\\(\\boldsymbol{\\Sigma}_{k}=\\frac{\\sum_{i} q_{i k}\\left(\\boldsymbol{x}_{i}-\\boldsymbol{\\mu}_{k}\\right)\\left(\\boldsymbol{x}_{i}-\\boldsymbol{\\mu}_{k}\\right)^{T}}{\\sum_{i} q_{i k}}\\)<br></p>"
            ],
            "guid": "bxMaI5KqNV",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Compare \\(k\\)-means with EM. What distinguishs both?</p>",
                "<div>\n<div><ul>\n<li>k-means can be seen as a special case of EM</li>\n<li>EM is harder to learn than k-means but also gives variances and densities</li>\n<li>Often k-means is used to initialize the means of EM</li>\n</ul>\n</div></div>"
            ],
            "guid": "d.6gb|/Gj_",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Was ist ein <b>Kernel</b>?",
                "<div><b>Definition:</b></div><div>Gehe von einer Abbildung \\(\\varphi: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}\\) aus, die unsere Inputvektoren in \\(\\mathbb{R}^{n}\\) auf einen feature space \\(\\mathbb{R}^{m}\\) abbildet. Dann ist das Skalarprodukt von \\(\\mathbf{x}\\) und \\(\\mathbf{y}\\) in diesem Raum \\(\\varphi(\\mathbf{x})^{T} \\varphi(\\mathbf{y})\\).&nbsp;</div><div><br></div><div>Ein Kernel ist nun eine Funktion \\(\\kappa\\), die dem Skalarprodukt entspricht z. B. \\(\\kappa(\\mathbf{x}, \\mathbf{y})=\\varphi(\\mathbf{x})^{T} \\varphi(\\mathbf{y})\\).</div><div><br></div><div>Man erhält also ein Skalar und braucht Berechnung nicht in hochdimensionalen Raum durchführen.</div><div><br></div><div><b>Intutition:</b></div>Ein Kernel wird verwendet, um das <b>Skalarprodukt</b> zweier Vektoren \\(x\\) und \\(y\\) in einem hochdimensionalen Raum auszuführen."
            ],
            "guid": "MY-$%J<&n#",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "07_kernel_methods",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Sei<div>\\[\\mathbf{K}=\\left(\\begin{array}{lll}<br>1 &amp; 0.5 &amp; 0.3 \\\\<br>0.5 &amp; 1 &amp; 0.6 \\\\<br>0.3 &amp; 0.6 &amp; 1<br>\\end{array}\\right)\\]<br><br></div><div>eine <b>Kernelmatrix</b>.</div><div><br></div><div><div>(Matrix zeigt Ähnlichkeit von Datenpunkten)</div><div><br></div><div>Was sind <b>Eigenschaften</b> der <b>Kernel-Matrix</b>?</div></div>",
                "<div>\n<div><ul>\n<li>\\(\\mathbf{K}\\) hat Größe \\(n \\times n\\)</li>\n<li>Schlechte Skalierbarkeit (Speicherung \\(\\mathbf{K}\\) und Berechnung \\(n^2\\).</li>\n</ul>\n</div></div>"
            ],
            "guid": "L@,MFA_jMN",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "07_kernel_methods"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Welche <b>Eigenschaften</b> hat <b>positiv-definiter kernel</b>?",
                "Ein positiv-definiert kernel \\(k\\) ist eine Funktion \\(k: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}\\) die:<div><div>\n<div><ul>\n<li><b>Symmetrisch</b>: \\(\\quad \\forall x, x^{\\prime}: k\\left(x, x^{\\prime}\\right)=k\\left(x^{\\prime}, x\\right)\\)</li>\n<li><b>Ähnlichkeitsmatrix </b>ist immer positiv-definit: \\(\\boldsymbol{a}^{T} \\boldsymbol{K} \\boldsymbol{a}=\\sum_{i=1}^{n} \\sum_{j=1}^{n} a_{i} a_{j} k\\left(\\boldsymbol{x}_{i}, \\boldsymbol{x}_{j}\\right) \\geq 0, \\quad \\forall \\boldsymbol{a}, \\forall S=\\left\\{\\boldsymbol{x}_{1}, \\ldots, \\boldsymbol{x}_{n}\\right\\}\\)</li></ul></div></div></div>"
            ],
            "guid": "bf){EXe0y>",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "07_kernel_methods",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Wie ist ein <b>linearer kernel</b> definiert?",
                "\\(\\kappa\\left(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime}\\right)=\\left\\langle\\boldsymbol{x}, \\boldsymbol{x}^{\\prime}\\right\\rangle\\), wobei \\(\\langle\\cdot, \\cdot\\rangle\\) das Skalarprodukt bezeichnet."
            ],
            "guid": "BMTI+vayXY",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "07_kernel_methods",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Wie ist ein <b>polynomial kernel</b> definiert?",
                "<div>Kernel für Polynome vom Grad \\(d\\):</div>\\[\\kappa(x, y)=\\langle x, y\\rangle^{d}\\]<br><div><br></div><div><b>Beispiel:</b></div><div><br></div><div>Für \\(x=\\left[x_{1}, x_{2}\\right]^{T}\\), sei \\(\\phi(x)=\\left[x_{1}^{2}, \\sqrt{2} x_{1} x_{2}, x_{2}^{2}\\right]\\).</div><div><br>Der Kernel ist dann definiert mit:<br><br>\\[\\begin{aligned}<br>\\kappa\\left(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime}\\right) &amp;=x_{1}^{2} x_{1}^{\\prime 2}+2 x_{1} x_{2} x_{1}^{\\prime} x_{2}^{\\prime}+x_{2}^{2} x_{2}^{\\prime 2} \\\\<br>&amp;=\\left(x_{1} x_{1}^{\\prime}+x_{2} x_{2}^{\\prime}\\right)^{2} \\\\<br>&amp;=\\left\\langle x, x^{\\prime}\\right\\rangle^{2}<br>\\end{aligned}<br>\\]<br></div><div><br></div><div><img src=\"paste-0794fed3b91b29540d61d65276d9b20b14458a0b.jpg\"><br></div>"
            ],
            "guid": "Ki(@[L(1*6",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "07_kernel_methods",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Wie ist der <b>Gaussian Kernel</b> definiert?",
                "<br>\\[\\kappa(\\boldsymbol{x}, \\boldsymbol{y})=\\operatorname{exp}\\left(-\\frac{\\|\\boldsymbol{x}-\\boldsymbol{y}\\|^{2}}{2 \\sigma^{2}}\\right),\\]<br><br>wobei \\(\\sigma\\) der \"bandwidth\" Parameter ist.<div><br></div><div>Visualisierung verschiedener \\(\\sigma\\):</div><div><img src=\"paste-f06366296c0279ade3fd9fc59d4ad9a419d17c28.jpg\"><br></div>"
            ],
            "guid": "ol}Y_~Ikr",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "07_kernel_methods",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Was versteht man unter dem <b>Kernel Trick</b>?",
                "<div><div>\n<div><div><b>Eigene</b></div><ul>\n<li>Kernels können verwendet werden um Daten \\(x\\) in einen <em>infinite dimensional feature space</em> abzubilden.</li>\n<li>Der <em>feature vector</em> muss dabei nie explizit repräsentiert sein. Es reicht aus, wenn wir das Kreuzprodukt bestimmen können.</li>\n<li>Der <strong>kernel trick</strong>\n erlaubt, im originalen Feature Space zu operieren ohne dass \nBerechnungen der Koordinaten in einem höher-dimensionalen Raum \nnotwendig sind.</li>\n<li>Interessant, weil man dann eine mächtigere Repräsentation ggü. <strong>standard linear feature models</strong> erhält vgl. etwa SVM.</li>\n</ul>\n</div></div></div><div><br></div><div><b>Erklärung Vorlesung</b></div><div><br></div><div>Ersetzen von einem Produkt zweier feature vectors durch eine Funktion.</div>"
            ],
            "guid": "E_~|S(f1)N",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "07_kernel_methods",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Wie hängen die Kernelmatrix \\(\\mathbf{K}\\), der Kernel \\(\\kappa\\) und die Abbildung \\(\\mathbf{\\Phi}_X\\) zusammen?",
                "<div>Feature matrix:</div><div><br></div>Sei \\(\\boldsymbol{\\Phi}_{X}=\\left[\\begin{array}{c}\\phi\\left(\\boldsymbol{x}_{1}\\right)^{T} \\\\ \\vdots \\\\ \\phi\\left(\\boldsymbol{x}_{N}\\right)^{T}\\end{array}\\right] \\in \\mathbb{R}^{N \\times d}\\) dann gelten nachfolgende Identitäten für die Matrix. \\(d\\) ist Dimension von der Samples. Können \\(\\infty\\) sein.<div><div><br>Kernel matrix: \\(\\quad K=\\Phi_{X} \\Phi_{X}^{T}\\)<br><br></div><div>Check: \\([\\boldsymbol{K}]_{i j}=\\phi\\left(\\boldsymbol{x}_{i}\\right)^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{j}\\right)=k\\left(\\boldsymbol{x}_{i}, \\boldsymbol{x}_{j}\\right)\\)<br><br></div><div>Kernel vector: \\(\\quad k\\left(x^{*}\\right)=\\left[\\begin{array}{c}k\\left(x_{1}, x^{*}\\right) \\\\ \\vdots \\\\ k\\left(x_{N}, x^{*}\\right)\\end{array}\\right]=\\left[\\begin{array}{c}\\phi\\left(x_{1}\\right)^{T} \\phi\\left(x^{*}\\right) \\\\ \\vdots \\\\ \\phi\\left(x_{N}\\right)^{T} \\phi\\left(x^{*}\\right)\\end{array}\\right]=\\Phi_{X} \\phi\\left(x^{*}\\right)\\)</div></div>"
            ],
            "guid": "bUrePuV*W`",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "07_kernel_methods",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Was machen <b>kernels</b>?",
                "Kernels messen die Ähnlichkeit zwischen zwei Samples.<div><br></div><div><b>Beispiel:</b></div><div><br></div><div>Ähnlichkeit der Zeichenketten:&nbsp;</div><div>aatcgagtcac, atggacgtct, tgcactact</div>"
            ],
            "guid": "kC`>ZWH{L(",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "07_kernel_methods"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Welche <b>ML-Algorithmen</b> können <b>Kernel</b> verwenden?",
                "<div><b>Regression:</b> Kernel Ridge regression, Gaussian processes</div><div><b>Klassifikation: </b>SVMs, Kernel Logistic Regression</div>"
            ],
            "guid": "A:u{CT8B[E",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "07_kernel_methods"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Was sind die Vor- und Nachteile von&nbsp;<b>Kernel-Methods</b>?",
                "<b>Vorteile:</b><div><div>\n<div><ul>\n<li>Sehr flexible Repräsentation, die sich der Komplexität der Daten anpasst</li>\n<li>Funktioniert gut mit kleinen Datensets</li>\n</ul>\n</div></div></div><div><br></div><div><b>Nachteile</b>:</div><div><div>\n<div><ul>\n<li>Skaliert schlecht für große Probleme, weil Kernelmatrix die gesamten Samples enthält</li>\n</ul>\n</div></div></div><div><br></div><div><br></div>"
            ],
            "guid": "w=#H7|&wkt",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "07_kernel_methods"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Wie sollten die <b>Hyperparameter</b> z. B. eines <i>Gaussian kernels</i> bestimmt werden?",
                "Die Wahl der Hyperparameter z. B. \\(\\sigma\\) ist selbst <i>model selection problem</i> und sollte mittels <i>cross-validation</i> gelöst werden.<div><br></div><div><img src=\"paste-2cb62211eede3fe03134d1b9fd9eabcc0a4d3938.jpg\"><br></div>"
            ],
            "guid": "s7/FU(7/1@",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "07_kernel_methods"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Wie funktioniert <b>kernel ridge regression</b>?",
                "<div>Wie bekannt lautet die Lösung für \\(\\mathbf{w}^{*}\\):</div><div><br></div><div>\\[\\boldsymbol{w}_{\\text {ridge }}^{*}=\\underbrace{\\left(\\Phi^{T} \\Phi+\\lambda I\\right)^{-1}}_{d \\times d \\text { matrix inversion }} \\boldsymbol{\\Phi}^{T} \\boldsymbol{y}\\]<br></div><div>Bei unendlicher Dimension \\(d\\) ist Invertierung der Matrix nicht machbar.<br></div><div><br></div><div><b>Anwendung des kernel tricks:</b></div><div>D. h. Umschreiben der Lösung als Skalarprodukt der <i>feature spaces</i>. Man nutzt <i>searle set of identities</i>.</div><div><br></div><div>Verwendung von Identität:</div><div><br></div><div>\\[(\\boldsymbol{I}+\\boldsymbol{\\Phi}^{T}\\boldsymbol{\\Phi})^{-1} \\boldsymbol{\\Phi}^{T}=\\boldsymbol{\\Phi}^{T}(\\boldsymbol{I}+\\boldsymbol{\\Phi}\\boldsymbol{\\Phi}^{T})^{-1}\\]</div><div><br></div><div>\\[\\begin{aligned} \\boldsymbol{w}^{*} &amp;=\\underbrace{\\left(\\Phi^{T} \\Phi+\\lambda I\\right)^{-1}}_{d \\times d \\text { matrix inversion }} \\mathbf{\\Phi}^{T} \\boldsymbol{y}=\\boldsymbol{\\Phi}^{T} \\underbrace{\\left(\\boldsymbol{\\Phi} \\Phi^{T}+\\lambda I\\right)^{-1}}_{N \\times N \\text { matrix inversion }} \\boldsymbol{y}\\\\\\end{aligned}\\]</div><div><br></div><div>\\[\\boldsymbol{w}^{*}&nbsp;=\\boldsymbol{\\Phi}^{T} \\underbrace{(\\boldsymbol{K}+\\lambda \\boldsymbol{I})^{-1} \\boldsymbol{y}}_{\\boldsymbol{\\alpha}}=\\boldsymbol{\\Phi}^{T} \\boldsymbol{\\alpha}\\]<br></div><div><br></div><div>Auswertung von \\(\\boldsymbol{w}^{*}\\):</div><div><br></div><div>\\[f(\\boldsymbol{x})=\\phi(\\boldsymbol{x})^{T} \\boldsymbol{w}^{*}=\\boldsymbol{\\phi}(\\boldsymbol{x})^{T} \\boldsymbol{\\Phi}^{T} \\boldsymbol{\\alpha}=\\boldsymbol{k}(\\boldsymbol{x})^{T} \\boldsymbol{\\alpha}=\\sum_{i} \\alpha_{i} k\\left(\\boldsymbol{x}_{i}, \\boldsymbol{x}\\right)\\]<br></div><div><br></div><div><b>Auswirkungen</b><br></div><div><br></div><div>Man muss nicht mehr \\(d \\times d\\) Matrix sondern \\(N \\times N-\\)Matrix invertieren, was von Vorteil ist, wenn \\(d &gt; N\\). \\(\\alpha\\) ist aus \\(\\mathbb{R}^{N \\times 1}\\).</div><div><br></div><div>Aber \\(\\boldsymbol{w}^{*}\\) stammt immernoch aus \\(\\mathbb{R}^{d \\times N}\\) und kann unendlich dimensional sein und damit nicht repräsentiert werden.</div>"
            ],
            "guid": "oDV*0{e]()",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "07_kernel_methods",
                "checklater",
                "formula_sheet"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Was ist die Grundidee von <b>EM</b> for <b>Dimensionality Reductio</b>n (sg. <b>proabilistic PCA</b>)?</p>",
                "<p>Introduce a latent variable model to relate a \\(D\\)-dimensional observation vector to a corresponding \\(M\\)-dimensional gaussian latent variable (with \\(M &lt; D\\))<br></p><p>\\[<br>\\boldsymbol{x}=\\boldsymbol{W} \\boldsymbol{z}+\\boldsymbol{\\mu}+\\boldsymbol{\\epsilon}<br>\\]</p><p><br>\\(\\boldsymbol{z}\\) is a \\(d\\)-latent variable (our low dimensional representation)<br>\\(\\boldsymbol{W}\\) is a \\(D \\times M\\) matrix relating the latent space \\(z\\) with the original space \\(x\\)<br>\\(\\boldsymbol{\\mu}\\) is a constant offset vector&nbsp;</p><p>\\(\\boldsymbol{\\epsilon}\\) is a d-dimensional Gaussian noise vector \\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N}\\left(\\mathbf{0}, \\sigma^{2} \\boldsymbol{I}\\right)\\)</p>"
            ],
            "guid": "AP<Sqn]nVf",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Wie funktioniert der <b>Generative Process</b> bei <b>PPCA</b>?</p>",
                "<div>\n<div><ol>\n<li>Sample latent variable<br>\\[<br>\n\\boldsymbol{z} \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{I})<br>\\]</li>\n<li>Linearly project to high-\\(D\\) space<br>\\[<br>\n\\boldsymbol{y}=\\boldsymbol{W} \\boldsymbol{z}+\\boldsymbol{\\mu}<br>\\]</li>\n<li>Sample noise<br>\\[<br>\n\\epsilon \\sim \\mathcal{N}\\left(\\mathbf{0}, \\sigma^{2} \\boldsymbol{I}\\right)<br>\\]</li>\n<li>Add noise to obtain \\(\\mathbf{x}\\)<br>\\[<br>\n\\boldsymbol{x}=\\boldsymbol{y}+\\boldsymbol{\\epsilon}<br>\\]</li>\n</ol>\n</div></div><p><span>\n\n\n</span><img src=\"12wgBdw7BJtcqzLcw4vx.png\" style=\"width: 366px;\"><br></p><p><br></p><p><b>Intuition:</b></p><p>Intuitiv kann man p(x) als isotropische Gauss'sche Sprühdose betrachten, die man über die man über den principle subspace verschiebt, die Gauss'sche Tinte verteilt mit einer Dichte die von sigma^2 und gewichtet mit der prior-Verteilung.</p><p><br></p>"
            ],
            "guid": "MaZm8{q-Ti",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>How does the EM algorithm for probabilistic PCA work?</p>",
                "<p><b>Initialize</b>: Use average of \\(\\boldsymbol{x}\\) for \\(\\boldsymbol{\\mu}\\), random matrix \\(\\boldsymbol{W}\\)</p><p><br></p><p>Repeat until convergence:</p><p><b>Expectation step:</b></p><p>Compute posterior mean and covariance:</p><p>\\(\\boldsymbol{\\mu}_{\\boldsymbol{z} \\mid \\boldsymbol{x}_{i}}=\\left(\\boldsymbol{W}^{T} \\boldsymbol{W}+\\sigma^{2} \\boldsymbol{I}\\right)^{-1} \\boldsymbol{W}^{T} \\boldsymbol{x}_{i}, \\quad \\boldsymbol{\\Sigma}_{\\boldsymbol{z} \\mid \\boldsymbol{x}_{i}}=\\sigma^{2}\\left(\\boldsymbol{W}^{T} \\boldsymbol{W}+\\sigma^{2} \\boldsymbol{I}\\right)^{-1}\\)<br></p><p>Generate latent samples:</p><p>\\(\\boldsymbol{z}_{i} \\sim \\mathcal{N}\\left(\\boldsymbol{\\mu}_{\\boldsymbol{z} \\mid \\boldsymbol{x}_{i}}, \\mathbf{\\Sigma}_{z \\mid \\boldsymbol{x}_{i}}\\right)\\)<br></p><p><b>Maximization step:</b>&nbsp;Update \\(\\boldsymbol{W}, \\boldsymbol{\\mu}\\) and \\(\\sigma^{2}\\)</p><p>\\(\\left[\\begin{array}{c}\\boldsymbol{\\mu} \\\\ \\boldsymbol{W}\\end{array}\\right]=\\left(\\boldsymbol{Z}^{T} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{T} \\boldsymbol{X}, \\quad \\sigma^{2}=\\frac{1}{n d} \\sum_{i=1}^{n} \\sum_{k=1}^{d}\\left(y_{i k}-x_{i k}\\right)^{2}\\)<br></p><p><br></p><p><br></p>"
            ],
            "guid": "A}lcjxSk@F",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p><span>\n\nVergleiche&nbsp;<strong>probabilistic PCA</strong>&nbsp;mit&nbsp;<strong>PCA</strong>\n\n</span><br></p>",
                "<p><span>\n\n<p style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\">PCA mit Eigenvektor-Zerlegung wird bevorzugt, weil:</p><ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li>man eine Lösung in einem Schritt hat</li><li>sie schnell ist</li></ul><p style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><em>EM for dimensionality reduction</em>&nbsp;ist sinnvoll, wenn:</p><ul style=\"font-weight:400;letter-spacing:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px\"><li>man eine Dichteverteilung braucht</li><li>hilft beim Verständnis von EM</li><li>gut fürs Verständnis von komplexen&nbsp;<em>dimensionality reduction</em>&nbsp;Techniken</li></ul>\n\n</span></p>"
            ],
            "guid": "t`A6=T8cWb",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<p>Vervollständige folgendes Bild:</p><p><img src=\"1CVoqiVLK6idiaz6MDrP.png\" style=\"width: 366px;\"><br></p>",
                "<p><img src=\"12wEbPAvWMvR4QxaRYe8.png\" style=\"width: 366px;\"><br></p>"
            ],
            "guid": "AfCt?{/SVD",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Aus welchen <b>Komponenten </b>besteht ein <b>Bayesian Linear Regression</b>-Modell? Was ist jeweils die Bedeutung?",
                "\\[p(\\boldsymbol{w} \\mid \\boldsymbol{X}, \\boldsymbol{y})=\\frac{p(\\boldsymbol{y} \\mid \\boldsymbol{X}, \\boldsymbol{w}) p(\\boldsymbol{w})}{p(\\boldsymbol{y} \\mid \\boldsymbol{X})}=\\frac{p(\\boldsymbol{y} \\mid \\boldsymbol{X}, \\boldsymbol{w}) p(\\boldsymbol{w})}{\\int p(\\boldsymbol{y} \\mid \\boldsymbol{X}, \\boldsymbol{w}) p(\\boldsymbol{w}) d \\boldsymbol{w}}\\]<br><br><div><br></div><div>Posterior of Model Parameters: \\(p(w \\mid y, X)\\)</div><div><br></div><div>Likelihood of the Response Features given the Model and Predictors Features:</div><div>\\[p(\\boldsymbol{y} \\mid \\boldsymbol{X}, \\boldsymbol{w})=\\prod_{i} \\mathcal{N}\\left(y_{i} \\mid \\boldsymbol{w}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right), \\sigma^{2}\\right)=\\underbrace{\\mathcal{N}\\left(\\boldsymbol{y} \\mid \\boldsymbol{\\Phi} \\boldsymbol{w}, \\sigma^{2} \\boldsymbol{I}\\right)}_{\\text {Multivariate distribution }},\\]</div><div><br></div><div>where \\(\\boldsymbol{w}^{T}\\)&nbsp; is a linear model, \\(\\sigma^2\\) is the noise variance and \\(\\boldsymbol{\\Phi}\\) the feature matrix.</div><div><br></div><div>Prior Probability of Model Parameters: \\(p(w)=\\mathcal{N}\\left(w \\mid \\mathbf{0}, \\lambda^{-1} \\boldsymbol{I}\\right)\\)</div><div><br></div><div>Evidence / Normalizer: \\(p(y, X)\\)</div><div><br></div><div><br></div>"
            ],
            "guid": "f]Ua*OgCNo",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "09_bayesian_learning"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Welche beiden Schritte sind notwendig bei <b>Bayesian Linear Regression</b>?",
                "<b>compute posterior:</b><br><div><br>\\[p(\\boldsymbol{w} \\mid \\boldsymbol{X}, \\boldsymbol{y})=\\frac{p(\\boldsymbol{y} \\mid \\boldsymbol{X}, \\boldsymbol{w}) p(\\boldsymbol{w})}{p(\\boldsymbol{y} \\mid \\boldsymbol{X})}=\\frac{p(\\boldsymbol{y} \\mid \\boldsymbol{X}, \\boldsymbol{w}) p(\\boldsymbol{w})}{\\int p(\\boldsymbol{y} \\mid \\boldsymbol{X}, \\boldsymbol{w}) p(\\boldsymbol{w}) d \\boldsymbol{w}}\\]<br><br></div><div><b>compute predictive distribution:</b>&nbsp;</div><div><br></div><div>Integrate posterior out:</div><div><br></div>\\[p\\left(y^{*} \\mid \\boldsymbol{x}^{*}, \\boldsymbol{X}, \\boldsymbol{y}\\right)=\\int p\\left(y^{*} \\mid \\boldsymbol{w}, \\boldsymbol{x}^{*}\\right) p(\\boldsymbol{w} \\mid \\boldsymbol{X}, \\boldsymbol{y}) d \\boldsymbol{w},\\]<br><div><br></div><div>where \\(p(\\boldsymbol{w} \\mid \\boldsymbol{X}, \\boldsymbol{y})\\) is the posterior and \\(p\\left(y^{*} \\mid \\boldsymbol{w}, \\boldsymbol{x}^{*}\\right)\\) is the parameter-specific prediction.</div><div><br></div>"
            ],
            "guid": "wko?{DEk%7",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "09_bayesian_learning"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Wie berechnet sich der <b>Posterior</b> \\(p(\\boldsymbol{w} \\mid \\boldsymbol{X}, \\boldsymbol{y})\\) unter Annahme von Normalverteilung bei <b>Bayesian Linear Regression</b>?&nbsp;<div><br></div><div>Gehen Sie dabei auf den <b>Posterior Mean</b>&nbsp;\\(\\boldsymbol{\\mu}_{\\boldsymbol{w} \\mid \\boldsymbol{X}, \\boldsymbol{y}}\\) und <b>Posterior Covariance </b>\\(\\boldsymbol{\\Sigma}_{\\boldsymbol{w} \\mid \\boldsymbol{X}, \\boldsymbol{y}}\\) ein.</div>",
                "Posterior&nbsp;<div><br></div><div>\\[p(\\boldsymbol{w} \\mid \\boldsymbol{X}, \\boldsymbol{y})=\\mathcal{N}\\left(\\boldsymbol{w} \\mid \\boldsymbol{\\mu}_{\\boldsymbol{w} \\mid \\boldsymbol{X}, \\boldsymbol{y}}, \\boldsymbol{\\Sigma}_{\\boldsymbol{w} \\mid \\boldsymbol{X}, \\boldsymbol{y}}\\right)\\]<div><br>Posterior mean:&nbsp;</div><div>\\[\\boldsymbol{\\mu}_{\\boldsymbol{w} \\mid \\boldsymbol{X}, \\boldsymbol{y}}=\\left(\\boldsymbol{\\Phi}^{T} \\boldsymbol{\\Phi}+\\sigma_{\\boldsymbol{y}}^{2} \\lambda \\boldsymbol{I}\\right)^{-1} \\boldsymbol{\\Phi}^{T} \\boldsymbol{y}\\]</div><div><br>Posterior covariance:&nbsp;</div><div><br></div><div>\\[\\boldsymbol{\\Sigma}_{\\boldsymbol{w} \\mid \\boldsymbol{X}, \\boldsymbol{y}}=\\sigma_{\\boldsymbol{y}}^{2}\\left(\\boldsymbol{\\Phi}^{T} \\boldsymbol{\\Phi}+\\sigma_{\\boldsymbol{y}}^{2} \\lambda \\boldsymbol{I}\\right)^{-1}\\]</div></div>"
            ],
            "guid": "etT^4OHN#~",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "09_bayesian_learning"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Welchen Einfluss hat das Vorhandensein von mehr <b>Trainingspunkten</b> bei Bayesian Learning auf die <b>Unsicherheit</b> der <b>Prognose</b>?",
                "<br><img src=\"28441879.png\">"
            ],
            "guid": "k3vx]h[m%v",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "09_bayesian_learning"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Wie berechnet sich die <b>Predictive Distribution</b> \\(p(\\boldsymbol{w} \\mid \\boldsymbol{X}, \\boldsymbol{y})\\) unter Annahme von Normalverteilung bei <b>Bayesian Linear Regression</b>?",
                "\\[\\begin{aligned}<br>p\\left(y^{*} \\mid x^{*}, \\boldsymbol{X}, \\boldsymbol{y}\\right) &amp;=\\int p\\left(y^{*} \\mid \\boldsymbol{w}, \\boldsymbol{x}^{*}\\right) p(\\boldsymbol{w} \\mid \\boldsymbol{X}, \\boldsymbol{y}) d \\boldsymbol{w} \\\\<br>&amp;=\\int \\mathcal{N}\\left(y_{*} \\mid \\boldsymbol{\\phi}_{*}^{T} \\boldsymbol{w}, \\sigma_{\\boldsymbol{y}}^{2}\\right) \\mathcal{N}\\left(\\boldsymbol{w} \\mid \\boldsymbol{\\mu}_{\\boldsymbol{w} \\mid \\boldsymbol{X}, \\boldsymbol{y}}, \\boldsymbol{\\Sigma}_{\\boldsymbol{w} \\mid \\boldsymbol{X}, y}\\right) d \\boldsymbol{w}<br>\\end{aligned}\\]<br><div><br></div><div><b>Mittelwert:</b><br>\\[\\mu\\left(\\boldsymbol{x}^{*}\\right)=\\phi\\left(\\boldsymbol{x}^{*}\\right)^{T}\\left(\\boldsymbol{\\Phi}^{T} \\boldsymbol{\\Phi}+\\lambda \\sigma_{y}^{2} \\boldsymbol{I}\\right)^{-1} \\boldsymbol{\\Phi}^{T} \\boldsymbol{y}\\]<br><br></div><div><b>Varianz:</b></div><div><br></div><div>\\[\\quad \\sigma^{2}\\left(x^{*}\\right)=\\sigma_{y}^{2}\\left(1+\\phi\\left(\\boldsymbol{x}^{*}\\right)^{T}\\left(\\boldsymbol{\\Phi}^{T} \\boldsymbol{\\Phi}+\\lambda \\sigma_{y}^{2} \\boldsymbol{I}\\right)^{-1} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}^{*}\\right)\\right)\\]</div><div><br></div><div><b>Intutition:</b></div><div><br></div><div>Mittelwert ist derselbe wie bei ridge regression. Jedoch hängt die Varianz von \\(x^{*}\\) ab.</div><div><br></div>"
            ],
            "guid": "FDh(ftJw8q",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "09_bayesian_learning"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Wie ist die <b>Kullback-Leibler Divergence</b> definiert?",
                "\\[\\mathrm{KL}(q(\\boldsymbol{x}) \\| p(\\boldsymbol{x}))=\\sum_{\\boldsymbol{x}} q(\\boldsymbol{x}) \\log \\frac{q(\\boldsymbol{x})}{p(\\boldsymbol{x})}\\]"
            ],
            "guid": "GJP!Muw&US",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Nennen Sie 3 Eigenschaften der <b>Kullback-Leibler Divergence</b>",
                "<div><div><div><div></div><div></div></div></div>\n</div>\n<div>\n<div><ul>\n<li>Its always non-negative:&nbsp;\\(\\mathrm{KL}(q \\| p) \\geq 0\\)</li>\n<li>If its zero, both distributions are the same:&nbsp;\\(\\mathrm{KL}(q \\| p)=0 \\Longleftrightarrow q=p\\)</li>\n<li>It is non-symmetric:&nbsp;\\(\\mathrm{KL}(q \\| p) \\neq \\mathrm{KL}(p \\| q)\\)</li>\n</ul>\n</div></div>"
            ],
            "guid": "sk+8%%qqN?",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Name 2 types of random variables, that can be distinguished.",
                "<div><div><ul><li>discrete random variables</li><li>continous random variables</li></ul></div></div>"
            ],
            "guid": "LeR7hC3u`c",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "00_introduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give the definition of a scalar / inner product for&nbsp; the vectors:<div><br></div><div>\\(\\boldsymbol{v}=\\left[\\begin{array}{l}<br>1 \\\\<br>2 \\\\<br>4<br>\\end{array}\\right], \\quad \\boldsymbol{w}=\\left[\\begin{array}{l}<br>2 \\\\<br>4 \\\\<br>8<br>\\end{array}\\right]\\)<br></div>",
                "\\(\\langle\\boldsymbol{v}, \\boldsymbol{w}\\rangle=1 \\cdot 2+2 \\cdot 4+4 \\cdot 8=42\\)"
            ],
            "guid": "P@h?k5P,it",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "00_introduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the <b>transpose </b>of the the <b>matrix product</b>?&nbsp;\\((\\boldsymbol{V} \\boldsymbol{W})^{T}\\)",
                "\\((\\boldsymbol{V} \\boldsymbol{W})^{T}=\\boldsymbol{W}^{T} \\boldsymbol{V}^{T}\\)"
            ],
            "guid": "IE1!_pZ&xO",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "00_introduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Is a matrix product <b>commutative</b> or <b>associative</b>?",
                "<b>Associative:</b><div>\\(\\boldsymbol{X}(\\boldsymbol{M} \\Lambda)=(\\boldsymbol{X} \\boldsymbol{M}) \\Lambda\\)<br></div><div><br></div><div><b>Non-commutative:</b></div><div>\\(\\boldsymbol{V}(\\boldsymbol{W} \\boldsymbol{X})=(\\boldsymbol{V} \\boldsymbol{W}) \\boldsymbol{X}\\)</div>"
            ],
            "guid": "Xp0e,y+u{",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "00_introduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How can one calculate the row / column averages of a matrix?",
                "<b>row average</b><div>\\[\\left[\\begin{array}{c}<br>\\frac{1}{m} \\sum_{i=1}^{m} X_{1, i} \\\\<br>\\vdots \\\\<br>\\frac{1}{m} \\sum_{i=1}^{m} X_{n, i}<br>\\end{array}\\right]=\\boldsymbol{X}\\left[\\begin{array}{c}<br>\\frac{1}{m} \\\\<br>\\vdots \\\\<br>\\frac{1}{m}<br>\\end{array}\\right]=\\boldsymbol{X} \\boldsymbol{a}, \\quad \\text { with } \\boldsymbol{a}=\\left[\\begin{array}{c}<br>\\frac{1}{m} \\\\<br>\\vdots \\\\<br>\\frac{1}{m}<br>\\end{array}\\right]\\]<br></div><div><b><br></b></div><div><b>column average</b></div><div><br></div><div>\\[\\left[\\frac{1}{n} \\sum_{i=1}^{n} X_{i, 1}, \\ldots, \\frac{1}{n} \\sum_{i=1}^{n} X_{i, m}\\right]=\\left[\\frac{1}{n}, \\ldots, \\frac{1}{n}\\right] \\boldsymbol{X}=\\boldsymbol{b}^{T} \\boldsymbol{X}, \\text { with } \\boldsymbol{b}=\\left[\\begin{array}{c}<br>\\frac{1}{n} \\\\<br>\\vdots \\\\<br>\\frac{1}{n}<br>\\end{array}\\right]\\]<br></div>"
            ],
            "guid": "cK/}i]7[5b",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "00_introduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How does the <b>identity</b> of a matrix / scalar relate to its <b>inverse</b>?",
                "<div><b>scalar case:</b></div>\\(w \\cdot w^{-1}=1\\)<div><br></div><div><b>matrix case:</b></div><div>\\(\\boldsymbol{W} \\boldsymbol{W}^{-1}=\\boldsymbol{I}, \\quad \\boldsymbol{W}^{-1} \\boldsymbol{W}=\\boldsymbol{I}\\)<br></div>"
            ],
            "guid": "dUDy/X<!%{",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "00_introduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How can one calculate the <b>derivative </b>of a <b>scalar / vector</b>?",
                "<b>derivative case</b><div>\\(\\frac{\\partial f(x)}{\\partial x}=g\\)<br></div><div><br></div><div><b>vector case</b><br><div>\\(\\nabla f(\\boldsymbol{x}) = \\frac{\\partial f(x)}{\\partial x}=\\left[\\frac{\\partial f(x)}{\\partial x_{1}}, \\ldots, \\frac{\\partial f(x)}{\\partial x_{d}}\\right]^{T}\\)<br></div><div><br></div></div>"
            ],
            "guid": "FBrW_Un{6E",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "00_introduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give the derivative of&nbsp;<div>\\(\\nabla_{\\boldsymbol{x}} \\boldsymbol{A x}\\)<br></div><div><br></div><div>\\(\\nabla_{\\boldsymbol{x}} \\boldsymbol{x}^{T} \\boldsymbol{x}\\)<br></div><div><br></div><div>\\(\\nabla_{\\boldsymbol{x}} \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x}\\)<br></div>",
                "<div>\\(\\nabla_{\\boldsymbol{x}} \\boldsymbol{A} \\boldsymbol{x}=\\boldsymbol{A}^{T}\\)<br></div><div><br></div><div><div>\\(\\nabla_{\\boldsymbol{x}} \\boldsymbol{x}^{T} \\boldsymbol{x}=2 \\boldsymbol{x}\\)<br></div><div><br></div><div>\\(\\nabla_{\\boldsymbol{x}} \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x}=2 \\boldsymbol{A} \\boldsymbol{x}\\)</div></div><div><br></div>"
            ],
            "guid": "e1)YzArnEP",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "00_introduction",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the properties of a <b>kernel matrix</b>?",
                "\\(K\\) is always an \\(n \\times n\\) matrix, whatever the nature of data e. g. vectors strings etc. is."
            ],
            "guid": "jHBZmfH|rP",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "07_kernel_methods"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Was sind die <b>Vor- und Nachteile</b> von <b>Kernel Ridge Regression</b>?",
                "<br><div><div>Die Lösung für Kernel Ridge Regression ist gegeben durch:</div><div>\\(f^{*}(\\boldsymbol{x})=\\boldsymbol{k}(\\boldsymbol{x})^{T}(\\boldsymbol{K}+\\lambda \\boldsymbol{I})^{-1} \\boldsymbol{y})\\)<b><br></b></div><div><strong><br></strong></div><div><strong>Vorteile</strong></div><ul><li>Keine Auswertung der Feature Vektoren erforderlich</li><li>Lediglich der Skalarprodukte der Inputvektoren (ausgewertetet durch den Kernel)</li><li>Nur wenige Hyperparameter notwendig</li></ul><div><strong>Nachteile</strong></div><ul><li>Erfordert die Invertierung einer \\(N \\times N\\) Matrix, was teuer ist \\(\\mathcal{O}(n^{2.376})\\)</li><li>Alle Samples müssen in einer Kernel-basierten Methode gespeichert werden, was sich aus Größe der Kernel-Matrix begründen lässt.</li></ul></div>"
            ],
            "guid": "s7~-x[_yT]",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "07_kernel_methods"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How can the <b>binary classification</b> problem be rephrased for the <b>SVM</b>?",
                "Given the training data \\(\\left(\\boldsymbol{x}_{i}, y_{i}\\right), \\mathrm{i}=1 \\ldots \\mathrm{N}\\), with \\(x_{i} \\in \\mathbb{R}^{d}\\) and \\(y_{i} \\in\\{-1,1\\}\\)<br>learn a classifier \\(f(\\boldsymbol{x})\\) such that:<br><br>\\[f\\left(\\boldsymbol{x}_{i}\\right)= \\begin{cases}&gt;0, &amp; \\text { if } y_{i}=1 \\\\ &lt;0, &amp; \\text { if } y_{i}=-1\\end{cases}\\]<div><br></div><div>Or: \\(f\\left(\\boldsymbol{x}_{i}\\right) y_{i}&gt;0\\) for a correct classification</div><div>E. g. if \\(y_i = -1\\), \\(f(x_i) = 1\\) would tell that the classification is incorrect.</div>"
            ],
            "guid": "M&%Bhy=1RP",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How can one calculate the scalar projection (\\(a_b\\)) of \\(\\boldsymbol{a}\\) on \\(\\boldsymbol{b}\\)?<div><img src=\"paste-34def274df302ae886b95f6bb9cf90cf9f317069.jpg\"><br></div>",
                "\\(a_{b}=\\|\\boldsymbol{a}\\| \\cos \\theta=\\frac{\\boldsymbol{a}^{\\boldsymbol{T}} \\boldsymbol{b}}{\\|\\boldsymbol{b}\\|}\\)"
            ],
            "guid": "E}u1sk0$Y<",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What does the Maximum Margin principle do?",
                "Margin should be maximized in way in a way that the <b>minimum distance </b>between the decision boundary and examples are maximized.<div><br></div><div><b>Intuition:</b></div><div>Less examples close to the decision boundary&nbsp;→ less uncertainty</div>"
            ],
            "guid": "IihtK[(Eo)",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give a definition for the support vector.",
                "Data points close to the the decision boundary."
            ],
            "guid": "L@/!IVyN0+",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give a definition for the margin \\(\\rho\\).",
                "Margin is the distance between the <b>support vectors</b> of the <b>decision boundary</b>."
            ],
            "guid": "kw4o<fGnw",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Why is it inconvient to solve the following optimization for SMV?<div><br></div><div>\\[\\begin{aligned}<br>\\operatorname{argmax}_{\\mathbf{w}} \\frac{2}{\\|\\mathbf{w}\\|}, \\\\<br>\\text { s.t. } \\quad &amp; \\mathbf{w}^{T} \\mathbf{x}_{i}+b\\left\\{\\begin{array}{l}<br>\\geq+1, &amp; \\text { falls } y_{i}=+1 \\\\<br>\\leq-1, &amp; \\text { falls } y_{i}=-1<br>\\end{array}\\right.<br>\\end{aligned}\\]<br></div>",
                "The above is hard to solve. It's easier to solve it as a quadratic optimization problem."
            ],
            "guid": "Dbo$]gml9@",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give (former) applications of SVMs",
                "<div>\n<div><ul>\n<li>text  (and hypertext-) categorization</li>\n<li>image classification</li>\n<li>bioinformatics (Protein classification, cancer classification)</li>\n<li>hand-written character recognition</li>\n</ul>\n</div></div>"
            ],
            "guid": "MD*9;1{59F",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the motiviation for&nbsp;<b>sub gradients</b>?",
                "One can calculate the gradient of a convex function, even if the function is itself not differentiable."
            ],
            "guid": "y1v:NZJfAf",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What condition holds for the&nbsp;<b>sub-gradient</b>?",
                "\\(f\\) is a convex function mapping \\(\\mathbb{R}^{d} \\rightarrow \\mathbb{R}\\).<div><br></div><div>A&nbsp; subgradient at any point \\(\\boldsymbol{x}\\) is any \\(\\boldsymbol{g}\\) (aka the subgradient) such that:</div><div><br></div><div>\\(f(\\boldsymbol{z}) \\geq f(\\boldsymbol{x})+\\boldsymbol{g}^{T}(\\boldsymbol{z}-\\boldsymbol{x})\\)<br></div><div><br></div><div>If \\(f\\) is differntiable at \\(\\boldsymbol{x}\\), then \\(\\boldsymbol{g}=\\nabla f(\\boldsymbol{x})\\).&nbsp;</div>"
            ],
            "guid": "jb$sds+Ksg",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give an example for a <b>subgradient</b>.",
                "Consider&nbsp;\\(f(x)=|x|\\).<div><br></div><div><img src=\"paste-d08f51da828bd4e295b9886a97a9be9599bf9ec5.jpg\"><br></div><div>(left function; right its subgradient)</div><div><br></div><div><b>Explanation:</b></div><div>For \\(x \\neq 0\\), unique sub-gradient of \\(g=\\operatorname{sign}(x)\\).<br></div><div><br>For \\(x=0\\), sub-gradient is any element of \\([-1,1]\\).<br></div>"
            ],
            "guid": "P_1E?GpwY,",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain what the motivation is for using sub-gradients in conjunction with gardient descent.",
                "Gradient descent requires the function to be differentiable."
            ],
            "guid": "wv%Hl:s^a2",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain how <b>sub-gradients</b> can be used with <b>gradient descent</b>.",
                "<div>\n<div><ul>\n<li>Given convex \\(f\\), not necessarily differentiable</li>\n<li>Initialize \\(\\boldsymbol{x}_{0}\\)</li>\n<li>Repeat: \\(\\boldsymbol{x}_{t+1}=\\boldsymbol{x}_{t}+\\eta \\boldsymbol{g}\\), where \\(\\boldsymbol{g}\\) is any sub-gradient of \\(f\\) at point \\(\\boldsymbol{x}_{t}\\).</li>\n</ul>\n</div></div>"
            ],
            "guid": "N@%8a&(A6G",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain how <b>sub-gradients</b> can be applied to SVMs for minimizing the <b>hinge loss</b> using <b>gradient descent</b>.",
                "\\[\\operatorname{argmin}_{\\mathbf{w}} \\quad C \\underbrace{\\sum_{i=1}^{N} \\max \\left(0,1-y_{i} f\\left(\\boldsymbol{x}_{i}\\right)\\right)}_{\\text {loss function }}+\\underbrace{\\|\\mathbf{w}\\|^{2}}_{\\text {regularization }}\\]<div><br></div><div>\n<div><div>At each iteration, pick random training sample&nbsp;\\(\\left(\\boldsymbol{x}_{i}, y_{i}\\right)\\)</div>\n<ul>\n<li>If \\(y_{i} f\\left(\\boldsymbol{x}_{i}\\right)&lt;1: \\quad \\boldsymbol{w}_{t+1}=\\boldsymbol{w}_{t}-\\eta\\left(2 \\boldsymbol{w}_{t}-C y_{i} \\boldsymbol{x}_{i}\\right)\\)</li>\n<li>Otherwise: \\(\\boldsymbol{w}_{t+1}=\\boldsymbol{w}_{t}-\\eta 2 \\boldsymbol{w}_{t} + 0\\)</li>\n</ul><div><br></div><div>As subgradients of \\(\\max \\left(0,1-y_{i} f\\left(\\mathbf{x}_{i}\\right)\\right)\\) are:</div><div><img src=\"paste-32f91c27bffa66c78bba9a3f73283b74dea8518d.jpg\"><br></div>\n</div></div>"
            ],
            "guid": "l,WXN>6=Et",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Compare the (SVM) hinge loss to the logistic loss.",
                "<b>SVM (hinge) loss:</b><div>\\(\\max \\left(0,1-y_{i} f\\left(\\boldsymbol{x}_{i}\\right)\\right)\\)<br></div><div><div><div><ul><li>Outputs -1 or 1</li><li>Estimates maximum margin solution</li><li>Loss contribution is 0 for correct classification</li></ul></div></div></div><div><b>logistic loss:</b></div><div>\\(\\log \\left(1+\\exp \\left(-y_{i} f\\left(\\boldsymbol{x}_{i}\\right)\\right)\\right)\\)</div><div><div><div><div><ul><li>Outputs probabilities</li><li>Contribution never 0<ul><li>Often results in slightly less accurate classification</li></ul></li><br><li>Diverges faster than hinge loss<ul><li>More sensitive to outliers</li></ul></li></ul></div></div></div><div><br></div><div><img src=\"paste-cc573063e4172324c94f74334aa8b150d2e734cb.jpg\"><br></div></div>"
            ],
            "guid": "B{o1Rx)wZQ",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Using which technique can the <b>hinge loss</b> (SVMs) be minimized?",
                "Using gradient descent."
            ],
            "guid": "cFN@~Y&A>s",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Name three important <b>properties</b> of the <b>hinge loss </b>(SVMs<b>)</b>.",
                "<div>\n<div><ul>\n<li>not differentiable</li>\n<li>convex</li>\n<li>one local minimum exists</li>\n</ul>\n</div></div>"
            ],
            "guid": "LMn!g1M*(r",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give a definitition of the <b>hinge loss</b> (SVMs). Make sure to explain its components.",
                "\\[\\operatorname{argmin}_{\\mathbf{w}} \\quad C \\underbrace{\\sum_{i=1}^{N} \\max \\left(0,1-y_{i} f\\left(\\boldsymbol{x}_{i}\\right)\\right)}_{\\text {loss function }}+\\underbrace{\\|\\mathbf{w}\\|^{2}}_{\\text {regularization }}\\]<div><br></div><div><img src=\"paste-c14aae6b9bd97fc81ea48fde4fc70b7e605c982d.jpg\"><br></div>"
            ],
            "guid": "wvq>{-K:5l",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Consider a classifier \\(f(x)\\) and a class true class&nbsp;\\(y_{i} \\in\\{-1,1\\}\\) and \\(x_i\\) a sample. Explain three different variants how points can contribute to the hinge loss of a SVM.",
                "<div>\n<div><ul>\n<li>\\(y_{i} f\\left(\\boldsymbol{x}_{i}\\right)&gt;1\\): Point outside margin, no contribution to loss</li>\n<li>\\(y_{i} f\\left(\\boldsymbol{x}_{i}\\right)=1\\): Point is on the margin, no contribution to loss as in hard margin</li>\n<li>\\(y_{i} f\\left(\\boldsymbol{x}_{i}\\right) \\leq 1\\): Point violates the margin, contributes to loss</li>\n</ul>\n</div></div>"
            ],
            "guid": "nC.y@azg)&",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How can overfitting be controlled for SVMs?",
                "<div>\n<div><ul>\n<li>Setting \\(C\\) (a lower \\(C\\) leads to smaller complexity)</li>\n<li>varying the kernel</li>\n<li>Varying parameters of the kernel (bandwith etc.)</li>\n</ul>\n</div></div>"
            ],
            "guid": "jMNcfc;?Q@",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Should one worry about overfitting with SVMs when using kernels in huge feature spaces?",
                "Even though the SVM is an extension to the maximum margin classifier and the maximium margin leads generally to a good generalization, everything can overfit."
            ],
            "guid": "HMz@+]UEE^",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give the <b>primal formulation</b>&nbsp;of a SVM.",
                "\\[\\begin{aligned}<br>\\operatorname{argmin}_{\\mathbf{w}, \\boldsymbol{\\xi}} &amp;\\|\\mathbf{w}\\|^{2}+C \\sum_{i}^{N} \\xi_{i}, \\\\<br>\\text { s.t. } &amp; y_{i}\\left(\\mathbf{w}^{T} \\mathbf{x}_{i}+b\\right) \\geq 1-\\xi_{i}, \\quad \\xi_{i} \\geq 0<br>\\end{aligned}\\]"
            ],
            "guid": "H1r,37sfj9",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give the&nbsp;<b>dual formulation</b>&nbsp;of a SVM.",
                "\\[<br>\\begin{aligned}<br>&amp;\\max _{\\boldsymbol{\\lambda}} \\sum_{i} \\lambda_{i}-\\frac{1}{2} \\sum_{i} \\sum_{j} \\lambda_{i} \\lambda_{j} y_{i} y_{j} \\boldsymbol{k}\\left(\\boldsymbol{x}_{i}, \\boldsymbol{x}_{j}\\right) \\\\<br>&amp;\\text { s.t. } C \\geq \\lambda_{i} \\geq 0, \\forall i \\in[1 \\ldots N], \\quad \\sum_{i} \\lambda_{i} y_{i}=0<br>\\end{aligned}<br>\\]"
            ],
            "guid": "ij!+`@Q:UN",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm",
                "checklater",
                "formula_sheet"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain how one can derive the dual form of the SVM. Do not consider slack variables.",
                "First we have to introduce the <b>feature space</b> \\(\\phi (x_i)\\) by replacing \\(x_i\\).<div><br></div><div>\\(\\begin{aligned}<br>\\operatorname{argmin}_{\\mathbf{w}} &amp;\\|\\mathbf{w}\\|^{2}, \\\\<br>\\text { s.t. } &amp; y_{i}\\left(\\mathbf{w}^{T} \\phi\\left(\\mathbf{x}_{i}\\right)+b\\right) \\geq 1<br>\\end{aligned}\\)<br><div><br></div></div><div>Define <b>Langraganian</b> as:</div><div>\\(\\operatorname{argmin}_{\\mathbf{w}}\\|\\mathbf{w}\\|^{2}, \\quad \\text { s.t. } \\quad y_{i}\\left(\\mathbf{w}^{T} \\phi\\left(\\mathbf{x}_{i}\\right)+b\\right) \\geq 1\\)<br></div><div><br></div><div>Compute the optimal \\(w\\) as:</div><div>\\(\\begin{gathered}<br>L(\\boldsymbol{w}, \\boldsymbol{\\lambda})=\\frac{1}{2} \\boldsymbol{w}^{T} \\boldsymbol{w}-\\sum_{i} \\lambda_{i}\\left(y_{i}\\left(\\boldsymbol{w}^{T} \\phi\\left(\\boldsymbol{x}_{i}\\right)+b\\right)-1\\right) \\\\<br>\\frac{\\partial L}{\\partial \\boldsymbol{w}}=\\boldsymbol{w}-\\sum_{i} \\lambda_{i} y_{i} \\phi\\left(x_{i}\\right)=0 \\\\<br>\\boldsymbol{w}^{*}=\\sum_{i} \\lambda_{i} y_{i} \\phi\\left(x_{i}\\right)<br>\\end{gathered}\\)<br></div><div><br></div><div>Note that many of the \\(\\lambda\\) will be 0. This is the case when the constraint is fullfilled. If it's not 0, \\(\\phi (x_i)\\) is a support vector. Therefore, the optimal \\(w\\) is combination of the support variables.</div><div><br></div><div>Find the optimal \\(b\\):</div><div><br></div><div>\\(L(\\boldsymbol{w}, \\boldsymbol{\\lambda})=\\frac{1}{2} \\boldsymbol{w}^{T} \\boldsymbol{w}-\\sum_{i} \\lambda_{i}\\left(y_{i}\\left(\\boldsymbol{w}^{T} \\phi\\left(x_{i}\\right)+b\\right)-1\\right)\\)<br></div><div><br></div><div>\\(\\frac{\\partial L}{\\partial b}=-\\sum_{i} \\lambda_{i} y_{i} \\Rightarrow \\sum_{i} \\lambda_{i} y_{i}=0\\)<br></div><div><br></div><div>One gets no optimal solution for \\(b\\), but an additional constraint.&nbsp;</div><div><br></div><div>Define <b>Langranian</b> as:</div><div>\\(L(\\boldsymbol{w}, \\boldsymbol{\\lambda})=\\frac{1}{2} \\boldsymbol{w}^{T} \\boldsymbol{w}-\\sum_{i} \\lambda_{i}\\left(y_{i}\\left(\\boldsymbol{w}^{T} \\phi\\left(x_{i}\\right)+b\\right)-1\\right), \\quad \\boldsymbol{w}^{*}=\\sum_{i} \\lambda_{i} y_{i} \\phi\\left(\\boldsymbol{x}_{i}\\right)\\)<br></div><div><br></div><div><br></div><div>Plug in \\(w^{*}\\).</div><div><br></div><div><b>Dual function:</b></div><div>\\(\\begin{aligned}<br>g(\\boldsymbol{\\lambda}) &amp;=L\\left(\\boldsymbol{w}^{*}, \\boldsymbol{\\lambda}\\right) \\\\<br>&amp;=\\frac{1}{2} \\underbrace{\\sum_{i} \\sum_{j} \\lambda_{i} \\lambda_{j} y_{i} y_{j} \\phi\\left(x_{i}\\right)^{T} \\phi\\left(\\boldsymbol{x}_{j}\\right)}_{w^{* T} w^{*}}-\\sum_{i} \\lambda_{i} y_{i}(\\underbrace{\\sum_{j} \\lambda_{j} y_{j} \\phi\\left(x_{j}\\right)}_{w^{*}})^{T} \\phi\\left(x_{i}\\right)+\\sum_{i} \\lambda_{i} \\\\<br>&amp;=\\sum_{i} \\lambda_{i}-\\frac{1}{2} \\sum_{i} \\sum_{j} \\lambda_{i} \\lambda_{j} y_{i} y_{j} \\phi\\left(\\boldsymbol{x}_{i}\\right)^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{j}\\right)<br>\\end{aligned}\\)<br></div><div><br></div>"
            ],
            "guid": "K7aWH;(P>/",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm",
                "checklater",
                "formula_sheet"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the impact of \\(b\\) for a SVM?",
                "\\(b\\) shifts the decision boundary along the (negative direction of \\(w\\)).<div><br></div><div><img src=\"paste-0bb0d6856d32a0b2dcf81b4459ce1ed966334f08.jpg\"><br></div>"
            ],
            "guid": "luadd|DW;_",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Why is the optimal hyperplane alway in the centre of two support vectors?",
                "If it was not, one would find a hyperplane were the distance between the closest point and the hyperplane was larger. Thus, this can not be hyperplane giving the maximum margin."
            ],
            "guid": "fFZg:F5j2@",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How can one calculate the width of the <b>margin </b>of a SVM?",
                "Note that&nbsp;\\(\\mathbf{w}^{T} \\mathbf{x}+b=0\\) and&nbsp;\\(c\\left(\\mathbf{w}^{T} \\mathbf{x}+b\\right)=0\\) define the same hyperplane.<div><br></div><div>That means \\(c\\) can be chosen freely.&nbsp;</div><div><br></div><div>For positive support vectors we set</div><div><br></div><div>\\(\\mathbf{w}^{T} \\mathbf{x}_{+}+b=+1\\)<br></div><div><br></div><div>For negative support vectors</div><div><br></div><div>\\(\\mathbf{w}^{T} \\mathbf{x}_{-}+b=-1\\)<br></div><div><br></div><div>The margin is given by:</div><div><br></div><div>\\(\\frac{\\mathbf{w}^{T} \\mathbf{x}_{+}+b}{\\|\\mathbf{w}\\|}-\\frac{\\mathbf{w}^{T} \\mathbf{x}_{-}+b}{\\|\\mathbf{w}\\|}=\\frac{2}{\\|\\mathbf{w}\\|}\\)</div>"
            ],
            "guid": "Hn{Pe36/FC",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the purpose of <b>slack variables</b>&nbsp;\\(\\xi_{i}\\)?",
                "Slack variables allow <b>violating the margin conditions</b> to fnd more robust solutions.<div><br></div><div>\\(y_{i}\\left(\\mathbf{w}^{T} \\mathbf{x}_{i}+b\\right) \\geq 1-\\xi_{i}\\)<br></div>"
            ],
            "guid": "tTVbx)C!G)",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What happens if the slack variable&nbsp;\\(\\xi_{i} = 0\\),&nbsp;\\(0 \\leq \\xi_{i} \\leq 1\\) and \\(\\xi_{i}&gt;1\\)?",
                "<div>\n<div><ul>\n<li>\\(0 \\leq \\xi_{i} \\leq 1\\) sample is between margin and decision boundary: <b>margin violation</b></li>\n<li>\\(\\xi_{i}&gt;1\\) sample is on the wrong side of the decision boundary: misclassified</li>\n<li>\\(\\xi_{i}= 0\\) sample is on the decision boundary</li>\n</ul><div><img src=\"paste-b93f9590b30e8fe0d52e35a06477d728e200df28.jpg\"><br></div>\n</div></div>"
            ],
            "guid": "Hq+3M_l5AN",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Rewrite the following constrained optimization problem of a SVM to an unconstrained one.<div><br></div><div>\\[\\operatorname{argmin}_{\\mathbf{w}, \\boldsymbol{\\xi}} \\quad\\|\\mathbf{w}\\|^{2}+C \\sum_{i}^{N} \\xi_{i}, \\quad \\text { s.t. } \\quad y_{i}\\left(\\mathbf{w}^{T} \\mathbf{x}_{i}+b\\right) \\geq 1-\\xi_{i}, \\quad \\xi_{i} \\geq 0\\]<br></div>",
                "Rewrite constraints: \\(\\xi_{i} \\geq 1-y_{i}\\left(\\mathbf{w}^{T} \\mathbf{x}_{i}+b\\right)=1-y_{i} f\\left(\\boldsymbol{x}_{i}\\right)\\)<div><br>Together with \\(\\xi_{i} \\geq 0\\) this results in \\(\\xi_{i}=\\max \\left(0,1-y_{i} f\\left(\\boldsymbol{x}_{i}\\right)\\right)\\) (given that \\(\\xi_{i}\\) should be minimized)</div><div><br></div><div>unconstrained optimization (over w):</div><div><br></div><div>\\(\\operatorname{argmin}_{\\mathbf{w}} \\underbrace{\\|\\mathbf{w}\\|^{2}}_{\\text {regularization }}+C \\underbrace{\\sum_{i=1}^{N} \\max \\left(0,1-y_{i} f\\left(\\boldsymbol{x}_{i}\\right)\\right)}_{\\text {loss function }}\\)<br></div>"
            ],
            "guid": "JP%V{%3U;G",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the idea of a <b>soft max-margin</b>?",
                "One introduces <b>slack variables&nbsp;</b>\\(\\xi_{i}\\), so that some margin violations are tolerated.&nbsp;<div><br></div><div>However, a punishment term is applied for a large number of slack variables.</div><div><br></div><div>\\(\\begin{aligned}<br>\\operatorname{argmin}_{\\mathbf{w}, \\boldsymbol{\\xi}} &amp;\\|\\mathbf{w}\\|^{2}+C \\sum_{i}^{N} \\xi_{i}, \\\\<br>\\text { s.t. } &amp; y_{i}\\left(\\mathbf{w}^{T} \\mathbf{x}_{i}+b\\right) \\geq 1-\\xi_{i}, \\quad \\xi_{i} \\geq 0<br>\\end{aligned}\\)<br></div><div><br></div>"
            ],
            "guid": "L7qmj+,hdV",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain what the parameter \\(C\\) does with SVMs.",
                "\\(C\\) bounds the sum of the \\(\\epsilon_i\\), and so determines the number and severity of the margin violations it will tolerate."
            ],
            "guid": "nYq:@A`Jug",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain what happens if the parameter \\(C\\) is small, large or infinite for SVMs.",
                "<div>\n<div><ul>\n<li>Small \\(C\\). Constraints have little influence → large margin → large regularization</li>\n<li>Large \\(C\\)&nbsp;Constraints have large influence → small margin → small regularization</li>\n<li>\\(C\\)&nbsp;infinite: Constraints are enforced → hard margin → no regularization</li>\n</ul>\n</div></div>"
            ],
            "guid": "hIWEl|_Ot;",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "08_svm"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give a definition of the <b>cosine distance</b>.",
                "\\(d(\\boldsymbol{x}, \\boldsymbol{y})=1-\\frac{\\boldsymbol{x}^{T} \\boldsymbol{y}}{\\|\\boldsymbol{x}\\|\\|\\boldsymbol{y}\\|}\\)"
            ],
            "guid": "zup]gYKOz>",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give a definition of the <b>hamming distance</b>",
                "\\(d(\\boldsymbol{x}, \\boldsymbol{y})=\\sum_{k=1}^{d}\\left(\\boldsymbol{x}_{k} \\neq \\boldsymbol{y}_{k}\\right)\\)"
            ],
            "guid": "HbV$7W:wU+",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give a definition of the <b>manhattan distance</b>.",
                "\\(d(\\boldsymbol{x}, \\boldsymbol{y})=\\sum_{k=1}^{d}\\left|\\boldsymbol{x}_{k}-\\boldsymbol{y}_{k}\\right|\\)"
            ],
            "guid": "J1l9e=T7{m",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give a definition of the <b>mahalanobis distance</b>.",
                "\\(d(\\boldsymbol{x}, \\boldsymbol{y})=\\|\\boldsymbol{x}-\\boldsymbol{y}\\|_{\\boldsymbol{\\Sigma}^{-1}}=\\sqrt{(\\boldsymbol{x}-\\boldsymbol{y})^{T} \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x}-\\boldsymbol{y})}\\)"
            ],
            "guid": "%4T!wYt`X",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf",
                "checklater",
                "formula_sheet"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the <b>Expectation Maximization (EM)</b> Algorithm on a <b>intuitional level</b>?",
                "The EM algorithm is a general iterative scheme for learning parameters (e. g. maximum likelihood) in a latent-variable model."
            ],
            "guid": "pl[Tve.d7O",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain the <b>two steps</b> of the <b>Expectation Maximization</b> algorithm on an intuitional level for <b>Gaussian mixture models</b>.",
                "<div><div><div><div></div><div></div></div></div>\n</div>\n<div>\n<div><div>In our example of the Gaussian mixture model, we choose initial values for \\(\\mu_{k}, \\boldsymbol{\\Sigma}_{k}, \\pi_{k}\\)&nbsp;and alternate until convergence between</div>\n<ul>\n<li><b>E-step:</b> Evaluate the responsibilities \\(r_{n k}\\) (posterior probability of data point \\(n\\) belonging to mixture component \\(k\\) ).</li>\n<li><b>M-step:</b> Use the updated responsibilities to reestimate the parameters \\(\\mu_{k}, \\Sigma_{k}, \\pi_{k}\\)</li>\n</ul>\n</div></div>"
            ],
            "guid": "r{L0NFM-YY",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Wie bestimmt man die Parameter einer Gauß-Verteilung mittels einem <b>Maximum Likelihood Ansatz</b>?<div><br></div><div>\\[\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}=\\operatorname{argmax}_{\\boldsymbol{\\theta}} \\log \\operatorname{lik}(\\boldsymbol{\\theta} ; D)=\\sum_{i=1}^{N} \\log \\mathcal{N}\\left(\\boldsymbol{x}_{i} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}\\right)\\]<br></div>",
                "Indem man die partielle Ableitung bildet und mit 0 gleichsetzt:<div><br></div><div>\\(\\frac{\\partial \\log \\operatorname{like}(\\boldsymbol{\\theta} ; \\mathcal{D})}{\\partial \\boldsymbol{\\mu}}=\\mathbf{0}\\)<br></div><div><br></div><div>\\(\\frac{\\partial \\log \\operatorname{like}(\\boldsymbol{\\theta} ; \\mathcal{D})}{\\partial \\boldsymbol{\\Sigma}}=\\mathbf{0}\\)<br></div><div><br></div><div>Which yields the closed form solution:</div><div><br></div><div>\\(\\boldsymbol{\\mu}=\\frac{1}{N} \\sum_{i=1}^{N} \\boldsymbol{x}_{i}\\)<br></div><div><br></div><div>\\(\\mathbf{\\Sigma}=\\frac{1}{N} \\sum_{i=1}^{N}\\left(\\boldsymbol{x}_{i}-\\boldsymbol{\\mu}\\right)\\left(\\boldsymbol{x}_{i}-\\boldsymbol{\\mu}\\right)^{T}\\)<br></div>"
            ],
            "guid": "K3/R#aDg!L",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Was minimiert der \\(k\\)-Means Algorithmus?",
                "Die Sum of squared distances (SSD)"
            ],
            "guid": "Q`q7Us;^TT",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_clustering"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Wie ist die <b>Sum of Squared Distances</b> definiert?",
                "Für gegebene Daten \\(D=\\left\\{\\boldsymbol{x}_{1}, \\ldots, \\boldsymbol{x}_{n}\\right\\}\\) sucht man die Cluster-Centroids&nbsp;\\(C=\\left\\{c_{1}, \\ldots, c_{k}\\right\\}\\).<div><br></div><div>Mit \\(c(x)\\) bezeichnet man den nähesten Centroid Vektor \\(c \\in C\\) to \\(x\\).</div><div><br></div><div>Die Sum of Squared Distances ist dann:</div><div><br></div><div>\\(\\operatorname{SSD}(C ; \\mathcal{D})=\\sum_{i=1}^{n} d\\left(\\boldsymbol{x}_{i}, c\\left(\\boldsymbol{x}_{i}\\right)\\right)^{2}\\)<br></div><div><br></div><div><img src=\"paste-67c83c3991a1d93a7b0a6f96f28c216b133dd723.jpg\"><br></div>"
            ],
            "guid": "gT$+uc?qFs",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_clustering",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain how a neuron roughly works.",
                "A neuron is the basic computational unit of the brain.<div><br></div><div>Dendrites bring impulses / signal to the nucleus. The neucleus enforces the impulse, if it reaches a certain threshold and sends it to the axon. Axons themself are connected to other neurons.</div><div><br></div><div><img src=\"paste-7e84c5cc800736ec791cd4676a6ae9c683419e87.jpg\"><br></div>"
            ],
            "guid": "dv=#5),l={",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is an <b>artificial neuron</b>?&nbsp;",
                "An abstraction of the neuron in the brain."
            ],
            "guid": "jZDk(xp^%V",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain the components of an <b>artificial neuron</b>.",
                "<img src=\"paste-beb37c5b09d86aaf7ccd4b921b5fc2319cf4da54.jpg\"><br><div><br></div><div><b>Example:<br></b><br></div><div>Logistic Regression:</div><div>\\(y=\\sigma\\left(\\mathbf{w}^{T} \\mathbf{x}+b\\right)\\)<br></div><div><br></div><div>with sigmoid activiation function.</div>"
            ],
            "guid": "M<[iF(+S3g",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is characteristic for a f<b>eed-forward Neural Network</b>?",
                "A lot of of neurons connected by a directed acyclic graph.<div><br></div><div>Typically units are grouped in layers.</div><div><br></div><div><img src=\"paste-a847484cbd5bc6aedfd254238e395c7ac295386d.jpg\"><br></div>"
            ],
            "guid": "k[yeL%M^I<",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is characteristic for a recurrent&nbsp;<b>Neural Network</b>",
                "A lot of of neurons connected by a graph. Graph my contain cycles."
            ],
            "guid": "k@_vs<iP~G",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "When is a layer called <b>fully connected</b>?",
                "If all input units are connected to <b>all</b> output units."
            ],
            "guid": "j9ll5ef]%-",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain how the number of input units and the number of output units correspond to the size of the weight matrix of a neural net.",
                "Each layer connects \\(N\\) input units to \\(M\\) output units.<div><br></div><div>Each leayer has a \\(M \\times N\\) matrix \\(W\\).</div><div><br></div><div>The output is then calculated as a function of input units:</div><div>\\(\\mathbf{y}=\\phi(\\mathbf{W} \\mathbf{x}+\\mathbf{b})\\)</div>"
            ],
            "guid": "HSm}r=/9NF",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Name 5 activation functions.",
                "<div>\n<div><ul>\n<li>sigmoid</li>\n<li>tanh</li>\n<li>ReLU</li>\n<li>leaky ReLU</li>\n<li>ELU</li>\n</ul>\n</div></div>"
            ],
            "guid": "yG,R,C2E#%",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give the definition of the Sigmoid activation function.",
                "\\(\\sigma(x)=\\frac{1}{1+\\exp (-x)}\\)<div><br></div><div><img src=\"paste-ca14b0f5025cead0f2a6e1404457c5832f33fa92.jpg\"><br></div>"
            ],
            "guid": "nv>}u*LoVF",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give the range of the <b>sigmoid function</b>.",
                "\\([0,1]\\)"
            ],
            "guid": "E1Hz%nsEb3",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the problems of the <b>sigmoid function</b>?",
                "\n<div><ul>\n<li>saturated nerons “kill” the gradient, as it stays constant at one</li>\n<li>sigmoid outputs are not zero-centered (important for initialization)</li>\n<li>exp() is expensive to compute, esspecially in deep networks</li>\n</ul><div><br></div><div>Due to these problems, sigmoid is today only used for output layers but not for hidden layers.</div>\n</div>"
            ],
            "guid": "n8e8.K|@E1",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give the definition and domain of the \\(tanh(x)\\)",
                "\\(\\tanh (x)\\)<div><br></div><div>Domain:</div><div>\\([-1,1]\\)</div><div><br></div><div><img src=\"paste-e86f1f621250a98764d596d3210e22fa319185cf.jpg\"><br></div><div><br></div>"
            ],
            "guid": "w9Sd0l6Qdv",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the <b>advantages </b>/ <b>disadvantages </b>of the <b>tanh activation function</b>?",
                "<b>Advantage</b>:<div>zero centered, good for initialization as zero-centered input yields zero-centered output.</div><div><br></div><div><b>Disadvantages:</b></div><div>Still kills gradients when saturated</div>"
            ],
            "guid": "P^3D`wD&]7",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give a definition of the <b>ReLU activation</b> function.",
                "\\(f(x)=\\max (0, x)\\)<div><br></div><div><img src=\"paste-52f6048ff6aa60354ea06922a0ea519729e66bbd.jpg\"><br><div><br></div><div><br></div></div>"
            ],
            "guid": "C8Qh+$,L<)",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the advantages / disadvantages of the <b>ReLU activation function</b>?",
                "<div>\n<div><div><strong>Advantages</strong></div>\n<ul>\n<li>Does not saturate (in +region)</li>\n<li>Very computationally efficient</li>\n<li>Converges much faster than sigmoid/tanh in practice e. g. 6 times faster</li><li>Commonly used</li>\n</ul>\n<div><strong>Disadvantages</strong></div>\n<ul>\n<li>Not zero-centred output</li>\n<li>No gradient for \\(x&lt;0\\)</li>\n</ul>\n</div></div>"
            ],
            "guid": "D:;Q))SRds",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give a definition of the <b>Leaky ReLU </b>activation function.",
                "\\(f(x)=\\max (0.1 x, x)\\)<div><br></div><div><img src=\"paste-6e883ccb5efda948f8beac055d1221ac08ef10f4.jpg\"><br></div>"
            ],
            "guid": "fLi*_J,)A&",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give <b>advantages / disadvantages</b> of the <b>Leaky ReLU</b>.",
                "<div>\n<div><div><strong>Advantages</strong></div>\n<ul>\n<li>Does not saturate (in +region)</li>\n<li>Computationally efficient</li>\n<li>Converges much faster than sigmoid/tanh in practice e. g. 6 times faster</li>\n<li>will not “die”</li>\n</ul>\n<div><strong>Disadvantage</strong></div>\n<ul>\n<li>Learn one more parameter alpha</li>\n</ul>\n</div></div>"
            ],
            "guid": "j(JDeiOA+J",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give a definition of the <b>Exponential Linear Units (ELU)</b> activiation function.",
                "\\(f(x)= \\begin{cases}x &amp; \\text { if } x&gt;0 \\\\ \\alpha(\\exp (x)-1) &amp; \\text { if } x \\leq 0\\end{cases}\\)<div><br></div><div><br><div><img src=\"paste-cd53484264b2c55c813ec0a0fc6c5d8030d6c586.jpg\"><br></div><div><br></div></div>"
            ],
            "guid": "iU-znR?mo<",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Name advantages / disadvantages of the <b>ELU</b> activation function",
                "<div>\n<div><div><strong>Advantage</strong></div>\n<ul>\n<li>All benefits of RELU</li>\n<li>Closer to zero mean outputs, because of negative part</li>\n<li>Negative saturation regime compared to Leaky ReLU, which means more robustness to noise</li>\n</ul>\n<div><strong>Disadvantage</strong></div>\n<ul>\n<li>Computation requires exp()</li>\n</ul>\n</div></div>"
            ],
            "guid": "tAvl</wQk?",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "When formalizing a Feedforward Net as a stack of layers that compute a function.<div><br></div><div><img src=\"paste-bc5d93bf53cbdc86d9f1454b6c296f79b8620cb3.jpg\"><br></div><div><br></div><div>What is the mathematical equivalent of this?</div>",
                "\\(\\mathbf{y}=f^{L} \\circ f^{L-1} \\circ \\ldots f^{(1)}(\\mathbf{x})\\)"
            ],
            "guid": "w6V@:3No#7",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain how XOR can be implemented using a Neural Net?",
                "<div>Recall:</div><div>\\(XOR(a,b) = (a \\operatorname{or} b) \\operatorname{and} \\operatorname{not} (a \\operatorname{and} b)\\)</div><b><div><b><br></b></div>Architecture:</b><div><img src=\"paste-00f20df6a86ed0946abbfe58c92c82872f849780.jpg\"><br><div><br></div><div><br></div><div>Hard threshold for activation function (binary units).</div><div>\\(h_{1}\\) computes \\(x_{1}\\) OR \\(x_{2}\\)<br></div><div>\\(h_{2}\\) computes \\(x_{1}\\) AND \\(x_{2}\\)<br></div><div>y computers \\(h_{1}\\) AND NOT \\(h_{2}\\)<br></div></div><div><br></div><div><img src=\"paste-319cebfe22365c7b22818945d6b3cf790251a701.jpg\"><br></div><div><br></div><div>\\(h_1 &gt;=0\\) Are both inputs 0, only the bias term (-0.5) would impact. Thus -0.5 would not be greater than 0. However, if one input term is 1, the sum would be 0.5 which is greater than 0. The output would be 1.</div><div><br></div>"
            ],
            "guid": "f(MWY#=eIe",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What does the <b>Universal Function Approximation Theorem</b> say?",
                "<div>Given a potentially infinite amout of units, that can approximate any function arbitrarily well.<br></div><div><br></div><div>Already a single layer is enough to achieve \"universality\".</div>"
            ],
            "guid": "A9=Ng6-osu",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Why is single layer not enough in practice, eventhough the Universal Approximation Theorem states it?",
                "The problem is, that you'd need an exponential number of units, otherwise you overfit.<div><br></div><div>With multiple layers you can have similiar effects and have a compact representation with less units.</div>"
            ],
            "guid": "G*UjFrki;?",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Why are Neural Networks referred to as <b>Feature Learning Machines</b>?",
                "The network learns the features&nbsp;\\(\\psi(x)\\) such that linear regression / classification can solve it. (Important if input data is not linearily separable).<div><br></div><div>The last layer is then a standard linear regression / classification layer.<br><div><br></div></div><div><img src=\"paste-088e228eef7bcb8b8392a02cdd5c05b14be347ff.jpg\"><br></div>"
            ],
            "guid": "M(k]IMH}@.",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is <b>back-propagation</b> on an intuitional level?",
                "back-progropagation is an alogrithm for calculating the gradient of a neural network with respect to its parameters."
            ],
            "guid": "J*m?l+a!6a",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we optimize the objective function of a neural net on an abstract level?",
                "\\(\\mathcal{L}(\\boldsymbol{\\theta}, \\mathcal{D})=\\sum_{i=1}^{N} l\\left(\\boldsymbol{x}_{i}, \\boldsymbol{\\theta}\\right)+\\lambda \\operatorname{penalty}(\\boldsymbol{\\theta})\\),<div><br></div><div>where \\(\\theta\\) is the weight space that hold for all the weights or bias of the network of all layers for one coordinate<br><div>\\(\\boldsymbol{\\theta}=\\left\\{\\mathbf{W}^{(L)}, \\ldots, \\mathbf{W}^{(1)}, \\mathbf{b}^{(L)}, \\ldots, \\mathbf{b}^{(1)}\\right\\}\\)</div></div><div><br></div><div>We need to compute the following partial derivatives:</div><div><br></div><div>Layer weight matrices: \\(\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{(l)}}\\)<br>Layer bias vectors: \\(\\quad \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{(l)}}\\)<br></div><div><br></div><div>This can be done recursively using the chain rule, as each weights depend on the previous.</div>"
            ],
            "guid": "o,{j`ZCh?/",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give a definition of the <b>univariate chain rule</b>.",
                "If \\(f(x)\\) and \\(x(t)\\) are univariate functions, then&nbsp;<div><br></div><div>\\(\\frac{d}{d t} f(x(t))=\\frac{d f}{d x} \\frac{d x}{d t}\\)<br></div>"
            ],
            "guid": "jDMCfkeD>]",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain what a <b>computation graph</b> is.",
                "A computation graph is a diagramm that shows how variables relate and depend previous variables or inputs. This is indicated by arrows.<div><br></div><div>For computing the loss you go forward through the graph.<br><div><br></div><div>For computing the derivative you go back in the graph.</div><div><br></div><div><img src=\"paste-c0368e2f12364c4b2f4628f6c39483c989496ce5.jpg\"><br></div></div>"
            ],
            "guid": "p~(owM#N3Q",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain how one can use the following computational graph to calculate the derivatives. Note them down in a simplified notation / as error signals.<div><img src=\"paste-c0368e2f12364c4b2f4628f6c39483c989496ce5.jpg\"><br></div><div><br></div><div><b>Forward pass:</b></div><div><br></div><div>\\(z=w x+b\\)<br>\\(y=\\sigma(z)\\)<br>\\(\\mathcal{L}=\\frac{1}{2}(y-t)^{2}\\)</div>",
                "<div><b>Backward pass:</b></div><div><br></div>\\(\\bar{y}=y-t\\)<br>\\(\\bar{z}=\\bar{y} \\sigma^{\\prime}(z)\\)<br>\\(\\bar{w}=\\bar{z} x\\)<br>\\(\\bar{b}=\\bar{z}\\)"
            ],
            "guid": "n->IM!^k84",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How do computational cost of training a neural network grow with no. of layers and no. of units?",
                "<div><b>Computational cost of forward pass</b></div><div>\\(\\boldsymbol{z}=\\boldsymbol{W} x+\\boldsymbol{b}\\)<br></div><div><br></div><div>Roughly one add-multiply operation per weight</div><div><br></div><div><b>Computational cost of backward pass</b></div><div>\\(\\overline{\\boldsymbol{W}}=\\overline{\\boldsymbol{h}} \\boldsymbol{z}^{T}, \\quad \\overline{\\boldsymbol{h}}=\\boldsymbol{W}^{T} \\overline{\\boldsymbol{y}}\\)<br></div><div><br></div><div>Roughly two add-multiply operations per weight (twice the forward pass).</div><div><br></div><div>For a MLP this means the the cost is linear with the number of layers, quadratic in the number of units per layer.</div>"
            ],
            "guid": "Bb5w.LIN+|",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Why are mini batches&nbsp; commonly used for gradient descent?",
                "<div>\n<div><ul>\n<li>Mini batch is an intermediate version of stochastic and batch gradient descent</li>\n<li>Gives less noisy estimates than stochastic gradient descent</li>\n<li>More efficient than batch gradient descent</li>\n<li>Preferable for GPU implementations</li>\n</ul>\n</div></div>"
            ],
            "guid": "i(D@k4eD8:",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What happens if the learning rate is to low, to high and much to high?",
                "<b>Too low:</b> slow convergence<div><b>Too high: </b>oscillations and slow convergence</div><div><b>Much too high:</b> divergence</div><div><br></div><div><img src=\"paste-c8bee1571b288a2d7088ba6c2f7cafd453932458.jpg\"><br></div><div><br></div><div><br></div><div><div><img src=\"paste-60d96d5d77835e58f4c539424ef47b3d256e5cda.jpg\"><br></div></div>"
            ],
            "guid": "P,K,?*IX&1",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Name three approaches to speed up gradient descent",
                "<div>\n<div><ul>\n<li>Momentum terms</li>\n<li>Adaptive learning rates</li>\n<li>Second order methods (only for smaller networks)</li>\n</ul>\n</div></div>"
            ],
            "guid": "LBAvf+s)t~",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How should we change the learning rate \\(\\eta\\) for NN? Name 4 approaches.",
                "<div>\n<div><ul>\n<li>Step</li>\n<li>Cosine</li>\n<li>Linear</li>\n<li>Inverse Sqrt</li>\n</ul>\n</div></div>"
            ],
            "guid": "EtJp->J38=",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain how changing the learning rate \\(\\eta\\) using <b>Step</b> works.",
                "Reduce learning rate at a few fixed points. E. g. for ResNets, multiply LR by 0.1 after 30, 60 and 90.<div><br></div><div><img src=\"paste-f27af5f1ab8a8555bd4c4b683aa575bb094e0533.jpg\"><br></div>"
            ],
            "guid": "shy4gyLVvP",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain how changing the learning rate \\(\\eta\\) using&nbsp;<b>Cosine</b>&nbsp;works.",
                "\\(\\alpha_{t}=\\frac{1}{2} \\alpha_{0}(1+\\cos (t \\pi / T))\\)<div><br></div><div>where \\(\\alpha_{0}\\) is the Initial learning rate, \\(\\alpha_{t}:\\) Learning rate at epoch t and \\(T:\\) Total number of epochs.<br><div><br></div><div><img src=\"paste-5b0cf9d2d13ce4b605af6ad161ad868e3e04a5c2.jpg\"><br></div></div>"
            ],
            "guid": "k<4z;yNIEi",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain how changing the learning rate \\(\\eta\\) using&nbsp;<b>Linear</b>&nbsp;works.",
                "\\(\\alpha_{t}=\\alpha_{0}(1-t / T)\\)<div><br></div><div>where \\(\\alpha_{0}\\) is the Initial learning rate, \\(\\alpha_{t}:\\) Learning rate at epoch t and \\(T:\\) Total number of epochs.<br><div><br></div><div><img src=\"paste-0a471cfeb5cd8be7cacb461aca4b6e016b03a8aa.jpg\"><br></div></div>"
            ],
            "guid": "y4SpoMI8t-",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain how changing the learning rate \\(\\eta\\) using&nbsp;<b>Inverse sqrt</b>&nbsp;works.",
                "<div>\\(\\alpha_{t}=\\alpha_{0} / \\sqrt{t},\\)</div><div><br></div><div>where \\(\\alpha_{0}\\) is the Initial learning rate, \\(\\alpha_{t}:\\) Learning rate at epoch t and \\(T:\\) Total number of epochs.</div><br><div><br></div><div><img src=\"paste-f067be98ce82c7721b9ede76260c52388ff5515e.jpg\"><br></div>"
            ],
            "guid": "pi{9*&-~jU",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Name an approach to reduce overfitting in Neural Networks.",
                "Regularization"
            ],
            "guid": "e:|vM{o}VN",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Name several Regularization approaches for neural nets.",
                "<div>\n<div><ul>\n<li>model selection (no. layers, no. neurons etc.)</li>\n<li>Data augmentation (creating new variants of input data)</li>\n<li>Early stopping</li>\n<li>regularization loss</li>\n<li>model ensembles</li>\n<li>dropout</li>\n</ul>\n</div></div>"
            ],
            "guid": "w<$}1wH._I",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain what an <b>model ensemble</b> is in the context of neural nets.",
                "Train multiple independent models and average their results."
            ],
            "guid": "g-E?X8XL,B",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the <b>advantage</b>&nbsp;/ <b>disadvantage </b>of <b>model ensembles</b>?",
                "<div><b>advantage</b></div>They yield an increase in accuracy. Typically 2 %.<div><br></div><div><b>disadvantage</b><div>Training several models is expensive.</div></div>"
            ],
            "guid": "yvk0g<(_+>",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How can one avoid to train multiple models with model ensembles?",
                "One can take different snapshots of the model during training whenever an optima is found and combine these models to an ensemble.<div><br></div><div><img src=\"paste-1d7f4a32b9f1db4e99019c65ca65b5b3a272eb44.jpg\"><br></div>"
            ],
            "guid": "q+LD|=n(R3",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the idea of Dropout?",
                "In each forward pass, randomly set some neurons to zero. Other neurons have to compensate that.<div><br></div><div><img src=\"paste-2d8ea6871f44599f7199bc4da534d0e79241cdfd.jpg\"><br></div>"
            ],
            "guid": "v6rJ;TxCG>",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Why can dropout be interpreted as an ensemble?",
                "Droput is training a large ensemble of models (that share parameters)<div><br></div><div>Each binary mask is one model.&nbsp;</div>"
            ],
            "guid": "cu<!9p_:9i",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Why should Dropout be used?",
                "Forces the network to have a redundandent representation.<div><br></div><div>Prevents co-adaption of features.</div>"
            ],
            "guid": "AusZw#3Pll",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How is <b>Drop Connect</b> different from <b>Drop Out</b>?",
                "<b>Training:</b> Drop Connections between neurons (set weights to 0)<div><b>Testing: </b>Use all the connections</div><div><br></div><div><img src=\"paste-c854a6ff664136ecc1cf68d6e93038d2bba4ee4c.jpg\"><br></div>"
            ],
            "guid": "G$NBK#aqa`",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How can Testing be done with Drop Out in NN.&nbsp;<div><br></div><div>Name and explain two approaches.</div>",
                "<div><div><div><div></div><div></div></div></div>\n</div>\n<div>\n<div><div><strong>Ensemble view:</strong></div>\n<ul>\n<li>Average over multiple dropout masks (computationally expensive but quite robust)</li>\n<li>Also allows to get uncertainty estimates (not covered)</li>\n</ul>\n<div><strong>Expectation view:</strong></div>\n<ul>\n<li>Compute the expected input to the activation functions</li>\n<li>Multiply each weight by the dropout rate</li>\n</ul>\n</div></div>"
            ],
            "guid": "mgOJg=A&wV",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How should data be preprocessed for Neural Nets?",
                "Normalize data:<div><div>\\(\\tilde{\\boldsymbol{x}}=(\\boldsymbol{x}-\\boldsymbol{\\mu}) \\oslash \\boldsymbol{\\sigma}\\)<br></div><div>Mean \\(\\mu\\), standard deviation \\(\\sigma\\), element-wise divison (similar to Hardamard product).&nbsp;</div><div><br></div><div>\\(\\tilde{\\boldsymbol{x}}\\) is standardized with zero mean and unit variance.<br></div></div>"
            ],
            "guid": "B#-0|5sSoM",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Why is it important to work on normalized data with Neural Nets?",
                "Network initalization strategies are optimized for zero-mean unit variance."
            ],
            "guid": "J8j#(R1k<^",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What happens if we initialize all weights constantly e. g. 10 for a neural network.",
                "All the gradients are the the same.<div><br></div><div>Network will never learn distinct features.</div>"
            ],
            "guid": "n!`Y^/7_Zm",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is <b>random intialization</b> of a <b>Neural Network</b>.",
                "Initialize with small random numbers (e. g. gaussian-distributed with zero mean and 1e-2 standard deviation)"
            ],
            "guid": "BIX>B<7ZWC",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the problem with <b>random initialization</b> with deep neural networks?",
                "<div>All activations tend to be zero for deeper networks.</div><div><br></div><div>If activations are saturated gradients also tend to vanish.</div>"
            ],
            "guid": "j@@k)FkVj5",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give the formula for the Xavier initalization.",
                "Default:<div><br></div><div>\\(\\sigma_{\\mathbf{W}}=\\frac{1}{\\sqrt{D_{\\text {in }}}}\\)</div><div><br></div><div>Version that works with ReLU:</div><div><br></div><div>\\(\\sigma_{\\mathbf{W}}=\\frac{2}{\\sqrt{D_{\\text {in }}}}\\)</div>"
            ],
            "guid": "Iz_Fpf)L30",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain 7 steps on how to train a Neural Net in practice.",
                "Step 1: Check initial loss<div><br></div><div>Turn off weight decay (L2 regularization), sanity check loss at initialization e. g. log(C) for softmax with C classes.</div><div><br></div><div>Step 2: Overfit a small sample</div><div><br></div><div>Try to train to 100 % accuaracy on a small sample of training data (~5-10 minibatches)</div><div><div>\n<div><ul>\n<li>Fiddle with architecture, learning rate, weight initialization</li>\n<li>Loss not going down? LR too low, bad initialization</li>\n<li>Loss explodes to Inf or NaN? LR too high, bad initialization</li>\n</ul>\n</div></div></div><div>Step 3: Find LR that makes loss go down</div><div><br></div><div>Use the architecture from the previous step, use all training data, turn on small weight decay, find a learning rate that makes the loss drop signifcantly within ~100 iterations.</div><div><br></div><div>Step 4: Coarse grid, train for ~1-5 epochs</div><div><br></div><div>Choose a few values of learning rate and weight decay (\\(\\ell_2\\) loss) around whatworked from previous step, train a few models for ~1-5 epochs.</div><div><br></div><div>Step 5: Refine grid, train longer</div><div>Pick best models from Step 4, train them longer (~10-20 epochs) without learning rate decay.</div><div><br></div><div>Step 6: look at loss curves</div><div>Step 7: Go back to step 5</div>"
            ],
            "guid": "B0cjfXcd:w",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the causes of the following loss curves during training a neural net?<div><img src=\"paste-ba9ffad3dd33c9b1f3543ae75ddd2c8e7beae597.jpg\"><br></div><div><br></div><div><img src=\"paste-52eafc401f607994446b9de11e9ab7b881b61801.jpg\"><br></div><div><br></div><div><img src=\"paste-92e1e1454b1da41205a4be67ece285003f386301.jpg\"><br></div>",
                "<div><img src=\"paste-a8504c5f8f92e40fb4c84c8c02d1ce0584b95769.jpg\"><br></div>Initialization is poor.<br><div><br></div><div><img src=\"paste-dff10d7cc1878d3387d5b43c2dfd465f6f6f93df.jpg\"><br></div><div>If loss is plateuing, learning rate is to high or should be changed.</div><div><br></div><div><img src=\"paste-6469b5aec1cc4804472264da56e4d27baf9d616e.jpg\"><br></div><div>Higher learning rate should be kept and later decayed.</div>"
            ],
            "guid": "z}V|1x|XW[",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Name fields of application where CNNs are used",
                "<div><div>\n<div><ul>\n<li>Image Classification</li>\n<li>Image Retrival</li>\n<li>Object Detection</li>\n<li>Image Segmentation</li>\n</ul>\n</div></div></div>"
            ],
            "guid": "zxH3.:55R^",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Name 4 popular <b>CNN architectures</b>.",
                "<div>\n<div><ul>\n<li>AlexNet</li>\n<li>VGG</li>\n<li>GoogLeNet</li>\n<li>ResNet</li>\n</ul>\n</div></div>"
            ],
            "guid": ":$jlqY3E2",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the problem of processing images with a standard network using fully connected layers?",
                "Say we got a \\(32 \\times 32\\) image with 3 color channels that would stretch to an \\(3072 \\times 1\\) array.<br><div><br></div><div><img src=\"paste-466385bdbf325602a55808365f8f98792a218407.jpg\"><br></div><div><br></div><div>We would need a huge amount of weights using a fully-connected layer.</div>"
            ],
            "guid": "i:oq1L#f0j",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe what a <b>convolutional layer</b> is.",
                "<div>Convolutional layers <b>convolve </b>the input and pass its result to the next layer.<br></div><div><br></div><div>In a convolutional layer, we apply multiple <b>filters </b>at the image to extract different features. Filters always extend the full depth of the input volume. Filter has to be learned.<br></div><div><br></div><div>We convolve the filter with the image i. e. we slide over the image spartially , computing dot products.</div><div><br></div><div>The dot product is given by:</div><div>\\(w^{T} x+b\\)</div><div><br></div><div><b>Visualization:</b></div><div><img src=\"paste-45943dd0e0e24e5cd3ceac41704eb7d264cb730b.jpg\"><br></div><div><br></div><div><img src=\"paste-1f08bdc45f900c733ad728cff19fb90a391f1e5f.jpg\"><br></div>"
            ],
            "guid": "u<C{9DRC/W",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain how convolving / sliding over all spartial locations works with a CNN.",
                "<img src=\"paste-4e3cbac6da72820b542f141e4203608393035c4b.jpg\"><div>The window is then slided around. There are \\(4\\times4\\) locations, where the window can be slided to and the matrix is full, which gives the output shape.</div>"
            ],
            "guid": "w~xE92X${8",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain what a <b>pooling layer</b> does in a CNN.",
                "Pooling layers make the representations smaller and more managable.<div><br></div><div>They do this by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. The most popular way of combining is <b>max pooling</b>.</div><div><br></div><div>They operate over each activation map independently.</div><div><br></div><div><br></div><div><img src=\"paste-39cdad62a7dc76d4494ccd17fdd16fbc80476eeb.jpg\"><br></div>"
            ],
            "guid": "r]02x7D&tE",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain how <b>Max-Pooling</b> works.",
                "<div>For each channel, compute the max over the whole window.</div><div><br></div><div><img src=\"paste-fc59ce0439c2e5dee9b265e7904b20e95f272a94.jpg\"><br></div>"
            ],
            "guid": "f}D)]7BL@L",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain the basic structure of a <b>convolutional network</b>. Also make sure to sketch it.",
                "A convolutional network is a sequence of <b>convolution layers</b>, interspersed with <b>activation functions </b>e. g. ReLU and <b>pooling functions</b> followed by one or multiple <b>Fully-connected (FC) layers</b> to compute the output (regression or classification)<div><br></div><div><img src=\"paste-39b9827d44a2cbcf60ec14ec968f8946ac49b3fc.jpg\"><br></div>"
            ],
            "guid": "s1JT`#sKz%",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the <b>two hyperparameters</b> of a pooling layer with CNNs?",
                "Spartial extend / kernel size \\(F\\)<div>Stride \\(S\\)</div><div><br></div><div><b>Visualization:</b></div><div><img src=\"1FHPUtGrVP6fRmVHDn3A7Rw.png\"><br></div>"
            ],
            "guid": "NRp9}}kgXO",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How can one calculate the output volume of a pooling layer given its input size \\(W_{1} \\times H_{1} \\times D_{1}\\), the kernel size \\(F\\) and the stride \\(S\\).",
                "Output size&nbsp;\\(W_{2} \\times H_{2} \\times D_{2}\\) is:<div><br></div><div>\\(W_{2}=\\left(W_{1}-F\\right) / S+1\\)<br>\\(H_{2}=\\left(H_{1}-F\\right) / S+1\\)<br>\\(D_{2}=D_{1}\\)<br><div><br></div><div><br></div><div><img src=\"1FHPUtGrVP6fRmVHDn3A7Rw.png\"><br></div></div>"
            ],
            "guid": "Q|nuDdToz/",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is an <b>activation map</b> in a CNN?",
                "<div>An <b>activation map</b> is the output activations for a given <b>filter </b>in a <b>convoluational layer</b>.</div><div><br></div><div>If different filters are applied one can get different activation maps for the same input data.</div><div><br></div><div><img src=\"paste-46b5fa791349cc46f838d19f8c38d546138eda84.jpg\"><br></div>"
            ],
            "guid": "C-4_Vg6DVh",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What are <b>two problems</b> that arise convolving inputs in <b>CNNs</b>?",
                "<div>\n<div><ul>\n<li>input data from edges and corners is lost</li>\n<li>Sliding and convoling every single pixel is expensive and often not needed</li>\n</ul>\n</div></div>"
            ],
            "guid": "OF))DiAD-a",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain what <b>striding </b>is in the context of <b>CNNs</b>.",
                "We set a stride or step-size for our convolution. This is similar to downsampling the image.<div><br></div><div><img src=\"XD2O4.png\"><br></div>"
            ],
            "guid": "QG9FFTn1Qp",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain what <b>padding</b> is in the context of <b>CNNs</b>.",
                "Fill up the image borders. Zero-padding is most common.<div><br></div><div><img src=\"main-qimg-9e3419cfcd8535fb289bb1b710920d2f.png\"><br></div>"
            ],
            "guid": "bcOuRVT><3",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the main motivation for <b>transfer learning</b>?",
                "Features (convolutional layers) are generic and can be reused.<div><br></div><div>This speeds up training.</div>"
            ],
            "guid": "r$V8h`ZjM2",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain two possible variants of <b>transfer learning</b>.",
                "Train on huge data-set e. g. ImageNet&nbsp;<div><br></div><div>Freeze layers and adapt only last (FC) layers.</div><div><br></div><div><br></div><div><img src=\"paste-f2d8b2017eb2b715e04c819da1590444c2a1f3e1.jpg\"><br></div><div><br></div><div>One can retrain multiple (3.) or one (2.) fully connected layers.<br></div>"
            ],
            "guid": "k0jd5!b7~D",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the 3 main components of a CNN?",
                "<div>\n<div><ul>\n<li>Convolutional layer</li>\n<li>ReLU activation units</li>\n<li>Pooling layer</li>\n</ul>\n</div></div>"
            ],
            "guid": "q4RI]ml2:Y",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the <b>main characteristics</b> of <b>LeNet-5</b>?",
                "<img src=\"paste-daa333c2468ef7552b696b25c85305e1645f4503.jpg\"><div><br></div><div>Convolutional filters were \\(5 \\times 5\\), applied at the stride 1.</div><div><br></div><div>Subsampling (Pooling) layers were \\(2\\times 2\\) applied at stride 2.</div><div><br></div><div>Architecture consists of Convolutinal layer, Pooling layer, Convolutional layer, Pooling layer, Fully-Connected, Fully-Connected.</div>"
            ],
            "guid": "t[t:v<_yEw",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the 4 hyperparameters of a convolutional layer?",
                "<div>\n<div><ul>\n<li>Number of filters \\(K\\)</li>\n<li>Spatial extend kernel size \\(F\\)</li>\n<li>Stride \\(S\\)</li>\n<li>Amount of zero padding \\(P\\)</li>\n</ul>\n</div></div>"
            ],
            "guid": "qDK;R2mn`6",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How can one calculate the output size of a convolutional layer given the parameters<div><ul><li>Number of filters \\(K\\)</li><li>Spatial extend kernel size \\(F\\)</li><li>Stride \\(S\\)</li><li>Amount of zero padding \\(P\\)</li></ul><div>and the input size&nbsp;\\(W_{1} \\times H_{1} \\times D_{1}\\)</div></div>",
                "The output size \\(W_{2} \\times H_{2} \\times D_{2}\\) is:<div><br></div><div>\\(W_{2}=\\left(W_{1}-F+2 P\\right) / S+1\\)<br>\\(H_{2}=\\left(H_{1}-F+2 P\\right) / S+1\\)<br>\\(D_{2}=K\\)<br></div><div><br></div><div><b>Example:</b></div><div>Input volume: \\(32 \\times 32 \\times 3\\)</div><div>10 \\(5 \\times 5\\) filters with stride 1, pad 2.</div><div><br></div><div>Output volume size: \\((32+2 * 2-5) / 1+1=32\\) spatially, so \\(32 \\times 32 \\times 10\\)</div><div><br></div>"
            ],
            "guid": "csb}!7mJa1",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How can one calculculate the number of weights of a convolutional layer of a CNN?",
                "\\(F \\times F \\times D_1 \\times K\\) and \\(K\\) biases, where \\(K\\) is the kernel size and \\(D_1\\) is the depth e. g. no. of pixels of the input.&nbsp;"
            ],
            "guid": "u-H_o1uhQf",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Name different RNN architectures. Also, give an example if possible.",
                "<li><b>one-to-one:</b> Vanilla Neural Network</li>\n<li><b>one-to-many:</b> e. g. Image Captioning (single Image to the sequence of words)</li>\n<li><b>many-to-one: </b>e. g. Sentiment Classification (sequence of words to single sentiment)</li>\n<li><b>many-to-many:</b> e. g. machine translation, sequence of words to sequence of words</li>\n<li><b>many-to-many:</b> e. g. video classification, on frame-level<img src=\"paste-a4df8b815501d43b8d5d499f351ce6a6315c8989.jpg\"></li>"
            ],
            "guid": "PE0z;Q{/w`",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Draw the computation graph of a <b>Many-to-Many RNN</b>.",
                "<img src=\"paste-2e717330e5c31f2144754ac4fbf890829f2f1ce3.jpg\">"
            ],
            "guid": "r%))ogS/3}",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Draw the <b>computation graph</b> of a <b>Many-to-One architecture</b> of a RNN.",
                "<img src=\"paste-29b2fd376c05db4436ec961dad39e9e19b59e447.jpg\">"
            ],
            "guid": "sJM!J(0#d2",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Draw the&nbsp;<b>computation graph</b>&nbsp;of a&nbsp;<b>one-to-many&nbsp;architecture </b>of a <b>RNN</b>.",
                "<img src=\"paste-b6595a9cffefe3291a8dfd031e14c93f7dccd13b.jpg\">"
            ],
            "guid": "t.[)Hl]4z)",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Draw the&nbsp;<b>computation graph</b>&nbsp;of a&nbsp;<b>sequence-to-sequence architecture&nbsp;</b>of a&nbsp;<b>RNN</b>.",
                "<img src=\"paste-dc0452c2c0468c3ed1ffd7bcbaa651da2a215f5a.jpg\"><div><br></div><div>A sequence-to-sequence architecture combines a many-to-one and a one-to-many architecture.</div><div><br></div><div>First Input sequence is encoded into a single vector <b>(Many to one)</b>, this vector is then used to produce the output sequence <b>(One to many)</b>.</div>"
            ],
            "guid": "zfqY~}+2US",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is <b>Backprogopagation through time (BPTT)</b>?",
                "Forward through entire sequence to compute loss, then backward through entire sequence to compute gradient. Backpropagation happens from the very end to the beginning.<div><br></div><div><img src=\"paste-848510cf51999a08809cc308fab0cecaface5a47.jpg\"><br></div>"
            ],
            "guid": "dy^4|~?)YA",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is <b>Truncated backpropagation through time</b>? What is unique about it?",
                "Truncated backpropagation carries hidden states forward in time, but only backpropagate for some smaller number of steps.<div><ul><li>Computationally more efficient</li>\n<li>While hidden state are “more realistic”</li>\n</ul><div><img src=\"paste-8edcac52a727502b1537dcdec121493d93691d94.jpg\"><br></div>\n</div>"
            ],
            "guid": "v!9F[K)c~O",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain the problem of <b>vanishing gradients</b>.",
                "When computing the gradient of \\(h\\) the computation involes many factors of the weight matrix \\(W\\) and a repeated use of the \\(\\operatorname{tanh}\\).&nbsp;<div><br></div><div>If the largest Eigenvalue is smaller than 1, gradients vanish.</div><div><br></div><div><img src=\"paste-91e0d265856f3757aaf9d8d87f19b82bfb26fb42.jpg\"><br></div>"
            ],
            "guid": "C$`Xw3k2J5",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<div>Explain the problem of <b>exploding gradients.</b></div>",
                "When computing the gradient of \\(h\\) the computation involes many factors of the weight matrix \\(W\\) and a repeated use of the \\(\\operatorname{tanh}\\).&nbsp;<div><br></div><div>If the largest eigen value is &gt; 1, gradients explode.</div><div><br></div><div><img src=\"paste-91e0d265856f3757aaf9d8d87f19b82bfb26fb42.jpg\"><br></div>"
            ],
            "guid": "KZjslT#+lw",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How can one solve the problem of <b>exploding gradients</b>?",
                "Use <b>gradient clipping</b> i. E. scale gardients if the norm is too big."
            ],
            "guid": "x)gaskr91m",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How can one solve the problem of&nbsp;<b>vanishing gradients</b>?",
                "By switching the RNN architecture i. E. use LSTMs."
            ],
            "guid": "s-Bgt`Oj4V",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the <b>problem</b> with training <b>deep networks</b>?",
                "One would expect, that models perform at least as well as shallower models. Meaning they yield a better or equal training or test error.&nbsp;<div><br></div><div>In practice deeper models often perform worse, as they are harder to optimize.</div>"
            ],
            "guid": "id[S}4QpR",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain what a GRU is.",
                "<div><div>GRUs&nbsp;are&nbsp;a&nbsp;variant&nbsp;of&nbsp;RNNs&nbsp;using&nbsp;gates&nbsp;to&nbsp;control&nbsp;what&nbsp;information&nbsp;to&nbsp;remember&nbsp;and&nbsp;what&nbsp;to&nbsp;forget.&nbsp;A&nbsp;GRU&nbsp;is&nbsp;a&nbsp;<span style=\"font-weight: bold;\">simpler&nbsp;variant</span>&nbsp;of&nbsp;the&nbsp;LSTM.&nbsp;It&nbsp;contains&nbsp;only&nbsp;an&nbsp;<span style=\"font-weight: bold;\">update&nbsp;gate</span>&nbsp;\\(z_t\\)&nbsp;<span style=\"font-weight: bold;\">(</span>fuse&nbsp;of&nbsp;the&nbsp;forget&nbsp;and&nbsp;input&nbsp;gate&nbsp;<span style=\"font-weight: bold;\">or</span>&nbsp;fuse&nbsp;of&nbsp;long-term&nbsp;and&nbsp;working&nbsp;memory)&nbsp;<span style=\"font-weight: bold;\">and reset&nbsp;gate&nbsp;</span>\\(r_t\\)<span style=\"font-weight: bold;\">&nbsp;</span>with&nbsp;different&nbsp;weights.</div></div><div><br></div><div>Equations given by:</div><div><br></div><div>\\(\\begin{aligned} r_{t} &amp;=\\sigma\\left(W_{x r} x_{t}+W_{h r} h_{t-1}+b_{r}\\right) \\\\ z_{t} &amp;=\\sigma\\left(W_{x z} x_{t}+W_{h x} h_{t-1}+b_{z}\\right) \\\\ \\tilde{h}_{t} &amp;=\\tanh \\left(W_{x h} x_{t}+W_{h h}\\left(r_{t} \\odot h_{t-1}\\right)+b_{h}\\right) \\\\ h_{t} &amp;=z_{t} \\odot h_{t-1}+\\left(1-z_{t}\\right) \\odot \\tilde{h}_{t} \\end{aligned}\\)<br></div>"
            ],
            "guid": "f^/h1/mDT=",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the advantage / disadvantage of <b>GRUs</b> over <b>LSTMs</b>?",
                "<div><br><div><ul><br><li>GRUs have fewer parameters</li><br><li>GRUs deliver similar performance compared to LSTMs</li><br></ul><br></div></div>"
            ],
            "guid": "ooq/@YCQ5}",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain how updating a <b>GRU cell</b> works.",
                "<div>\n<div><ul>\n<li>Recall there are two gates:\n<ul>\n<li>The update gate \\(z_t\\)</li>\n<li>The reset gate \\(r_t\\)​</li>\n</ul>\n</li>\n<li>The first step is to calculate the <strong>update gate</strong> \\(z_t\\), which decides what information to throw away and what new information to add.  We plugin the input \\(x_t\\)​ and the previous hidden state \\(h_{t-1}\\)​, which holds the information for the previous \\(t-1\\)&nbsp;timesteps. Both are multiplied with their respective Weight matrices, added and squashed between \\(0\\)&nbsp;and \\(1\\) using the sigmoid function. An update gate close to \\(1\\) would implies, that information is copied through many times.</li>\n<li>The next step is to calculate the <strong>reset gate</strong>&nbsp;\\(r_t​\\).The <strong>reset gate</strong> decides how much past information to forget. The idea is similar to the <strong>update gate</strong>.</li>\n<li>In the next step is to calculate the current memory content \\(\\tilde{h}_t\\). First, we multiply the input \\(x_t\\)​ by its weight matrix and then use the Hadamard product of the previous hidden state \\(h_{t-1}\\)&nbsp;and the forget gate \\(r_t\\). A <strong>reset gate</strong> close to 0&nbsp;would mean the previous hidden state is ignored. The sum of both is plugged into a \\(\\tanh\\).</li>\n<li>In the last step we calcualte the new hth_tht​, a vector holding information for the current unit. It is (weighted) sum of the hadamard product between the update gate \\(z_t\\) ​and memory content \\(h_{t-1}\\) and&nbsp;\\((1-z_t)\\). The result is saved in \\(h_t\\).</li>\n</ul><div><img src=\"gru.png\"><br></div>\n</div></div>"
            ],
            "guid": "i7UunK@HF(",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How does backpropagation work for LSTM cells?",
                "<img src=\"paste-435c27b7deceeb7bfcdffff568c470a041d52aa1.jpg\"><div><br></div><div>Backpropagating from \\(c_t\\) to \\(c_{t-1}\\) only elementwise multiplication by \\(f\\), no matrix multiply by \\(W\\).</div>"
            ],
            "guid": "q_X_^P#@d5",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain what characterizes the <b>VGG Net</b>.",
                "<div>\n<div><ul>\n<li>16-19 layer (VGG16Net)</li>\n<li>Only 3x3 Convolutional layer with stride 1, padding 1</li>\n<li>2x2 max Pooling layer with stride 2</li>\n</ul>\n</div></div><div><br></div><div><img src=\"paste-c399ce6fe7b5c7a6a71cc35807bc955872ac6908.jpg\"><br></div>"
            ],
            "guid": "Ha,ZCwmYWi",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Compare the <b>AlexNet</b> to the <b>VGG16 / VGG19</b>.",
                "VGG16 and VGG19 feature more <b>convolutional layer</b>.<div><br></div><div><img src=\"paste-cb47415c1a6b36e4f26a7612dcb65b9ea4171540.jpg\"><br></div>"
            ],
            "guid": "AJmK!$|k`+",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What CNN architecture featured the use of ReLU for the first time?",
                "AlexNet"
            ],
            "guid": "FJr-ZBsh0&",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain what is characteristic about <b>AlexNet</b>.",
                "[227x227x3] INPUT (images of size 227 x 227)<br>[55x55x96] CONV1 : 96 11x11 filters at stride 4, pad 0<br>[27x27x96] MAX POOL1 : 3x3 filters at stride 2<br>[27x27x96] NORM1 : Normalization layer<br>[27x27x256] CONV2 : 256 5x5 filters at stride 1, pad 2<br>[13x13x256] MAX POOL2 : 3x3 filters at stride 2<br>[13x13x256] NORM2 : Normalization layer<br>[13x13x384] CONV3 : 384 3x3 filters at stride 1, pad 1<br>[13x13x384] CONV4 : 384 3x3 filters at stride 1, pad 1<br>[13x13x256] CONV5 : 256 3x3 filters at stride 1, pad 1<br>[6x6x256] MAX POOL3 : 3x3 filters at stride 2<br>[4096] FC6 : 4096 neurons<br>[4096] FC7 : 4096 neurons<br>[1000] FC8 : 1000 neurons (class scores)<br><div><br></div><div><img src=\"paste-01d904854611ff65e1ec4aa224c552e774baaf73.jpg\"><br></div>"
            ],
            "guid": "w9FBXxpHUB",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is characteristic about <b>ResNet</b>?",
                "<div>\n<div><ul>\n<li>152-layer model for ImageNet</li>\n<li>Uses so-called residual blocks / layers</li><li>Every residual block has two 3x3 convolutional layer</li><li>Periodically, double # of filters and downsample spatially using stride 2 (/2 in each dimension)</li><li>Additional convolutional layer at the beginning</li><li>No Fully-Connected layers at the end (only FC 1000 to output classes)</li><li>For deeper networks (ResNet-50+) use \"bottleneck\" layer to improve efficiency</li>\n</ul><div><img src=\"paste-cae52d4bc93e36739c26476b7256984ff7e4c919.jpg\"><br></div>\n</div></div>"
            ],
            "guid": "ft>F_eS6-Z",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Residual layers are a technique to make networks deeper.<div><br></div><div>Explain them.</div>",
                "<div>In networks with residual blocks, each layer feeds into the next layer and directly into the layers about 2-3 hops away.</div><div><br></div><div>Use network layers to fit a <b>residual mapping</b> instead of directly trying to fit a desired underlying mapping.</div><div><br></div><div>Use layers to fit residual&nbsp;\\(F(x)=H(x)-x\\) instead of \\(H(x)\\) directly. H(x) is the true distribution, we would like to learn. Learining \\(F(x)=H(x)-x\\) is much easier.</div><div><br></div><div>Initially, \\(F(x)\\) is set to 0, so the layer just computes the identity.&nbsp;</div><div><br></div><div>In the image below one has an identity connection from \\(x\\), so the layers are actually traing to learn the residuals \\(F(x)\\). Hence, the residual block.<br><br></div><div><img src=\"paste-a795048d2bef92bdbe249d3a19081382d6fa2a52.jpg\"></div>"
            ],
            "guid": "gF:gq!b$*W",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain the main components of a LSTM Cell.",
                "\n<div><ul>\n<li>LSTMs are a type of RNNs, that features a sophisticated <strong>forgetting</strong> and <strong>saving mechanism</strong>. To implement this, a LSTM is made up of four interacting <strong>layers</strong> and three <strong>gates</strong>.</li>\n<li>Gates \nregulate what information is saved and removed from the cell state an \nLSTM cell. Gates implement a way to let information through. They are \ncomposed of a sigmoid neural net layer and pointwise multiplication \noperation (the hadamard product). If the output of a sigmoid layer is \nclose to 0, hardly any information is passed through and vice versa if it’s close to 1.</li>\n<li>Recall there are four gates:\n<ul>\n<li><strong>forget gate:</strong> whether to erase a cell</li>\n<li><strong>input gate:</strong> how much to write to a cell</li>\n<li><strong>gate gate:</strong> what to write to a cell (this is no gate in a closer sense?)</li>\n<li><strong>output gate:</strong> how much to reveal a cell</li>\n</ul>\n</li>\n</ul>\n</div>"
            ],
            "guid": "Dt;yGa)V{=",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain how calculating the next memory state \\(c_t\\) of a LSTM cell works.",
                "<div><b>Forget gate</b> \\(f\\) (whether to erase cell)</div><div>\\(\\boldsymbol{f}_{t}=\\sigma\\left(\\boldsymbol{W}_{f}\\left[\\begin{array}{c}\\boldsymbol{h}_{t-1} \\\\ \\boldsymbol{x}_{t}\\end{array}\\right]\\right)\\)<br></div><div><br></div><div><b>Input gate </b>\\(i\\) (whether to write to cell)</div><div>\\(i_{t}=\\sigma\\left(\\begin{array}{c}\\boldsymbol{W}_{i} &amp; {\\left[\\begin{array}{c}\\boldsymbol{h}_{t-1} \\\\ \\boldsymbol{x}_{t}\\end{array}\\right]}\\end{array}\\right)\\)<br></div><div><br></div><div><b>Gate gate </b>\\(g\\) (how much to write to cell)</div><div>\\(\\boldsymbol{g}_{t}=\\tanh \\left(\\boldsymbol{W}_{g}\\left[\\begin{array}{c}\\boldsymbol{h}_{t-1} \\\\ \\boldsymbol{x}_{t}\\end{array}\\right]\\right)\\)<br></div><div><br></div><b>Output gate</b> \\(o\\) (how much to reveal to cell)<br><div>\\(\\boldsymbol{o}_{t}=\\sigma\\left(\\boldsymbol{W}_{o}\\left[\\begin{array}{c}\\boldsymbol{h}_{t-1} \\\\ \\boldsymbol{x}_{t}\\end{array}\\right]\\right)\\)<br></div><div><br></div><div>Next memory state</div><div>\\(\\boldsymbol{c}_{t}=\\boldsymbol{f}_{t} \\circ \\boldsymbol{c}_{t-1}+\\boldsymbol{i}_{t} \\circ \\boldsymbol{g}_{t}, \\quad \\boldsymbol{h}_{t}=\\boldsymbol{o}_{t} \\circ \\tanh \\left(\\boldsymbol{c}_{t}\\right)\\)<br></div>"
            ],
            "guid": "kP[bmMn4n6",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is unique about Reccurrent Neural Networks?",
                "We can process a sequence of vectors \\(x\\) by applying a reccurence formula at every time step:<div><br></div><div>\\(\\boldsymbol{h}_{t}=f_{\\boldsymbol{W}}\\left(\\boldsymbol{h}_{t-1}, \\boldsymbol{x}_{t}\\right)\\)<br></div><div><br></div><div>where \\(\\boldsymbol{h}_t\\) is the new hidden state, \\(\\boldsymbol{h}_{t-1}\\) is the previous hidden state, \\(x_t\\) is the input and \\(f_{\\boldsymbol{W}}\\) some function with parameters \\(\\boldsymbol{W}\\) e. g. \\(\\tanh\\) function. \\(\\boldsymbol{W}\\) is the weight matrix.</div><div><br></div><div>The state consists of a single \"hidden\" vector \\(h\\) for \\(f = \\tanh\\):</div><div>\\(\\boldsymbol{h}_{t}=\\tanh \\left(\\boldsymbol{W}_{h h} \\boldsymbol{h}_{t-1}+\\boldsymbol{W}_{x h} \\boldsymbol{x}_{t}\\right)\\)<br>\\(\\boldsymbol{y}_{t}=\\boldsymbol{W}_{h y} \\boldsymbol{h}_{t}\\)<br></div><div><br></div><div>One can clearly see that the hidden state for \\(t\\) is dependent on the Weightmatrix for the last internal state and the input of the neural network. tanh is used to compute the next internal state.<br></div><div><br></div><div>Note that same parameters and function are used.&nbsp;</div><div><br></div><div>As we dependent on the previous step, we require a recursive calculation. Hence, the name <b>Recurrent Neural Net</b>.</div><div><br></div><div><img src=\"paste-b5b85fd9853b59c76953a26ca1100700dd7c43c3.jpg\"><br></div><div><img src=\"https://firebasestorage.googleapis.com/v0/b/gitbook-28427.appspot.com/o/assets%2F-MWnh07mnETIlqhjJW0c%2F-Mez-oZInzLyLqK64v0B%2F-Mez8cmuRa0y6Wlhj4Jq%2Fcomputational_graph_rnn.JPG?alt=media&amp;token=a14b29dc-ec2d-4067-917a-01c0cb0bd00d\"><br></div>"
            ],
            "guid": "he}$$h#3.",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the <b>difference </b>between <b>first orde</b>r and <b>second order optimization</b>?",
                "<div>\n<div><div><strong>First order optimization</strong></div>\n<ul>\n<li>Use gradient to form linear approximation</li>\n<li>Step in the direction of the minimum of the approximization</li>\n</ul><div><img src=\"paste-e2f42c11feb690d750e38e5a51e69153d11399d6.jpg\"><br></div>\n<div><strong>Second order optimization</strong></div>\n<ul>\n<li>Use gradient and Hessian to form quadratic approximization</li>\n<li>Step to the minimum of approximation</li>\n</ul><div><img src=\"paste-c8ebb5806f43f725bd542b66b71cdcacf753d5b6.jpg\"><br></div>\n</div></div>"
            ],
            "guid": "NcpwZK+.gg",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Name two popular <b>second order methods</b> for optimization",
                "<div>\n<div><ul>\n<li>BFGS</li>\n<li>L-BFGS (not suitable for mini-batches)</li>\n</ul>\n</div></div>"
            ],
            "guid": "caSw7F/&WE",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is an optimizer used for in Neural networks?",
                "Optimizers are \nmathematical functions which are dependent on model's learnable \nparameters i.e Weights &amp; Biases.&nbsp;<div><br></div><div>Optimizers help to know how to change weights and learning rate of neural network to reduce the losses.</div>"
            ],
            "guid": "f&&h[ovODm",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Name problems that arise when using <b>stochastic gradient descent</b>?",
                "<div>\n<div><ul>\n<li><strong>Loos changes quickly in one direction and slowly in another</strong> very slow progress along shallow dimension, jitter along steep direction</li>\n<li><strong>Loss function has local minima and plateaus</strong> gradient descent gets stuck, if gradient is zero</li>\n<li><strong>loss function is noisy</strong>, due to minibatches</li>\n</ul>\n</div></div>"
            ],
            "guid": "K>kZVu@TnK",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the advantages and disadvantages of <b>SGD</b> with <b>momentum term</b>?",
                "<div>\n<div><div><strong>advantages</strong></div>\n<ul>\n<li>Momentum helps to reduce the noise.</li>\n<li>Exponential Weighted Average is used to smoothen the curve.</li>\n</ul>\n<div><strong>disadvantages</strong></div>\n<ul>\n<li>Extra hyperparameter is added.</li>\n</ul>\n</div></div>"
            ],
            "guid": "I(#WJTHh3.",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the inuitition of adding a <b>momentum term</b> to <b>stochastic gradient descent</b>?",
                "Moment simulates the inertia (slow adaption) of an object when its moving, that is, the direction of the previous update is retained to a certain extent during the update, while the current update gradient is used to fine-tune the final update direction.&nbsp;<div><br></div><div>By doing so we increase stability and avoid local optimizations.</div><div><br></div><div><img src=\"SGD.jpg\"><br></div>"
            ],
            "guid": "o-Wm5=jVUu",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How does one calculate the gradient update with SGD with momentum term?",
                "<br><div>\\(\\boldsymbol{\\theta}_{k+1}=\\boldsymbol{\\theta}_{k}-\\eta \\boldsymbol{m}_{k+1}\\)<br></div><div><br></div><div>where \\(m\\) can be calculated using the geometric average (constant \\(\\gamma\\)) or adaptive \\(\\gamma\\) using the&nbsp;</div><div>arithmetic average.</div><div><br></div><div>Geometric average (constant \\(\\gamma\\)):</div><div>\\(\\boldsymbol{m}_{k}=(1-\\gamma) \\sum_{i=1}^{k} \\gamma^{k-i} \\boldsymbol{g}_{i}\\)<br></div><div><br></div><div>Arithmetic average \\(\\left(\\gamma_{k}=(k-1) / k\\right)\\):</div><div>\\(\\boldsymbol{m}_{k}=\\frac{1}{k} \\sum_{i=1}^{k} \\boldsymbol{g}_{i}\\)</div><div><br></div><div><img src=\"SGD.jpg\"><br></div>"
            ],
            "guid": "ju51ZmQ$c}",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the intuition of <b>RMS-prop / Gradient Normalisation</b>?&nbsp;",
                "IN RMS-Prop the learning rate is an exponential average of the gradients instead of a cumulative sum of squared gradients.&nbsp;<div><br></div><div>In plateaus we take smaller steps, as they do not have much risk. In steep areas take smaller steps.</div>"
            ],
            "guid": "Po4-$4X7xV",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the <b>advantages / disadvantages</b> of <b>RMSProp / Gradient Normalisation</b>?",
                "<div><strong>Advantages</strong></div>\n<ul>\n<li>In RMS-prop learning rate gets adjusted automatically and it chooses different learning rate for each parameter</li>\n</ul>\n<div><strong>Disadvantages</strong></div>\n<ul>\n<li>slow learning</li></ul>"
            ],
            "guid": "v5k<?X%(JA",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>gradient normalisation</b> / <b>RMSProp</b>.",
                "\\(\\begin{aligned} \\boldsymbol{g}_{k} &amp;=\\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}\\left(\\boldsymbol{\\theta}_{k}\\right) \\\\ \\boldsymbol{v}_{k+1, i} &amp;=\\gamma \\boldsymbol{v}_{k, i}+(1-\\gamma) \\boldsymbol{g}_{k, i}^{2} \\\\ \\boldsymbol{\\theta}_{k+1, i} &amp;=\\boldsymbol{\\theta}_{k, i}-\\frac{\\eta}{\\sqrt{\\boldsymbol{v}_{k+1, i}+\\epsilon}} \\boldsymbol{g}_{k, i} \\end{aligned},\\)<div><br></div><div><br></div><div>where \\(\\boldsymbol{v}_{k+1, i}\\) computes running average of the squared gardients (root mean square). RMSProp uses small \\(\\epsilon\\) to prevent division by zero.</div>"
            ],
            "guid": "kA86UaxqOK",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the advantages / disadvantages of <b>Adaptive Momentum / Adam</b>?",
                "<div><strong>Advantages</strong></div>\n<ul>\n<li>Easy to implement</li>\n<li>Computationally efficient.</li>\n<li>Little memory requirements.</li>\n</ul>\n<div><strong>Disadvantages</strong></div>\n<ul>\n<li>Convergence not always guaranteed</li></ul>"
            ],
            "guid": "P6{sr7(NI]",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the intuition behind <b>Adaptive Momentum</b> / <b>Adam</b>?",
                "<div>Combine momentum term of stochastic gradient descent with momentum term with gradient normalization.</div><div><br></div><div>Adam stores the decaying average of past gradients and also decaying average of past squared gradients.</div><div><br></div>"
            ],
            "guid": "nX+<ILsmX2",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give the definition of Adaptive Momentum (Adam).",
                "\\(\\begin{aligned} \\boldsymbol{g}_{k} &amp;=\\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}\\left(\\boldsymbol{\\theta}_{k}\\right) \\\\ \\boldsymbol{v}_{k+1, i} &amp;=\\gamma_{1} \\boldsymbol{v}_{k, i}+\\left(1-\\gamma_{1}\\right) \\boldsymbol{g}_{k, i}^{2} \\quad \\ldots \\text { gradient norm } \\\\ \\boldsymbol{m}_{k+1} &amp;=\\gamma_{2} \\boldsymbol{m}_{k}+\\left(1-\\gamma_{2}\\right) \\boldsymbol{g}_{k} \\quad \\ldots \\text { momentum } \\\\ \\boldsymbol{\\theta}_{k+1, i} &amp;=\\boldsymbol{\\theta}_{k, i}-\\underbrace{\\frac{\\eta c_{2}(k)}{\\sqrt{c_{1}(k) \\boldsymbol{v}_{k+1, i}+\\epsilon}}}_{\\text {norm-based scaling }} \\\\ &amp; \\boldsymbol{m}_{k+1, i} \\end{aligned}\\)"
            ],
            "guid": "Bjlo&6P]-V",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the central part of <b>second order optimization methods</b>?",
                "Usage of taylor approximization:<div><br></div><div>\\[<br>\\mathcal{L}(\\boldsymbol{\\theta}) \\approx \\mathcal{L}\\left(\\boldsymbol{\\theta}_{0}\\right)+\\left(\\boldsymbol{\\theta}-\\boldsymbol{\\theta}_{0}\\right)^{T} \\boldsymbol{g}+\\left(\\boldsymbol{\\theta}-\\boldsymbol{\\theta}_{0}\\right)^{T} \\boldsymbol{H}\\left(\\boldsymbol{\\theta}-\\boldsymbol{\\theta}_{0}\\right)<br>\\]<br>With \\(\\boldsymbol{g}=\\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}(\\boldsymbol{\\theta})\\) is the gradient and \\(\\boldsymbol{H}=\\nabla_{\\boldsymbol{\\theta}}^{2} \\mathcal{L}(\\boldsymbol{\\theta})\\) is the Hessian matrix.</div><div><br></div><div>Solving for \\(\\theta\\) yields the Newton update.</div><div>\\(\\boldsymbol{\\theta}^{*}=\\boldsymbol{\\theta}_{0}-\\boldsymbol{H}^{-1} \\boldsymbol{g}\\)<br></div><div><br></div>"
            ],
            "guid": "v5HG5UL%E4",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the central idea of <b>second order optimization methods</b>?",
                "<ul><li>Use gradient and Hessian to form quadratic approximization</li><li>Step to the minimum of approximation</li></ul><div><img src=\"paste-c8ebb5806f43f725bd542b66b71cdcacf753d5b6.jpg\"></div>"
            ],
            "guid": "EIVN%^oG=.",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the <b>advantages / disadvantages</b> of <b>second order optimization methods</b>?",
                "<div>\n<div><div><strong>advantages</strong></div>\n<ul>\n<li>no hyperparameters</li>\n<li>no learning rate</li>\n<li>less iterations required</li>\n</ul>\n<div><strong>disadvantages</strong></div>\n<ul>\n<li>Hessian has \\(\\mathcal{O}(N^2)\\) parameters</li>\n<li>Inverse is \\(\\mathcal{O}(N^3)\\) (problematic for large \\(n\\))</li>\n</ul>\n</div></div>"
            ],
            "guid": "nq)n)!|NSJ",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the main two steps in the <b>backpropagation algorithm</b>?",
                "<div>\n<div><ol>\n<li>forward pass</li>\n<li>backward pass</li>\n</ol>\n</div></div>"
            ],
            "guid": "zhZfHl<+wV",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain the Backpropagation algorithm.",
                "Let \\(v_1 \\cdots v_N\\) be a topical ordering of the computation graph (i. e. parents come before children.)<br><br>\\(V_N\\) denotes the variable we're trying to compute derivates e. g. loss.<div><br></div><div><img src=\"paste-e1dd8c4cb6b5298053e89ee19e0448f8fb3389fa.jpg\"><br><div><br></div><div><i>Forward pass:</i></div><div>For \\(i=1, \\ldots, N\\)<br>&nbsp; &nbsp; &nbsp; Compute \\(v_{i}\\) as a function of \\(\\operatorname{Pa}\\left(v_{i}\\right)\\)<i><br></i></div><div><br></div><div>\\(\\overline{v_{N}}=1\\)<br>For \\(i=N-1, \\ldots, 1\\)<br>&nbsp; &nbsp; &nbsp; \\(\\quad \\overline{v_{i}}=\\sum_{j \\in \\operatorname{Ch}\\left(v_{i}\\right)} \\overline{v_{j}} \\frac{\\partial v_{j}}{\\partial v_{i}}\\),<br></div><div><br></div><div>where \\(\\operatorname{Ch}\\) is the children node and \\(\\operatorname{Pa}\\) is the parent node.</div></div>"
            ],
            "guid": "niw;O*)Y*i",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is referred to as <b>fan-out</b>?",
                "Number of outgoing network connections from one layer.<div><br></div><div><b>Example </b>(regularized regression):</div><div><br></div><div>See purple lines below</div><div><br></div><div><img src=\"paste-f4ea162621a06aece55728cbef89f943b1591026.jpg\"><br></div>"
            ],
            "guid": "H)r,:lCLWS",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give the definition of the <b>multivariate chain rule</b>.<div><br></div><div>Assume the following graph:<br><div><br></div><div><img src=\"paste-0547953726d47f9767f7c17559c163c8bab8ee26.jpg\"><br></div></div>",
                "\\(\\frac{d}{d t} f(x(t), y(t))=\\frac{\\partial f}{\\partial x} \\frac{d x}{d t}+\\frac{\\partial f}{\\partial y} \\frac{d y}{d t}\\)<div><br></div><div>Note that \\(\\partial f\\) has already been computed by the algorithm. And \\({dt}\\) is the expression to be evaluated.</div><div><br></div><div>\\(\\bar{t}=\\bar{x} \\frac{d x}{d t}+\\bar{y} \\frac{d y}{d t}\\)<br></div>"
            ],
            "guid": "xij(@L;Tb;",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Calculate the derivative for the following computational graph using the chain rule.<div><br></div><div><img src=\"paste-976fda0679c3ce7eff570a48ef8bde4ee4e178af.jpg\"><br></div><div><br></div><div>\\(\\begin{aligned} f(x, y) &amp;=y+\\exp (x y) \\\\ x(t) &amp;=\\cos t \\\\ y(t) &amp;=t^{2} \\end{aligned}\\)<br></div>",
                "<div>Multivariate chain rule:</div><div>\\(\\frac{d}{d t} f(x(t), y(t))=\\frac{\\partial f}{\\partial x} \\frac{d x}{d t}+\\frac{\\partial f}{\\partial y} \\frac{d y}{d t}\\)<br></div><div><br></div><div>\\(\\begin{aligned} \\frac{d}{d t} f(x(t), y(t))=&amp; \\frac{\\partial f}{\\partial x} \\frac{d x}{d t}+\\frac{\\partial f}{\\partial y} \\frac{d y}{d t} \\\\=&amp;(y \\exp (x y))(-\\sin t) \\\\ &amp;+(1+x \\exp (x y))(2 t) \\end{aligned}\\)<br></div><div><br></div>"
            ],
            "guid": "N)oOB8dhWR",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "10_neural_networks"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give the definition of <b>generalized linear regression models / linear basis function models</b>.",
                "\\(f(\\boldsymbol{x})=\\boldsymbol{\\phi}(\\boldsymbol{x})^{T} \\boldsymbol{w},\\)<div><br></div><div>where \\(\\phi(\\boldsymbol{x})\\) is a vector valued function of the input vector \\(\\boldsymbol{x}\\). So \\(\\boldsymbol{x}\\) gets transformed by this function. That is, \\(\\boldsymbol{y}\\) is no longer linearily dependent on \\(\\boldsymbol{x}\\).&nbsp;</div><div><br></div><div><b>Example:</b></div><div><br></div><div>\\[<br>f(\\boldsymbol{x})=\\boldsymbol{\\phi}(\\boldsymbol{x})^{T} \\boldsymbol{w}<br>\\]<br>where<br>\\[<br>\\boldsymbol{w}=\\left[\\begin{array}{l}<br>w_{0} \\\\<br>w_{1} \\\\<br>w_{2} \\\\<br>w_{3}<br>\\end{array}\\right], \\quad \\phi(\\boldsymbol{x})=\\left[\\begin{array}{c}<br>1 \\\\<br>x \\\\<br>x^{2} \\\\<br>x^{3}<br>\\end{array}\\right]<br>\\]<br></div><div><br></div><div><img src=\"paste-6fd93cf457a811c4f11916b6f814266eb94ad44d.jpg\"><br></div>"
            ],
            "guid": "A5WF0%Iiq/",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "01_linear_regression",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Name one technique to prevent overfitting with <b>regression models</b>",
                "Regularization."
            ],
            "guid": "F[~38i7BXk",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "01_linear_regression"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the main idea of <b>regularization</b>?",
                "Limit the model such that it can not fit the training data perfectly any more.<div><br></div><div>One introduces a <b>regularization penalty</b> in cost function:</div><div>\\(E_{D}(\\mathbf{w})+\\lambda E_{W}(\\mathbf{w}),\\)<br></div><div><br></div><div>where \\(E_D\\) is the error term from data, \\(E_W\\) is the regularization term and \\(\\lambda\\) is the regularization factor.</div>"
            ],
            "guid": "qv]s*8B0yU",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "01_linear_regression",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give an intuition for overfitting with regard to training and test error.",
                "<div>\n<div><ul>\n<li>Training error goes down</li>\n<li>Test error goes up&nbsp;</li><li>The model is too complex.</li>\n<li>It fits the noise and has unspecified behavior between the training points.</li>\n</ul>\n</div></div>"
            ],
            "guid": "NBNMq4q!S+",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "01_linear_regression"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give an intuition for <b>underfitting</b> with regard to <b>training</b> and <b>test error</b>.",
                "<div>\n<div><ul>\n<li>Training + test error are high</li>\n<li>The model is too simple to fit the data</li>\n</ul>\n</div></div>"
            ],
            "guid": "pY?t6A}DoW",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "01_linear_regression"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain how one can find the optimal \\(\\boldsymbol{w}\\), that minimizes SSE.<div><br></div><div>\\(\\boldsymbol{w}^{*}=\\operatorname{argmin}_{\\boldsymbol{w}} \\mathrm{SSE}\\)<br></div>",
                "One tries to find&nbsp;\\(\\frac{\\partial \\mathrm{SSE}}{\\partial \\boldsymbol{w}}=\\mathbf{0}^{T}\\).<div><br></div><div>\\(\\begin{aligned}<br>\\operatorname{SSE}(\\boldsymbol{w}) &amp;=(\\boldsymbol{y}-\\boldsymbol{X} \\boldsymbol{w})^{T}(\\boldsymbol{y}-\\boldsymbol{X} \\boldsymbol{w}) \\\\<br>&amp;=\\boldsymbol{w}^{T} \\boldsymbol{X}^{T} \\boldsymbol{X} \\boldsymbol{w}-\\boldsymbol{y}^{T} \\boldsymbol{X} \\boldsymbol{w}-\\boldsymbol{w}^{T} \\boldsymbol{X}^{T} \\boldsymbol{y}+\\boldsymbol{y}^{T} \\boldsymbol{y} \\\\<br>&amp;=\\boldsymbol{w}^{T} \\boldsymbol{X}^{T} \\boldsymbol{X} \\boldsymbol{w}-2 \\boldsymbol{y}^{T} \\boldsymbol{X} \\boldsymbol{w}+\\boldsymbol{y}^{T} \\boldsymbol{y}<br>\\end{aligned}\\)<br></div><div><br></div><div>Take the derivative w.r.t. \\(\\boldsymbol{w}\\).</div><div>\\(\\begin{aligned}<br>\\nabla_{\\boldsymbol{w}} \\operatorname{SSE}(\\boldsymbol{w})&amp;=\\frac{\\partial}{\\partial \\boldsymbol{w}}\\left\\{\\boldsymbol{w}^{T} \\boldsymbol{X}^{T} \\boldsymbol{X} \\boldsymbol{w}-2 \\boldsymbol{y}^{T} \\boldsymbol{X} \\boldsymbol{w}+\\boldsymbol{y}^{T} \\boldsymbol{y}\\right\\}\\\\<br>&amp;= \\boldsymbol{X} \\boldsymbol{X}^{T} \\boldsymbol{X} \\boldsymbol{w} - \\boldsymbol{X} \\boldsymbol{X}^{T} \\boldsymbol{y} = 0&nbsp; <br>\\end{aligned}\\)<br></div><div><br></div><div><br></div><div>Setting the gradient to 0 yields:</div><div>\\(\\boldsymbol{w}^{*}=\\left(\\boldsymbol{X}^{T} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{T} \\boldsymbol{y}\\)<br></div><div><br></div>"
            ],
            "guid": "Qb@t*Vd(lx",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "01_linear_regression",
                "checklater",
                "formula_sheet"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Why is it possible to solve the <b>least squares solution</b> in closed form?",
                "<div>\n<div><ul>\n<li>the cost-function is convex, so the minium is easy to obtain</li>\n<li>as the cost function is quadratic in \\(w\\), there is only one minimum</li>\n</ul>\n</div></div>"
            ],
            "guid": "NTXiErQ7R4",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "01_linear_regression"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give the closed-form solution for \\(\\boldsymbol{w}^{*}\\) for the generalized linear regression.",
                "\\(\\begin{aligned}<br>\\boldsymbol{w}^{*}=\\left(\\boldsymbol{\\Phi}^{T} \\boldsymbol{\\Phi}\\right)^{-1} \\boldsymbol{\\Phi}^{T} \\boldsymbol{y} \\\\<br>\\text { with } \\boldsymbol{\\Phi}=\\left[\\begin{array}{c}<br>\\boldsymbol{\\phi}_{1}^{T} \\\\<br>\\vdots \\\\<br>\\boldsymbol{\\phi}_{n}^{T}<br>\\end{array}\\right] &amp;<br>\\end{aligned}\\)<div><br></div><div>Data matrix is replaced by the basis function matrix.</div>"
            ],
            "guid": "bBmc|0?q3U",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "01_linear_regression"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give the definition of <b>generalized ridge regression</b> / weight decay.",
                "\\(L_{\\text {ridge }}=(\\boldsymbol{y}-\\boldsymbol{\\Phi} \\boldsymbol{w})^{T}(\\boldsymbol{y}-\\boldsymbol{\\Phi} \\boldsymbol{w})+\\lambda \\boldsymbol{w}^{T} \\boldsymbol{w}\\)<div><br></div><div>with&nbsp;</div><div><br></div><div>\\(\\mathbf{\\Phi}=\\left[\\begin{array}{c}<br>\\boldsymbol{\\phi}_{1}^{T} \\\\<br>\\vdots \\\\<br>\\boldsymbol{\\phi}_{n}^{T}<br>\\end{array}\\right]\\)<br></div>"
            ],
            "guid": "H&%3j+/b,v",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "01_linear_regression",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give the optimal solution for \\(\\boldsymbol{w}^{*}\\) for <b>ridge regression</b>.",
                "\\(\\boldsymbol{w}_{\\text {ridge }}^{*}=\\left(\\boldsymbol{\\Phi}^{T} \\boldsymbol{\\Phi}+\\lambda \\boldsymbol{I}\\right)^{-1} \\mathbf{\\Phi}^{T} \\boldsymbol{y}\\),<div><br></div><div>where \\(\\boldsymbol{I}\\) is the identity matrix.&nbsp;</div><div><br></div><div>Compare this to the <b>generalized linear regression solution</b>:</div><div>\\(\\boldsymbol{w}^{*}=\\left(\\boldsymbol{\\Phi}^{T} \\boldsymbol{\\Phi}\\right)^{-1} \\boldsymbol{\\Phi}^{T} \\boldsymbol{y}\\)<br></div><div><br></div>"
            ],
            "guid": "jlz^rylV3f",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "01_linear_regression",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the impact <b>low / high variance</b> and <b>low / high bias</b>?",
                "<div>\n<div><ul>\n<li><b>Low variance, high bias:</b> Underfitting</li>\n<li><b>High variance, low bias:</b> Overfitting</li>\n<li><b>High variance, high bias:</b> something is terribly wrong</li>\n<li><b>Low variance, low bias:</b> too good to be true</li>\n</ul><div><img src=\"paste-ace22c55511538845e267e883707b20a9bba3b4f.jpg\"><br></div>\n</div></div>"
            ],
            "guid": "J[cS!XCH/j",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "03_model_selection"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Name hyperparameters of the <b>Linear Regression</b>, the <b>Decision Tree</b>, <b>Neural Network</b>, <b>SVM</b> and <b>Gaussian Processes</b>.",
                "<div>\n<div><ul>\n<li>Linear Regression: number of features, regularization coefficient</li>\n<li>Decision Trees: maximum depth, number of leaves</li>\n<li>Neural Networks: number of layers, number of neurons</li>\n<li>Support Vector Machine: which features, regularization</li>\n<li>Gaussian Processes: kernel bandwith</li>\n</ul>\n</div></div>"
            ],
            "guid": "H(ZK=.AOWZ",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "03_model_selection",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How does the <b>emprirical </b>and <b>true risk</b> behave in case of <b>overfitting</b>?",
                "Empirical risk is zero. True risk is huge."
            ],
            "guid": "wSvJ{T_^~b",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "03_model_selection"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How does <b>empirical </b>and <b>true risk</b> relate to <b>overfitting </b>and <b>underfitting</b>?",
                "<img src=\"paste-4e6a2de0523f3a0224bf4d08b835eec6926c1257.jpg\"><br><div><br></div><div><b>Overfitting:</b> small empirical risk, but high true risk</div><div><b>Underfitting:</b> high empirical risk and true risk</div>"
            ],
            "guid": "q/H#]~v1SN",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "03_model_selection"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Name the 3 components of the expected loss.",
                "\\(\\text { Expected Loss }=\\text { Variance }+\\text { Bias }^{2}+\\text { Noise }\\)<div><br></div><div>\\(\\begin{aligned}<br>R\\left(\\hat{f}_{D_{n}}\\right) &amp;=\\mathbb{E}_{D_{n}}\\left[\\mathbb{E}_{x, y}\\left[\\left(\\hat{f}_{D_{n}}(x)-y\\right)^{2}\\right]\\right] \\\\<br>&amp;=\\underbrace{\\mathbb{E}_{D_{n}}\\left[\\mathbb{E}_{x, y}\\left[\\left(\\hat{f}_{D_{n}}(x)-\\hat{f}_{*}(x)\\right)^{2}\\right]\\right]}_{\\text {Variance }}+\\underbrace{\\mathbb{E}_{x, y}\\left[\\left(\\hat{f}_{*}(x)-f(x)\\right)^{2}\\right]}_{\\text {Bias }^{2}}+\\underbrace{\\sigma^{2}}_{\\text {noise }}<br>\\end{aligned}\\)<br></div>"
            ],
            "guid": "r&fvvv&M,6",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "03_model_selection",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the difference between <b>bias</b>,&nbsp;<b>variance</b> and <b>noise</b>?",
                "<div><div><div><div></div><div></div></div></div>\n</div>\n<div>\n<div><div><b>Bias:</b></div>\n<ul>\n<li>Due to restriction of your model class</li>\n<li>Also called “structure error”</li>\n</ul>\n<div><b>Variance</b>:</div>\n<ul>\n<li>Due to randomness of the data set</li>\n</ul><div><b>Noise</b>:</div><div><ul><li>Nothing we can do abourt</li></ul></div>\n</div></div>"
            ],
            "guid": "t~cQsS(uQO",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "03_model_selection",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain how the <b>Bias</b> is defined and what it is?",
                "<div><div>\\[\\mathbb{E}_{x,&nbsp;y}\\left[\\left(\\hat{f}_{*}(\\boldsymbol{x})-f(\\boldsymbol{x})\\right)^{2}\\right],\\]<br></div><div><br></div><div>where \\(\\hat{f}_{*}(\\boldsymbol{x})\\)&nbsp;is \\(\\mathbb{E}_{D_{n}}\\left[\\hat{f}_{D_{n}}(\\boldsymbol{x})\\right]\\)&nbsp;is&nbsp;the&nbsp;average&nbsp;estimate,&nbsp;averaged&nbsp;over&nbsp;all&nbsp;data&nbsp;sets&nbsp;of&nbsp;size&nbsp;\\(n\\)</div></div><div><ul><li>Difference of the true function to the \"best\" estimate</li><li>The best is, is the best you can do with your model class</li></ul></div>"
            ],
            "guid": "xg5]-khj;d",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "03_model_selection",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain how the&nbsp;<b>Variance</b>&nbsp;is defined and what it is?",
                "<div><div>\\[\\mathbb{E}_{D_{n}}\\left[\\mathbb{E}_{x,&nbsp;y}\\left[\\left(\\hat{f}_{D_{n}}(\\boldsymbol{x})-\\hat{f}_{*}(\\boldsymbol{x})\\right)^{2}\\right]\\right],\\]<br></div><div><br></div><div>where \\(\\hat{f}_{*}(\\boldsymbol{x})\\)&nbsp;is&nbsp;\\(\\mathbb{E}_{D_{n}}\\left[\\hat{f}_{D_{n}}(\\boldsymbol{x})\\right]\\)&nbsp;is&nbsp;the&nbsp;average&nbsp;estimate,&nbsp;averaged&nbsp;over&nbsp;all&nbsp;data&nbsp;sets&nbsp;of&nbsp;size&nbsp;\\(n\\)&nbsp;and \\(\\hat{f}_{D_{n}}\\)&nbsp;is&nbsp;the&nbsp;estimate&nbsp;of&nbsp;\\(f\\)&nbsp;we&nbsp;obtain&nbsp;using&nbsp;data&nbsp;\\(D_n\\).</div></div><div><ul><li>Difference of the estimates to the \"best\" estimates</li><li>Due to limited size of the data set</li><li>Depends on the the number of data points</li></ul></div>"
            ],
            "guid": "tFtT[B2b*[",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "03_model_selection",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the problem of optimizing the \"0-1-loss\"?",
                "Optimizing is a NP-hard problem, as one adds either 1 or nothing to the sum."
            ],
            "guid": "iz*M$Rcs6,",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Wie ist die <b>Sigmoid Funktion</b> definiert?",
                "\\(\\sigma(a)=\\frac{1}{1+\\exp (-a)}\\)"
            ],
            "guid": "KQ:Ju6)w$<",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How is the <b>chain / product rule</b> in <b>probability theory</b> defined?",
                "\\(\\begin{aligned}<br>p(x, y) &amp;=p(x \\mid y) p(y) \\\\<br>p\\left(x_{1}, \\cdots, x_{D}\\right) &amp;=p\\left(x_{1}\\right) p\\left(x_{2} \\mid x_{1}\\right) \\ldots p\\left(x_{D} \\mid x_{1}, \\ldots, x_{D}\\right)<br>\\end{aligned}\\)<div><br></div><div><img src=\"paste-78a885aa7419fa81a808101ecd3d522d8cc921a1.jpg\"><br></div>"
            ],
            "guid": "Qhc~Gi-bOs",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How is the <b>sum rule</b> (marginalization / integrating out) in <b>probablity theory </b>be defined?",
                "\\(\\begin{aligned}<br>p(x) &amp;=\\sum_{y} p(x, y) \\\\<br>p\\left(x_{1}\\right) &amp;=\\sum_{x_{2}} \\sum_{x_{3}} \\cdots \\sum_{x_{D}} p\\left(x_{1}, \\ldots, x_{D}\\right)<br>\\end{aligned}\\)<div><br></div><div><img src=\"paste-246b22e3e7cfacb271b5b82ab2637864d058bdb4.jpg\"><br></div>"
            ],
            "guid": "Q:.@=3)o&*",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Warum optimiert man den <b>Log-Likelihood</b> anstelle des <b>Likelihood</b>?",
                "<div><div><div><div></div><div></div></div></div>\n</div>\n<div>\n<div><div>\\(\\log \\operatorname{lik}(\\boldsymbol{\\theta} ; D)=\\sum_{i} \\log p_{\\boldsymbol{\\theta}}\\left(x_{i}, y_{i}\\right)\\)<br></div><div><br></div><div>Log-Likelihood is easier to optimize:</div>\n<ul>\n<li>log is monotonous</li>\n<li>sums are nicer to optimize than products</li>\n<li>logs cancel exponential forms</li>\n</ul>\n</div></div>"
            ],
            "guid": "n0I6Zc>&/E",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Caculate the Maximum Likelihood Estimation (MLE) given a training data \\(D=\\left\\{\\left(x_{i}, y_{i}\\right)\\right\\}_{i=1 \\ldots N}\\) iid. from the data distribution \\(p_{\\text {data }}\\).",
                "Let \\(p_{\\boldsymbol{\\theta}}(x, y)\\) be a family of distributions parametrized by \\(\\boldsymbol{\\theta} \\in \\Theta\\).<div><br></div><div>\\(\\operatorname{lik}(\\boldsymbol{\\theta} ; D)=\\prod_{i} p_{\\boldsymbol{\\theta}}\\left(x_{i}, y_{i}\\right)\\)<br></div>"
            ],
            "guid": "kzIY[e>xos",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Under which assumptions is the least squares objective from linear regression equivalent to a maximum likelihood objective?",
                "Gaussian likelihood with linear mean and constant noise."
            ],
            "guid": "l=lV]??y&;",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "00_klausurfragen"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Write down the objective of linear binary logistic regression. The samples are given by \\(x_{i}\\) and the labels by \\(c_{i} \\in\\{0,1\\}\\). How is \\(p\\left(c_{i} \\mid \\boldsymbol{x}_{i}\\right)\\) assumed to be distributed in binary logistic regression?",
                "\\[<br>\\operatorname{argmax}_{\\boldsymbol{w}} \\sum_{i=1}^{N} c_{i} \\log \\left(\\sigma\\left(\\boldsymbol{w}^{T} \\boldsymbol{x}_{i}\\right)\\right)+\\left(1-c_{i}\\right) \\log \\left(1-\\sigma\\left(\\boldsymbol{w}^{T} \\boldsymbol{x}_{i}\\right)\\right)<br>\\]<br>Logistic regression assumes \\(p\\left(c_{i} \\mid x_{i}\\right)\\) to be Bernoulli distributed"
            ],
            "guid": "cKFMTtf}rS",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "00_klausurfragen"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<div><div><div><div></div><div></div></div></div>\n</div>\n<div>\n<div><div>What\n are the hyperparameters for choosing the model complexity for each of \nthe following algorithms. Name at least one hyperparameter for every \nalgorithm.</div>\n<ul>\n<li>Neural Networks</li>\n<li>Support Vector Machines</li>\n<li>Gaussian Processes</li>\n<li>Decision Trees?</li>\n</ul>\n</div></div>",
                "<div>\n<div><ul>\n<li>Neural Networks: number of layers, number of neurons</li>\n<li>Support Vector Machines: which features to choose (also kernel bandwidth included), regularization</li>\n<li>Gaussian Processes: kernel bandwidth, prior</li>\n<li>Decision Trees: maximum depth, number of leaves</li>\n</ul>\n</div></div>"
            ],
            "guid": "zek>w:mq^O",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "00_klausurfragen"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Name at least two advantages and two disadvantages of decision trees.",
                "<div><div><div><div></div><div></div></div></div>\n</div>\n<div>\n<div><div><strong>Advantages:</strong></div>\n<ul>\n<li>Applicable to both regression and classification problems.</li>\n<li>Handle categorical predictors naturally.</li>\n<li>Computationally simple and quick to fit, even for large problems.</li>\n<li>No formal distributional assumptions (non-parametric).</li>\n<li>Can handle highly non-linear interactions and classification boundaries.</li>\n<li>Very easy to interpret if the tree is small. Disadvantages:</li>\n</ul>\n<div><strong>Disadvantage</strong></div>\n<ul>\n<li>Accuracy:\n current methods, such as support vector machines and ensemble \nclassifiers often have (30 %) lower error rates than CART.</li>\n<li>Instability:\n If we change the data a little, the tree picture can change a lot. So \nthe interpretation is not as straightforward as it appears.</li>\n</ul>\n</div></div>"
            ],
            "guid": "QNRI9Fgz>*",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "00_klausurfragen"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Which data-structure is usually used to efficiently implement k-Nearest Neighbors? Name the main steps in building that data-structure.",
                "<div>\n<div><div>KD-Trees</div><div><br></div>\n<div>Building the tree:</div>\n<ul>\n<li>Choose dimension (e.g., longest hyper-rectangle).</li>\n<li>Choose median as pivot</li>\n<li>Split node according to (pivot, dimension)</li>\n</ul>\n</div></div>"
            ],
            "guid": "rb?Cb-}`K3",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "00_klausurfragen"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "First, explain the intuition behind slack-variables in support vector machine training. Second, for a single data-point \\(\\left(x_{i}, c_{i}\\right)\\) the margin condition with slack variable \\(\\xi_{i}\\) is given as<br>\\[<br>c_{i}\\left(\\boldsymbol{w}^{T} \\boldsymbol{x}_{i}+b\\right) \\geq 1-\\xi_{i}<br>\\]<div><br></div><div>Assuming \\(0 \\leq \\xi_{i} \\leq 1\\), is \\(x_{i}\\) classified correctly? Assuming \\(\\xi_{i}&gt;1\\), is \\(x_{i}\\) classified correctly?<br></div>",
                "\n<div><div><div>\n<div><ul>\n<li>Allow violation of margin condition</li>\n<li>Act as regularization, avoid over-fitting</li>\n<li>Make constraint optimization problem solvable, even if the data is not linearly separable</li>\n</ul>\n<div>Other Questions</div>\n<ul>\n<li>Yes, (but margin is violated)</li>\n<li>No</li>\n</ul>\n</div></div></div><ul>\n</ul>\n</div>"
            ],
            "guid": "fY.b)h,?Tc",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "00_klausurfragen"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "You are given the following optimization problem:<br>\\[<br>\\begin{aligned}<br>\\underset{a}{\\operatorname{argmax}} &amp; a^{2} h \\\\<br>\\text { s.t. } &amp; S_{\\max } \\geq 2 a^{2}+4 a h<br>\\end{aligned}<br>\\]<br>Write down the Lagrangian. Derive the optimal value for \\(a\\) depending on your lagrangian multiplier.",
                "\\[<br>\\begin{aligned}<br>L &amp;=a^{2} h+\\lambda\\left(S_{\\max }-2 a^{2}-4 a h\\right) \\\\<br>\\frac{d L}{d a} &amp;=2 a h-4 \\lambda a-4 \\lambda h \\\\<br>a^{*} &amp;=\\frac{4 \\lambda h}{2 h-4 \\lambda}<br>\\end{aligned}<br>\\]<br>(The sign in the Lagrangian may change. But the following equations should be correct according to the Lagrangian.)"
            ],
            "guid": "w#r?$OQB;u",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "00_klausurfragen"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the key idea behind second order optimization methods? What are their benefits? Why are second order optimization methods usually not applicable for Deep Neural Networks?",
                "<div>\n<div><ul>\n<li>To use the second derivative (Hessian) of the objective and directly step towards the minimum of the quadratic approximation.</li>\n<li>No learning rate needs to be tuned and they need fewer function evaluations.</li>\n<li>Because the Hessian is huge and needs to be inverted.</li>\n</ul>\n</div></div>"
            ],
            "guid": "J@MUWu_Q6?",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "00_klausurfragen"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Why is it not feasible to use fully connected layer for images? How do convolutional neural networks solve this problem and which property of an image do they exploit.",
                "<div>\n<div><ul>\n<li>They have to many parameters</li>\n<li>less parameters by parameter sharing</li>\n<li>They exploit spatial structure</li>\n</ul>\n</div></div>"
            ],
            "guid": "B*#^9#p5A5",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "00_klausurfragen"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Gaussian Processes(GP) are also referred to as a \"Bayesian Kernel Regression\" approach. Why?",
                "You can derive Gaussian Processes by using the Kernel Trick from Bayesian Linear Regression, as in Kernel Regression."
            ],
            "guid": "pQOb,c4,<|",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "00_klausurfragen"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give an <b>intuition</b> how <b>Standard Gradient Descent</b> and <b>Stochastic Gradient Descent</b> are different?",
                "<div><div><div><div></div><div></div></div></div>\n</div>\n<div>\n<div><ul>\n<li><b>Standard Gradient Descent</b> uses the full dataset to compute gradients and update its parameters, one pass at a time.</li>\n<li><b>Stochastic Gradient Descent</b> uses one observation at a time.</li>\n</ul><div><img src=\"3017990105-68a50cb98b8cc286_fix732.png\"><br></div>\n</div></div>"
            ],
            "guid": "Byg$xM.5Up",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give the definition of <b>Standard / Batch Gradient Descent</b>.",
                "\\(\\frac{1}{n} \\sum_{i} l\\left(\\boldsymbol{x}_{i} ; \\boldsymbol{\\theta}\\right)\\)<br><div><br></div><div>\\(\\boldsymbol{\\theta}_{t+1}=\\boldsymbol{\\theta}_{t}-\\frac{\\eta}{n} \\sum_{i} \\nabla_{\\boldsymbol{\\theta}} l\\left(\\boldsymbol{x}_{i} ; \\boldsymbol{\\theta}_{t}\\right)\\)<br></div><div><br></div><div>One uses the <b>entire data</b> set to compute gradients and update its parameters!</div>"
            ],
            "guid": "NgC~/bLiUX",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give the definition of <b>Stochastic&nbsp;Gradient Descent</b>.",
                "\\(l\\left(\\boldsymbol{x}_{i} ; \\boldsymbol{\\theta}\\right)\\)<br><div>\\(\\boldsymbol{\\theta}_{t+1}=\\boldsymbol{\\theta}_{t}-\\eta \\nabla_{\\boldsymbol{\\theta}} l\\left(\\boldsymbol{x}_{i} ; \\boldsymbol{\\theta}_{t}\\right)\\)<br></div><div><br></div><div>Stochastic gradient descent uses a cheaper approximation by just using the \\(i\\)-th sample at a time, instead of the entire data set.</div>"
            ],
            "guid": "s|7Q_N_^z)",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Given the training data \\(D=\\left\\{\\left(x_{i}, y_{i}\\right)\\right\\}_{i=1 \\ldots N}\\) iid. from the data distribution \\(p_{\\text {data }}\\) .<div><br></div><div>Let \\(p_{\\theta}(y \\mid x)\\) be a family of distributions parametrized by \\(\\boldsymbol{\\theta} \\in \\Theta\\).</div><div><br></div><div>How could the <b>conditional log-likelihood</b> for this distribution be calculated?</div>",
                "\\(\\operatorname{loglik}(\\boldsymbol{\\theta} ; D)=\\sum_{i} \\log p_{\\boldsymbol{\\theta}}\\left(y_{i} \\mid x_{i}\\right)\\)"
            ],
            "guid": "O_(+++Q4i9",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification",
                "formula_sheet"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is referred to as the <b>exponential trick</b> in terms of <b>conditional probability distributions</b>?<div><br></div><div>Give an example.</div>",
                "Say we have a Bernoulli distribution. We have to possible outcomes of the event c, that depends on x.<div><br></div><div>\\(p(c=1 \\mid \\boldsymbol{x})=\\sigma\\left(\\boldsymbol{w}^{T} \\boldsymbol{x}+b\\right), \\quad p(c=0 \\mid \\boldsymbol{x})=1-\\sigma\\left(\\boldsymbol{w}^{T} \\boldsymbol{x}+b\\right)\\)<br></div><div><br></div><div>We could then calculate the correct probabilty using the exponential trick depending on the value of \\(c\\):</div><div>\\(p(c \\mid \\boldsymbol{x})=p(c=1 \\mid \\boldsymbol{x})^{c} p(c=0 \\mid \\boldsymbol{x})^{1-c}=\\sigma\\left(\\boldsymbol{w}^{T} \\boldsymbol{x}+b\\right)^{c}\\left(1-\\sigma\\left(\\boldsymbol{w}^{T} \\boldsymbol{x}+b\\right)\\right)^{1-c}\\)<br></div><div><br></div>"
            ],
            "guid": "hw}H[;Yzq2",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What are we optimizing for with <b>generalized logistic models</b>?",
                "\\(\\operatorname{argmax}_{\\boldsymbol{w}} \\log \\operatorname{lik}(\\boldsymbol{w}, D)=\\operatorname{argmax}_{\\boldsymbol{w}} \\sum_{i} c_{i} \\log \\sigma\\left(\\boldsymbol{w}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)\\right)+\\left(1-c_{i}\\right) \\log \\left(1-\\sigma\\left(\\boldsymbol{w}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)\\right)\\right)\\)"
            ],
            "guid": "L-xmCH[!Bh",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What do we optimize for the in the <b>logistic regression</b> case?",
                "<div>We optimize the cross-entropy loss given by:</div><div><br></div>\\(\\operatorname{argmax}_{\\tilde{\\boldsymbol{w}}} \\log \\operatorname{lik}(\\tilde{\\boldsymbol{w}}, D)=\\operatorname{argmax}_{\\tilde{\\boldsymbol{w}}} \\sum_{i} c_{i} \\log \\sigma\\left(\\tilde{\\boldsymbol{w}}^{T} \\tilde{\\boldsymbol{x}}_{i}\\right)+\\left(1-c_{i}\\right) \\log \\left(1-\\sigma\\left(\\tilde{\\boldsymbol{w}}^{T} \\tilde{\\boldsymbol{x}}_{i}\\right)\\right)\\)<br><div><br></div><div>Note that \\(c_{i}\\) hold the class labels. We try to find the best estitmates for \\(\\tilde{\\boldsymbol{w}}\\).&nbsp;</div>"
            ],
            "guid": "tWU5:YUWuO",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification",
                "checklater",
                "formula_sheet"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How can one derive the <b>cross-entropy loss</b> for <b>logistic regression</b> using the <b>conditional Bernoulli log-likelihood</b>?<div><br></div><div>Conditional Bernoulli distribution is given by:</div><div>\\(p(c \\mid \\boldsymbol{x})=p(c=1 \\mid \\boldsymbol{x})^{c} p(c=0 \\mid \\boldsymbol{x})^{1-c}=\\)<br></div><div><br></div><div>Logistic function is given by:</div><div>\\(\\sigma(x)=\\frac{1}{1+\\exp (-x)}\\)<br></div>",
                "\\(\\begin{aligned} \\log \\operatorname{lik}(\\tilde{\\boldsymbol{w}}, D) &amp;=\\sum_{i} \\log p\\left(c_{i} \\mid \\boldsymbol{x}_{i}\\right)=\\sum_{i} \\log \\left(p\\left(c=1 \\mid \\boldsymbol{x}_{i}\\right)^{c_{i}} p\\left(c=0 \\mid \\boldsymbol{x}_{i}\\right)^{1-c_{i}}\\right) \\\\ &amp;=\\sum_{i} c_{i} \\log p\\left(c=1 \\mid \\boldsymbol{x}_{i}\\right)+\\left(1-c_{i}\\right) \\log p\\left(c=0 \\mid \\boldsymbol{x}_{i}\\right) \\\\ &amp;=\\sum c_{i} \\log \\sigma\\left(\\tilde{\\boldsymbol{w}}^{T} \\tilde{\\boldsymbol{x}}_{i}\\right)+\\left(1-c_{i}\\right) \\log \\left(1-\\sigma\\left(\\tilde{\\boldsymbol{w}}^{T} \\tilde{\\boldsymbol{x}}_{i}\\right)\\right) \\end{aligned}\\)"
            ],
            "guid": "bwDL},guLF",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "You are given the <b>data log-likelihood</b>:<div>\\(\\log \\operatorname{lik}(\\mathcal{D}, \\boldsymbol{w})=\\sum_{i=1}^{N} p\\left(c_{i} \\mid \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right), \\boldsymbol{w}\\right)=\\sum_{i=1}^{N} \\underbrace{c_{i} \\log \\sigma\\left(\\boldsymbol{w}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)\\right)+\\left(1-c_{i}\\right) \\log \\left(1-\\sigma\\left(\\boldsymbol{w}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)\\right)\\right)}_{\\operatorname{loss}_{i} \\ldots \\text { loss of the ith sample }}\\)<br></div><div><br></div><div>Calculate the loss with respect to \\(\\boldsymbol{w}\\)&nbsp;\\(\\frac{\\partial \\operatorname{loss}_{i}}{\\partial \\boldsymbol{w}}\\).</div>",
                "\\(\\begin{aligned} \\frac{\\partial \\operatorname{loss}_{i}}{\\partial \\boldsymbol{w}}=&amp; \\frac{\\partial}{\\partial \\boldsymbol{w}}\\left(c_{i} \\log \\sigma\\left(\\boldsymbol{w}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)\\right)+\\left(1-c_{i}\\right) \\log \\left(1-\\sigma\\left(\\boldsymbol{w}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)\\right)\\right)\\right.\\\\=&amp; c_{i} \\frac{1}{\\sigma\\left(\\boldsymbol{w}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)\\right)} \\sigma\\left(\\boldsymbol{w}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)\\right)\\left(1-\\sigma\\left(\\boldsymbol{w}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)\\right)\\right) \\phi\\left(\\boldsymbol{x}_{i}\\right) \\\\ &amp;+\\left(1-c_{i}\\right) \\frac{1}{1-\\sigma\\left(\\boldsymbol{w}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)\\right)}(-) \\sigma\\left(\\boldsymbol{w}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)\\right)\\left(1-\\sigma\\left(\\boldsymbol{w}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)\\right)\\right) \\phi\\left(\\boldsymbol{x}_{i}\\right) \\\\=&amp; c_{i}\\left(1-\\sigma\\left(\\boldsymbol{w}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)\\right)\\right) \\phi\\left(\\boldsymbol{x}_{i}\\right)-\\left(1-c_{i}\\right) \\sigma\\left(\\boldsymbol{w}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)\\right) \\phi\\left(\\boldsymbol{x}_{i}\\right) \\\\=&amp;\\left(c_{i}-\\sigma\\left(\\boldsymbol{w}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)\\right)\\right) \\phi\\left(\\boldsymbol{x}_{i}\\right) \\end{aligned}\\)"
            ],
            "guid": "Q`9y8KyYP,",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification",
                "checklater",
                "formula_sheet"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Calculate the <b>gradient</b> for the <b>multiclass classification case</b>.<div><br></div><div>Gradient is given by:</div><div><br></div><div>\\(\\frac{\\partial \\operatorname{loss}_{i}}{\\partial \\boldsymbol{w}_{k}}=\\frac{\\partial}{\\partial \\boldsymbol{w}_{k}}\\left(\\sum_{k=1}^{K} \\boldsymbol{h}_{c_{i}, k} \\boldsymbol{w}_{k}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)-\\log \\left(\\sum_{j=1}^{K} \\exp \\left(\\boldsymbol{w}_{j}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)\\right)\\right)\\right)\\)<br></div>",
                "\\(\\begin{aligned} \\frac{\\partial \\operatorname{loss}_{i}}{\\partial \\boldsymbol{w}_{k}} &amp;=\\frac{\\partial}{\\partial \\boldsymbol{w}_{k}}\\left(\\sum_{k=1}^{K} \\boldsymbol{h}_{c_{i}, k} \\boldsymbol{w}_{k}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)-\\log \\left(\\sum_{j=1}^{K} \\exp \\left(\\boldsymbol{w}_{j}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)\\right)\\right)\\right) \\\\ &amp;=\\boldsymbol{h}_{c_{i}, k} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)-\\frac{\\partial}{\\partial \\boldsymbol{w}_{k}} \\log \\left(\\sum_{j=1}^{K} \\exp \\left(\\boldsymbol{w}_{j}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)\\right)\\right) \\\\ &amp;=\\boldsymbol{h}_{c_{i}, k} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)-\\frac{1}{\\sum_{j=1}^{K} \\exp \\left(\\boldsymbol{w}_{j}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)\\right)} \\exp \\left(\\boldsymbol{w}_{k}^{T} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)\\right) \\phi\\left(\\boldsymbol{x}_{i}\\right) \\\\ &amp;=\\underbrace{\\phi\\left(\\boldsymbol{x}_{i}\\right)}_{\\text {feature vector }} \\underbrace{\\left(\\boldsymbol{h}_{c_{i}, k}-p\\left(k \\mid \\boldsymbol{x}_{k}\\right)\\right)}_{\\text {\"soft-max error\" }} \\end{aligned}\\)"
            ],
            "guid": "MN`{r.V3c3",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification",
                "checklater",
                "formula_sheet"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Derive the <b>MLE solution</b> for the univariate&nbsp;<b>Gaussian density function</b>.<div><br></div><div>\\(\\log \\operatorname{lik}(\\boldsymbol{\\theta} ; D)=-N \\log \\sqrt{2 \\pi \\sigma^{2}}-\\sum_{i} \\frac{\\left(x_{i}-\\mu\\right)^{2}}{2 \\sigma^{2}}\\)<br></div>",
                "<div>Estimation of \\(\\mu\\)&nbsp;</div><div><br></div><div>\\(\\begin{aligned}<br>\\frac{\\partial \\log \\operatorname{lik}(\\mu ; D)}{\\partial \\mu}&amp;=-\\sum_{i} \\frac{2\\left(x_{i}-\\mu\\right)(-1)}{2 \\sigma^{2}}=0 \\\\<br>&amp;\\sum_{i} x_{i}-N \\mu=0 \\\\<br>&amp;\\mu=\\frac{1}{N} \\sum_{i} x_{i}<br>\\end{aligned}\\)<br></div><div><br></div><div>Estimation of \\(\\sigma\\)&nbsp;<br></div><div><br></div><div>\\(\\begin{aligned}<br>\\frac{\\partial \\log \\operatorname{lik}(\\mu ; D)}{\\partial \\sigma}&amp;=-N \\log(\\sqrt{2\\pi}) -N\\log(\\sigma)-\\sum_{i} \\frac{\\left(x_{i}-\\mu\\right)^{2}}{2 \\sigma^{2}}\\\\<br>&amp;=-N\\frac{1}{\\sigma}+\\sum_{i} \\frac{\\left(x_{i}-\\mu\\right)^{2}}{\\sigma^{3}}\\\\<br>&amp;N\\sigma^2=\\sum_{i} \\left(x_{i}-\\mu\\right)^{2}\\\\<br>&amp;\\sigma=\\sqrt{\\frac{1}{N}\\sum_{i} \\left(x_{i}-\\mu\\right)^{2}}<br>\\end{aligned}\\)<br></div>"
            ],
            "guid": "zIE$A,Y}(6",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "02_linear_classification",
                "checklater",
                "formula_sheet"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How should the dimension \\(D\\) be chosen for the <b>principal component analysis</b>?",
                "\n<div><ul>\n<li>Choose \\(D\\) based on <b>application performance</b>, i.e. choose the smallest \\(D\\) that makes the application work well enough</li>\n<li>Choose \n\\(D\\) so that the Eigenbasis captures <b>some fraction of the variance</b> (for \nexample \\(\\eta= 0.9\\). The eigenvalue \\(\\lambda_{i}\\) describes the \nmarginal variance captured by \\(\\boldsymbol{u}_i\\)</li></ul><div><br></div><div>\\(\\begin{aligned}<br>&amp;\\text { Choose } D \\text { s.t. } \\sum_{i=1}^{M} \\lambda_{i}=\\eta \\underbrace{\\sum_{i=1}^{D} \\lambda_{i}}_{\\text { Total variance of the data }}\\\\<br>\\end{aligned}\\)<br></div>\n</div>"
            ],
            "guid": "gw_okVE^;J",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "05_dim_reduction",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give the MLE solution for the <b>multivariate Gaussian density function</b>.",
                "ML estimation for a Gaussian<div>\\(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}=\\operatorname{argmax}_{\\boldsymbol{\\theta}} \\log \\operatorname{lik}(\\boldsymbol{\\theta} ; D)=\\sum_{i=1}^{N} \\log \\mathcal{N}\\left(\\boldsymbol{x}_{i} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}\\right)\\)<br></div><div><br></div><div>Take the partial derivatives and set it to 0</div><div>\\(\\frac{\\partial \\log \\operatorname{like}(\\boldsymbol{\\theta} ; \\mathcal{D})}{\\partial \\boldsymbol{\\mu}}=\\mathbf{0}\\)<br></div><div>\\(\\frac{\\partial \\log \\operatorname{like}(\\boldsymbol{\\theta} ; \\mathcal{D})}{\\partial \\boldsymbol{\\Sigma}}=\\mathbf{0}\\)<br></div><div><br></div><div>Which leads to the closed form solution<br></div><div>\\(\\mu=\\frac{1}{N} \\sum_{i=1}^{N} \\boldsymbol{x}_{i}\\)<br></div><div>\\(\\boldsymbol{\\Sigma}=\\frac{1}{N} \\sum_{i=1}^{N}\\left(\\boldsymbol{x}_{i}-\\boldsymbol{\\mu}\\right)\\left(\\boldsymbol{x}_{i}-\\boldsymbol{\\mu}\\right)^{T}\\)<br></div><div><br></div>"
            ],
            "guid": "mVp%yv+iuZ",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How can \\(k\\)<b>-means</b> be calculated using <b>EM</b>?",
                "<div>\n<div><div>Co-variances are always set to 0 (in the limit)<br>\n<strong>E-Step</strong></div>\n<ul>\n<li>repsonsibilites \\(q_{ik}\\) of nearest cluster \\(k\\) are set to 1, all other values are 0.</li>\n</ul>\n<div><strong>M-Step</strong></div>\n<ul>\n<li>Update the mean is the same.</li>\n<li>Co-Variances are ignored (set close to 0)</li>\n</ul>\n</div></div>"
            ],
            "guid": "BNf2vOJyam",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How should the <b>number of components</b> be chosen in a <b>mixture model</b>?",
                "Choosing the number of mixture components is a model-selection problem requiring <b>cross-valdiation</b> on a <b>validation set</b>."
            ],
            "guid": "I3`i&]q!cZ",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the drawback of the <b>EM algorithm</b>? How can it be fixed?",
                "EM assumes that E-step can set the KL divergence to 0.&nbsp;<div>However, this is only possible if \\(z\\) is discrete or if we have linear Gaussian models.</div><div><br></div><div><b>Variational Bayes</b> approximate the posterior where a KL will be \\(&gt; 0\\) after E-Step.</div>"
            ],
            "guid": "rrWHGK5[$V",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What distinguishes <b>kernel density estimation</b> from \\(k\\)-<b>nearest neighbour</b> in terms of their approach to define a region.",
                "\n<div><div><strong>Kernel density estimation</strong></div>\n<ul>\n<li>Fix \\(V\\) and determine \\(K\\)</li>\n<li>Example: determine the number of data points \\(K\\) in a fixed hypercube</li>\n</ul>\n<div><strong>K-nearest neighbor</strong></div>\n<ul>\n<li>Fix \\(K\\) and determine \\(V\\)</li><li>Example: increase the size of a sphere until \\(K\\) data points fall into the sphere</li>\n</ul>\n</div>"
            ],
            "guid": "grg5L4Nc:%",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Why is a <b>activation function</b> used in a <b>convolutional network</b>?",
                "After every convolutional operation (i. e., after every convolutional layer) a pixel-by-pixel operation is used to replace all negative values by zeros.<div><br></div><div><img src=\"paste-19ffca0ed79c17d915f34cb0fc243b76ff585c9a.jpg\"><br></div>"
            ],
            "guid": "P&{YxXQQ>X",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "11_rnns",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What are advantages of a \\(k\\) nearest neighbour classifiers?",
                "<div>\n<div><ul>\n<li>Training is very fast</li>\n<li>Learn complex target functions</li>\n<li>Similar algorithm can be used for regression</li>\n</ul>\n</div></div>"
            ],
            "guid": "i7jjzwYQMg",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Does Bagging affect <b>variance </b>or <b>bias</b>?",
                "Bagging reduces the <b>variance </b>of the base learner but has limited effect on the <b>bias.</b>"
            ],
            "guid": "b6Pj%l@TAH",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "04_trees_rf",
                "checklater"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the motivation for <b>Bayesian Learning</b>?",
                "<ul><li>Instead of using a point estimate for the parameter vector \\(\\theta^{*}\\), we also consider uncertainty for the parameter vector.</li><li>Tries to find a more robust predictor by averaging over (infinitely) many predictors.</li><li>Uses estimate to quantify the uncertainty of the prediction.</li></ul>"
            ],
            "guid": "CG*Ay,iUAW",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "09_bayesian_learning"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How do <b>multiple models</b>&nbsp;e. g. ensembles / forests and <b>bayesian learning</b> relate?",
                "Both consider the <b>uncertainty </b>in the <b>parameter estimate</b> \\(\\theta^{*}\\).<div><br></div><div>Bayesian models could be considered as an ensemble method (with potentially infinite amount of models)</div>"
            ],
            "guid": "s(.~hSN9_Z",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "09_bayesian_learning"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the meaning of the prior, posterior, likelihood and evidence in bayesian learning?<div><br></div><div>\\(\\underbrace{p(\\boldsymbol{\\theta} \\mid \\mathcal{D})}_{\\text {posterior }}=\\frac{\\overbrace{p(\\mathcal{D} \\mid \\boldsymbol{\\theta})}^{\\text {data likelihood}} \\overbrace{p(\\boldsymbol{\\theta})}^{\\text {prior}}}{\\underbrace{p(\\mathcal{D})}_{\\text {evidence }}}\\)<br></div>",
                "<ul><li><strong>Prior:</strong>&nbsp;Our subjective belief</li><li><strong>Posterior:</strong>&nbsp;Probability of parameter vector given the data</li><li><strong>Likelihood:</strong>&nbsp;Specified by our parametric model \\(\\mathcal{D}\\)</li><li><strong>Evidence:</strong>&nbsp;Normalization, can be used for model comparsion</li></ul>"
            ],
            "guid": "D(5;,c-7et",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "09_bayesian_learning"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the intution of computing the predictive destribution (step 2) in bayesian learning?",
                "By assigning every parameter estimator the \"probability of being right\", the average parameter estimators will be better than a single one.<div><br></div><div>We could use samples of&nbsp;\\(p(\\boldsymbol{\\theta} \\mid \\mathcal{D})\\) to approximate the integral.</div>"
            ],
            "guid": "N]M8O0*CZ^",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "09_bayesian_learning"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the 2 steps in Bayesian learning?",
                "<b>Compute Posterior</b> (Probabilty of being right for \\(\\theta\\))<br><div><br></div><div>\\(p(\\boldsymbol{\\theta} \\mid \\mathcal{D})=\\frac{p(\\mathcal{D} \\mid \\boldsymbol{\\theta}) \\quad p(\\boldsymbol{\\theta})}{p(\\mathcal{D})}\\)<br></div><div><br></div><div><b>Predicting of a new data-point</b> \\(x^{*}\\)</div><div><br></div><div>\\(\\underbrace{p\\left(\\boldsymbol{x}^{*} \\mid \\mathcal{D}\\right)}_{\\text {marginal likelihood }}=\\int \\underbrace{p\\left(\\boldsymbol{x}^{*} \\mid \\boldsymbol{\\theta}\\right)}_{\\text {likelihood }} \\underbrace{p(\\boldsymbol{\\theta} \\mid \\mathcal{D})}_{\\text {posterior }} d \\boldsymbol{\\theta}\\)<br></div><div><br></div><div><b>Note:</b>&nbsp;Likelihood \\(p\\left(\\boldsymbol{x}^{*} \\mid \\mathcal{D}\\right)\\) is now purely determined by the data \\(\\mathcal{D}\\).</div>"
            ],
            "guid": "B^<x%|,nQ=",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "09_bayesian_learning"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the purpose of the <b>Prior</b> \\(p(\\theta)\\) in <b>Bayesian Learning</b>?<div><br></div><div>Also, give an example.</div>",
                "The prior \\(p(\\theta)\\) captures the belief and domain knowledge.<div><br></div><div>Example e. g. Gaussian prior:</div><div>\\(p(\\boldsymbol{\\theta})=\\mathcal{N}\\left(\\boldsymbol{\\theta} \\mid \\mathbf{0}, \\lambda^{-1} \\boldsymbol{I}\\right),\\)<br></div><div><br></div><div>where \\(\\lambda\\) is the precision of the prior.</div>"
            ],
            "guid": "N8Ej?)*(DT",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "09_bayesian_learning"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the impact of a larger number of observations when calculating the <b>posterior</b>?",
                "<ul><li><div>Variance decreases with more training samples</div></li><li><div>Posterior mean interpolates between prior mean and sample averages, as it is a sum of the maximum likelihood (average of samples) and prior&nbsp;</div></li></ul><div><img src=\"paste-8405ec9aba8b229a8d397ccc0a6285ad26ccd855.jpg\"><br></div>"
            ],
            "guid": "sdPrLtZWLP",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "09_bayesian_learning"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How can we estimate the <b>predictive distribution</b> of a <b>Gaussian </b>using <b>Bayesian Learning</b>?",
                "The predictive distribution is given by:<div><br></div><div>\\(\\begin{aligned} \\underbrace{p\\left(x^{*} \\mid \\boldsymbol{X}\\right)}_{\\text {marginal likelihood }} &amp;=\\int \\underbrace{p\\left(x^{*} \\mid \\mu\\right)}_{\\text {likelihood }} \\underbrace{p(\\mu \\mid \\boldsymbol{X})}_{\\text {posterior }} d \\mu \\\\ &amp;=\\int \\mathcal{N}\\left(x^{*} \\mid \\mu, \\sigma\\right) \\mathcal{N}\\left(\\mu \\mid \\mu_{N}, \\sigma_{N}\\right) d \\mu \\\\ &amp;=\\mathcal{N}\\left(x^{*} \\mid \\mu_{x^{*}}, \\sigma_{x^{*}}^{2}\\right) \\end{aligned}\\)<br></div><div><br></div><div>The predictive distribution is Gaussian with:</div><div>Mean:&nbsp;\\(\\mu_{x^{*}}=\\mu_{N}\\)</div><div>Variance:&nbsp;\\(\\sigma_{x^{*}}^{2}=\\sigma_{N}^{2}+\\sigma^{2}\\)</div><div><br></div><div><b>Observations</b>:</div><div><i>predictive mean</i> is the same as the posterior mean.</div><div><i>predictive variance</i> also considers uncertainty from mean.</div>"
            ],
            "guid": "n$l_/^$#(H",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "09_bayesian_learning"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is a <b>conjugate prior</b>?",
                "If the <b>posterior distribution</b>&nbsp;\\(p(\\boldsymbol{\\theta} \\mid \\mathcal{D})\\) ist the same <b>probability distribution family</b> as the prior probabilty distribution \\(p(\\theta)\\) the prior and posterior are then called <b>conjugate distributions</b>."
            ],
            "guid": "L_]F8+#TFW",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "09_bayesian_learning"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What problem does the maximum A-posteriori (MAP) solution solve?",
                "Integrating out the posterior can often not be done analytically. MAP provides a simplification."
            ],
            "guid": "Ki/CX51Q7=",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "09_bayesian_learning"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Name two <b>approaches </b>of <b>Bayesian Learning</b> discussed in class.",
                "<ul><li><div>Bayesian Linear Regression</div></li><li><div>Gaussian Processes</div></li></ul>"
            ],
            "guid": "1^M#&07i,",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "09_bayesian_learning"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the advantage of <b>Bayesian Learning</b> for smaller data sets?",
                "<ul><li><div>We know our model is uncertain</div></li><li><div>More robust estimates due to averaging</div></li></ul>"
            ],
            "guid": "Idl4Pt4<,N",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "09_bayesian_learning"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How do <b>Bayesian Learning</b> and <b>Maximum Likelihood</b> relate?",
                "For large datasets, the posterior will be a point estimate, as&nbsp;\\(\\lim _{n \\rightarrow \\infty} p\\left(\\boldsymbol{\\theta} \\mid \\mathcal{D}_{n}\\right)=\\delta\\left(\\boldsymbol{\\theta}-\\boldsymbol{\\theta}^{*}\\right)\\)<div><br></div><div><b>Maximum Likelihood</b> and <b>Bayesian Learning</b> will be the same.</div>"
            ],
            "guid": "Q/+ZOn#0{V",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "09_bayesian_learning"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the simplifications of <b>Maximum a-posteriori solution</b> over Bayesian Learning?",
                "Find parameter vector \\boldsymbol{\\theta}_{\\text{MAP}} that maximzes the posterior:<div>\\(\\boldsymbol{\\theta}_{MAP}=\\underset{\\boldsymbol{\\theta}}{\\arg \\max } p(\\boldsymbol{\\theta} \\mid \\mathcal{D})=\\underset{\\boldsymbol{\\theta}}{\\arg \\max } p(\\mathcal{D} \\mid \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta})\\)<br></div><div><br></div><div>Uncertainty in \\(\\boldsymbol{\\theta}\\) is ignored.&nbsp;</div><div>Optimization is done in log-domain.</div><div>\\(\\log p(\\boldsymbol{\\theta})\\) is the difference to the maximum likelihood solution.<br></div><div><br></div><div>&nbsp;\\(\\boldsymbol{\\theta}_{MAP}=\\underset{\\theta}{\\arg \\max } \\log p(\\mathcal{D} \\mid \\boldsymbol{\\theta})+\\log p(\\boldsymbol{\\theta})\\)</div><div><br></div><div>Use&nbsp;\\(\\boldsymbol{\\theta}_{MAP}\\) for prediction</div><div>\\(p\\left(\\boldsymbol{x}^{*} \\mid \\mathcal{D}\\right) \\approx p\\left(\\boldsymbol{x}^{*} \\mid \\boldsymbol{\\theta}_{MAP}\\right)\\)<br></div><div><br></div>"
            ],
            "guid": "v=,nJso;@",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "09_bayesian_learning"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How do the <b>MAP solution</b> and the <b>regularization loss</b> relate?",
                "If Prior is Gaussian, the the Prior has a similar role to the regularization loss \\(\\ell_2\\)."
            ],
            "guid": "kZ*>m(;HE_",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "09_bayesian_learning"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Derive the <b>MAP solution</b> for <b>Linear Regression</b>.",
                "Map solution is given by:<div>\\(\\boldsymbol{\\theta}_{\\mathrm{MAP}}=\\underset{\\theta}{\\arg \\max }(\\log p(\\mathcal{D} \\mid \\boldsymbol{\\theta})+\\log p(\\boldsymbol{\\theta}))\\)<br><div><br></div></div><div>Gaussian Likelihood:</div><div>\\(p(\\mathcal{D} \\mid \\boldsymbol{\\theta})=p(\\boldsymbol{Y} \\mid \\boldsymbol{X}, \\boldsymbol{\\theta})=\\prod_{i} \\mathcal{N}\\left(y_{i} \\mid f_{\\boldsymbol{\\theta}}\\left(\\boldsymbol{x}_{i}\\right), \\sigma^{2}\\right)\\)<br></div><div><br></div><div>Gaussian Prior:</div><div>\\(p(\\boldsymbol{\\theta})=\\mathcal{N}\\left(\\boldsymbol{\\theta} \\mid \\mathbf{0}, \\lambda^{-1} \\boldsymbol{I}\\right)\\)<br></div><div><br></div><div>Objective:</div><div>\\(\\underset{\\boldsymbol{\\theta}}{\\arg \\max } \\underbrace{\\sum_{i}-\\frac{1}{2 \\sigma^{2}}\\left(y_{i}-f_{\\boldsymbol{\\theta}}\\left(\\boldsymbol{x}_{i}\\right)\\right)^{2}}_{\\text {Sum of squared errors }}-\\underbrace{\\frac{\\lambda}{2} \\boldsymbol{\\theta}^{T} \\boldsymbol{\\theta}}_{-\\lambda / 2\\|\\theta\\|^{2}}+\\underbrace{c\\left(\\sigma^{2}, \\lambda\\right)}_{\\text {only interested in } \\boldsymbol{\\theta}}\\)<br></div>"
            ],
            "guid": "H)pn3PyEDF",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "09_bayesian_learning"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How do ridge regression and the MAP solution for linear regression relate?",
                "<div>\\(\\begin{aligned} \\boldsymbol{w}_{\\mathrm{MAP}} &amp;=\\underset{\\boldsymbol{w}}{\\arg \\max } \\underbrace{\\sum_{i}-\\frac{1}{2 \\sigma^{2}}\\left(y_{i}-\\boldsymbol{w}^{T} \\phi\\left(\\boldsymbol{x}_{i}\\right)\\right)^{2}}_{\\text {Sum of squared errors }}-\\underbrace{\\frac{\\lambda}{2} \\boldsymbol{w}^{T} w}_{-\\lambda / 2\\|w\\|^{2}}+\\underbrace{c\\left(\\sigma^{2}, \\lambda\\right)}_{\\text {only interested in } \\boldsymbol{w}} \\\\ &amp;=\\underset{w}{\\arg \\min } \\sum_{i}\\left(y_{i}-\\boldsymbol{w}^{T} \\phi\\left(\\boldsymbol{x}_{i}\\right)\\right)^{2}+\\lambda \\sigma^{2} \\boldsymbol{w}^{T} \\boldsymbol{w}-c\\left(\\sigma^{2}, \\lambda\\right) \\end{aligned}\\)</div><div><br></div><div>That is, if&nbsp;\\(\\lambda_{\\text {ridge }}=\\lambda \\sigma^{2}\\) the ridge solution and the MAP solution is equivalent.</div>"
            ],
            "guid": "G)b<%:Ecgi",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "09_bayesian_learning"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "In which way does the MAP solution improve on the standard Linear Regression?",
                "\\(\\boldsymbol{w}_{\\mathrm{MAP}}=\\left(\\boldsymbol{\\Phi}^{T} \\boldsymbol{\\Phi}+\\lambda \\sigma^{2} \\boldsymbol{I}\\right)^{-1} \\boldsymbol{\\Phi}^{T} \\boldsymbol{y}\\)<div><br></div><div>The MAP solution includes two additional parameters. \\(\\lambda\\) to set the importance of the prior and \\(\\sigma^2\\) to set the uncertainty in the training data.&nbsp;</div>"
            ],
            "guid": "L1mBWDQ>#V",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "09_bayesian_learning"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Why are there two Gaussian Bayes rules?",
                "Gaussian Bayes Rule require a matrix inversion of different dimension. Depending on which inverse is cheaper to calculate, one should a use one Gaussian Bayes Rule over another."
            ],
            "guid": "C$D$8RgN`z",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "09_bayesian_learning"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Why is the lower bound called lower bound?",
                "Since \\(\\mathrm{KL}(q|| p) \\geq 0\\) it follow that \\(\\quad \\mathcal{L}(q, \\boldsymbol{\\theta}) \\leq \\log p(\\boldsymbol{x} \\mid \\boldsymbol{\\theta})\\)"
            ],
            "guid": "pI-S^/T%*.",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Why can we use separate udpate of the single components and coefficients in EM for Gaussian models?",
                "As we have just additive objectives in the lower bound."
            ],
            "guid": "b#J.hzoFo7",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Give examples for the latent variable models",
                "<div>\n<div><ul>\n<li>mixture models</li>\n<li>missing data</li>\n<li>latent factors</li>\n</ul>\n</div></div>"
            ],
            "guid": "n>EXVajcPu",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the difference between the mariginal distribution of a continous and a discrete latent variable?",
                "Let \\(\\boldsymbol{x}\\) be an observed variable and \\(\\boldsymbol{z}\\) be an unobserved variable.<div><br></div><div>\\(\\underbrace{p(\\boldsymbol{x} \\mid \\boldsymbol{\\theta})=\\sum_{z} p(\\boldsymbol{x}, z \\mid \\boldsymbol{\\theta})}_{\\text {discrete latent variable }}, \\quad \\underbrace{p(\\boldsymbol{x} \\mid \\boldsymbol{\\theta})=\\int_{\\boldsymbol{z}} p(\\boldsymbol{x}, \\boldsymbol{z} \\mid \\boldsymbol{\\theta}) d \\boldsymbol{z}}_{\\text {continuous latent variable }}\\)<br></div>"
            ],
            "guid": "pXLgirijA|",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What are drawbacks of EM?",
                "EM is very sensitive to initialization.<div><br></div><div><img src=\"paste-b25c84dba80be779a94954f124f451382d05c4fd.jpg\"><br></div>"
            ],
            "guid": "EFYR<J)XzD",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Why do we need sampling for continuous latent variables in the M-Step of the EM algorithm?",
                "<div>\n<div><ul>\n<li>Typically\n it is not feasable to compute the integral in the Maximization step \nwith continous latent variables, as no analytical solutions exist for \nthe integral.</li>\n<li>Instead a MC estimation is used to calculate the lower bound.</li>\n</ul>\n</div></div>"
            ],
            "guid": "I1)5(#iNol",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Name observations of the E-step in the EM algorithm. What happens to the marginal log-likelihood? What&nbsp; happens to the KL?",
                "The marignal log-likelihood&nbsp;\\(\\log p(\\boldsymbol{x} \\mid \\boldsymbol{\\theta})\\) is unaffected by the E-step<div>As KL is minimized th elower bound has to go up</div><div>After the E-step&nbsp;\\(\\operatorname{KL}(q(z) \\| p(z \\mid \\boldsymbol{x}))=0\\) and therefore, the lowerbound is tight i. e.:</div><div>\\(\\log p(\\boldsymbol{x} \\mid \\boldsymbol{\\theta})=\\mathcal{L}(q, \\boldsymbol{\\theta})\\)<br></div>"
            ],
            "guid": "lkXg.*98#)",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the advantages and disadvantages of the parzen window?",
                "Simple to compute (+)<div>Not very smooth (-)</div>"
            ],
            "guid": "zAXAoQk}rB",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the advantages / disadvantages of Gaussian kernels for density estimation?",
                "smooth (+)<div>infinite support (-)</div><div>computationally demanding (-)</div>"
            ],
            "guid": "pvCBg.nEq.",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "How does the <b>Gaussian kernel</b> work for <b>density estimation</b>?",
                "<b>Kernel function:</b><div>\\(g(\\boldsymbol{u})=\\exp \\left(-\\frac{\\|\\boldsymbol{u}\\|^{2}}{2 h}\\right)\\)<br></div><div><br></div><div><b>Volume:</b></div><div>\\(V=\\int g(\\boldsymbol{u}) d \\boldsymbol{u}=\\sqrt{2 \\pi h^{d}}\\)<br></div><div><br></div><div><b>Estimated density:</b></div><div>\\(\\begin{aligned} p\\left(\\boldsymbol{x}_{*}\\right) &amp; \\approx \\frac{1}{N V} \\sum_{i=1}^{N} g\\left(\\boldsymbol{x}_{*}-\\boldsymbol{x}_{i}\\right) \\\\ &amp;=\\frac{1}{N \\sqrt{2 \\pi h^{d}}} \\sum_{i=1}^{N} \\exp \\left(-\\frac{\\left\\|\\boldsymbol{x}_{*}-\\boldsymbol{x}_{i}\\right\\|^{2}}{2 h}\\right) \\end{aligned}\\)<br></div><div><br></div>"
            ],
            "guid": "NiKu+pS-ov",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain the general approach how <b>kernels </b>can be used for <b>density estimation</b>.",
                "<b>Volume:</b><div>\\(V=\\int g(\\boldsymbol{u}) d \\boldsymbol{u}\\)<br></div><div><br></div><div><b>Summed kernel activation:</b></div><div>\\(K\\left(\\boldsymbol{x}_{*}\\right)=\\sum_{i=1}^{N} g\\left(\\boldsymbol{x}_{*}-\\boldsymbol{x}_{i}\\right)\\)<br></div><div><br></div><div><b>Estimated density:</b></div><div>\\(p\\left(\\boldsymbol{x}_{*}\\right) \\approx \\frac{K\\left(\\boldsymbol{x}_{*}\\right)}{N V} \\equiv \\frac{1}{N V} \\sum_{i=1}^{N} g\\left(\\boldsymbol{x}_{*}-\\boldsymbol{x}_{i}\\right)\\)<br></div><div><br></div>"
            ],
            "guid": "wJL[miUTt]",
            "note_model_uuid": "14eb92dc-5f06-11ec-8e0f-cb9b2d7b92c1",
            "tags": [
                "06_em_for_dim_reduction"
            ]
        }
    ]
}