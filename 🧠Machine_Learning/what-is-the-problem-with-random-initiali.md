## Note
nid: 1629201722451
model: Basic-d7a3e
tags: 10_neural_networks
markdown: false

### Front
What is the problem with <b>random initialization</b> with deep
neural networks?

### Back
<div>All activations tend to be zero for deeper networks.</div><div>
</div><div>If activations are saturated gradients also tend to vanish.</div>
