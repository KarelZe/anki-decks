## Note
nid: 1629179814060
model: Basic-d7a3e
tags: 10_neural_networks, checklater
markdown: false

### Front
What are the advantages / disadvantages of the <b>ReLU activation
function</b>?

### Back
<div>
  <div>
    <div>
      <strong>Advantages</strong>
    </div>
    <ul>
      <li>Does not saturate (in +region)
      <li>Very computationally efficient
      <li>Converges much faster than sigmoid/tanh in practice e. g.
      6 times faster
      <li>Commonly used
    </ul>
    <div>
      <strong>Disadvantages</strong>
    </div>
    <ul>
      <li>Not zero-centred output
      <li>No gradient for \(x<0\)
    </ul>
  </div>
</div>
