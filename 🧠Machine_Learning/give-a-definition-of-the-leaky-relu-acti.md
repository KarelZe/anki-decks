## Note
nid: 1629179914636
model: Basic-d7a3e
tags: 10_neural_networks, checklater
markdown: false

### Front
Give a definition of the <b>Leaky ReLU </b>activation function.

### Back
\(f(x)=\max (0.1 x, x)\)
<div><img src=
"paste-6e883ccb5efda948f8beac055d1221ac08ef10f4.jpg"></div>
