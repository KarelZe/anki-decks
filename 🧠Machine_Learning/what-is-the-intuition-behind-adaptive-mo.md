## Note
nid: 1629371238030
model: Basic-d7a3e
tags: 10_neural_networks, checklater
markdown: false

### Front
What is the intuition behind <b>Adaptive Momentum</b> / <b>Adam</b>?

### Back
<div>
  Combine momentum term of stochastic gradient descent with
  momentum term with gradient normalization.
</div>
<div>
  Adam stores the decaying average of past gradients and also
  decaying average of past squared gradients.
</div>
