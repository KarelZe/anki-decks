## Note
nid: 1629180407245
model: Basic-d7a3e
tags: 10_neural_networks, checklater
markdown: false

### Front
Name advantages / disadvantages of the <b>ELU</b> activation
function

### Back
<div>
<div><div><strong>Advantage</strong></div>
<ul>
<li>All benefits of RELU</li>
<li>Closer to zero mean outputs, because of negative part</li>
<li>Negative saturation regime compared to Leaky ReLU, which means more robustness to noise</li>
</ul>
<div><strong>Disadvantage</strong></div>
<ul>
<li>Computation requires exp()</li>
</ul>
</div></div>
