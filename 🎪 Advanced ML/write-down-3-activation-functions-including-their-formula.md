# Note
```
guid: FlR4AA~>%h
notetype: Basic-d7a3e-4ce08
```

### Tags
```
adv_ml::10_lecture
```

## Front
Write down 3 <b>activation functions</b> including their formula.

## Back
<div>
  <div>
    <ul>
      <li><b>Sigmoid:</b> \(\sigma(z)=\frac{1}{1+\exp (-z)}\)
      <li><b>Tanh:</b> \(\tanh (z)=\frac{\exp (z)-\exp (-z)}{\exp
      (z)+\exp (-z)}\)
      <li><b>ReLU (Rectified Linear Unit):</b>
      \(\operatorname{ReLU}(z)=\max (0, z)\)
    </ul>
  </div>
</div>
<div><img src="paste-c71f6af1e42b08b1c8baeb599ce45d7c75036e25.jpg"></div>
