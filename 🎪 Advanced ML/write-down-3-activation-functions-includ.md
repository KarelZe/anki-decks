## Note
nid: 1620552749097
model: Basic-d7a3e-4ce08
tags: adv_ml::10_lecture
markdown: false

### Front
Write down 3 <b>activation functions</b> including their formula.

### Back
<div>
  <div>
    <ul>
      <li><b>Sigmoid:</b> \(\sigma(z)=\frac{1}{1+\exp (-z)}\)
      <li><b>Tanh:</b> \(\tanh (z)=\frac{\exp (z)-\exp (-z)}{\exp
      (z)+\exp (-z)}\)
      <li><b>ReLU (Rectified Linear Unit):</b>
      \(\operatorname{ReLU}(z)=\max (0, z)\)
    </ul>
  </div>
</div>
<div><img src=
"paste-c71f6af1e42b08b1c8baeb599ce45d7c75036e25.jpg"></div>
