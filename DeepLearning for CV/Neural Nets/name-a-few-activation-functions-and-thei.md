## Note
nid: 1563890115069
model: Basic-66395
tags: 
markdown: false

### Front
Name a few activation functions and their properties.

### Back
Sigmoid: \(1/1+e^{(-x)}\) (Con: Vanishing Gradient, Not zero-centered) 
Tanh: (Pro: Zero-centered, Con: Vanishing Gradient)<div>ReLU: (Pro: Efficient, No vanishing Gradient, Con: Dying ReLU) 
Leaky Relu: Small negative slope instead of zero for x < 0. </div><div>many more based on ReLU </div>
