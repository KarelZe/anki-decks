# Note
```
guid: i(D@k4eD8:
notetype: Basic-d7a3e-4ce08
```

### Tags
```
ml::10_neural_networks
```

## Front
Why are mini batches commonly used for gradient descent?

## Back
<div>
  <div>
    <ul>
      <li>Mini batch is an intermediate version of stochastic and
      batch gradient descent
      <li>Gives less noisy estimates than stochastic gradient
      descent
      <li>More efficient than batch gradient descent
      <li>Preferable for GPU implementations
    </ul>
  </div>
</div>
