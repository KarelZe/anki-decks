## Note
nid: 1629179682686
model: Basic-d7a3e-4ce08
tags: ml::10_neural_networks
markdown: false

### Front
Give a definition of the <b>ReLU activation</b> function.

### Back
\(f(x)=\max (0, x)\)
<div><img src=
"paste-52f6048ff6aa60354ea06922a0ea519729e66bbd.jpg"></div>
