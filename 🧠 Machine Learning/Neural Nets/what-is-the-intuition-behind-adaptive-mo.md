## Note
nid: 1629371238030
model: Basic-d7a3e-4ce08
tags: checklater, ml::10_neural_networks
markdown: false

### Front
What is the intuition behind <b>Adaptive Momentum</b> /
<b>Adam</b>?

### Back
<div>
  Combine momentum term of stochastic gradient descent with
  momentum term with gradient normalization.
</div>
<div>
  Adam stores the decaying average of past gradients and also
  decaying average of past squared gradients.
</div>
