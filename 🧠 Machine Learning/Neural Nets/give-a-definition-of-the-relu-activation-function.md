# Note
```
guid: C8Qh+$,L<)
notetype: Basic-d7a3e-4ce08
```

### Tags
```
ml::10_neural_networks
```

## Front
Give a definition of the <b>ReLU activation</b> function.

## Back
\(f(x)=\max (0, x)\)
<div><img src="paste-52f6048ff6aa60354ea06922a0ea519729e66bbd.jpg"></div>
