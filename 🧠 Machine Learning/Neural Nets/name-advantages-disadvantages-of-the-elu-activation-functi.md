# Note
```
guid: tAvl</wQk?
notetype: Basic-d7a3e-4ce08
```

### Tags
```
checklater
ml::10_neural_networks
```

## Front
Name advantages / disadvantages of the <b>ELU</b> activation
function

## Back
<div>
  <div>
    <div>
      <strong>Advantage</strong>
    </div>
    <ul>
      <li>All benefits of RELU
      <li>Closer to zero mean outputs, because of negative part
      <li>Negative saturation regime compared to Leaky ReLU, which
      means more robustness to noise
    </ul>
    <div>
      <strong>Disadvantage</strong>
    </div>
    <ul>
      <li>Computation requires exp()
    </ul>
  </div>
</div>
