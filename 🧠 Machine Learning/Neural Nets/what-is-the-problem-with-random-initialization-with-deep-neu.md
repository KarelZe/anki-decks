# Note
```
guid: j@@k)FkVj5
notetype: Basic-d7a3e-4ce08
```

### Tags
```
ml::10_neural_networks
```

## Front
What is the problem with <b>random initialization</b> with deep
neural networks?

## Back
<div>
  All activations tend to be zero for deeper networks.
</div>
<div>
  If activations are saturated gradients also tend to vanish.
</div>
