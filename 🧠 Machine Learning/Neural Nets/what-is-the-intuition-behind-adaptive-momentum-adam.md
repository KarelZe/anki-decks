# Note
```
guid: nX+<ILsmX2
notetype: Basic-d7a3e-4ce08
```

### Tags
```
checklater
ml::10_neural_networks
```

## Front
What is the intuition behind <b>Adaptive Momentum</b> /
<b>Adam</b>?

## Back
<div>
  Combine momentum term of stochastic gradient descent with
  momentum term with gradient normalization.
</div>
<div>
  Adam stores the decaying average of past gradients and also
  decaying average of past squared gradients.
</div>
