# Note
```
guid: dQLwJsN|xN
notetype: Basic-02d89-e0e22
```

### Tags
```
ml::12_var_ae
```

## Front
Why autoencoders?

## Back
<ul>
  <li>Map high-dimensional data to two dimensions for
  visualization. Compression (i.e. reducing the file size; Requires
  VAE).
  <li>Learn abstract features in an unsupervised way so you can
  apply them to a supervised task.
  <li>Unlabled data can be much more plentiful than labeled data.
  <li>Learn a semantically meaningful representation where you can,
  e.g., interpolate between different images.
</ul>
