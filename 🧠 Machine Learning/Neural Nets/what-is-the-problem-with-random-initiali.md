## Note
nid: 1629201722451
model: Basic-d7a3e-4ce08
tags: ml::10_neural_networks
markdown: false

### Front
What is the problem with <b>random initialization</b> with deep
neural networks?

### Back
<div>
  All activations tend to be zero for deeper networks.
</div>
<div>
  If activations are saturated gradients also tend to vanish.
</div>
