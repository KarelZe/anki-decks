## Note
nid: 1629179914636
model: Basic-d7a3e-4ce08
tags: checklater, ml::10_neural_networks
markdown: false

### Front
Give a definition of the <b>Leaky ReLU</b> activation function.

### Back
\(f(x)=\max (0.1 x, x)\)
<div><img src=
"paste-6e883ccb5efda948f8beac055d1221ac08ef10f4.jpg"></div>
