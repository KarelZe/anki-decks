## Note
nid: 1629195827467
model: Basic-d7a3e-4ce08
tags: ml::10_neural_networks
markdown: false

### Front
Why are mini batches commonly used for gradient descent?

### Back
<div>
  <div>
    <ul>
      <li>Mini batch is an intermediate version of stochastic and
      batch gradient descent
      <li>Gives less noisy estimates than stochastic gradient
      descent
      <li>More efficient than batch gradient descent
      <li>Preferable for GPU implementations
    </ul>
  </div>
</div>
