## Note
nid: 1653639371313
model: Basic-02d89
tags: 05_neural_networks_bda
markdown: false

### Front
Define and sketch the <b>ReLU activation function</b>.

### Back
ReLU \(\quad f(z)= \begin{cases}0, & z<0 \\ z, & z \geq 0\end{cases}\)

<img src="paste-f1fd8fcfc190b01eaafee8b02ef85a3673d331b4.jpg">
