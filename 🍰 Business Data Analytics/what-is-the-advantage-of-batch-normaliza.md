## Note
nid: 1653203202187
model: Basic-02d89
tags: 06_trainining_tuning_bda
markdown: false

### Front
What is the <b>advantage </b>of <b>batch normalization</b>?

### Back
<ul><li>Reduces sensitivity to intialization</li><li>Acts as regularizer, reducing need for dropout</li><li>No need to scale initial data if first layer is batch normalizing</li></ul>
