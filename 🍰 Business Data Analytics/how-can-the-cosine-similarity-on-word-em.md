## Note
nid: 1651235843337
model: Basic-02d89
tags: 10_text_mining_bda
markdown: false

### Front
How can the <b>cosine similarity</b> on <b>word embeddings</b> be
interpreted?

### Back
<b>Similar words:</b> \(\cos (\theta) \approx 1\)
<b>Not very similar words: </b>\(\cos (\theta) \approx 0\)
<b>Two vectors, that are similar but opposite:</b> e. g., France-Paris vs. Rome-Italy \(\cos (\theta) \approx-1\)
