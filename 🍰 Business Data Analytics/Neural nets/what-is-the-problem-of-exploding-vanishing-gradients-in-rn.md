# Note
```
guid: MaX}rF0.3g
notetype: Basic-02d89-e0e22
```

### Tags
```
bda::06_trainining_tuning
```

## Front
What is the <b>problem</b> of <b>exploding / vanishing
gradients</b> in <b>RNNs</b>?

## Back
Gradients become extremly large or close to 0, so that learning is no longer possible.

Evident for (very) deep Neural Networks and RNNs.
