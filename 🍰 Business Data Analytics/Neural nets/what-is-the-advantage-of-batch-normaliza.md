## Note
nid: 1653203202187
model: Basic-02d89-e0e22
tags: bda::06_trainining_tuning
markdown: false

### Front
What is the <b>advantage</b> of <b>batch normalization</b>?

### Back
<ul>
  <li>Reduces sensitivity to intialization
  <li>Acts as regularizer, reducing need for dropout
  <li>No need to scale initial data if first layer is batch
  normalizing
</ul>
