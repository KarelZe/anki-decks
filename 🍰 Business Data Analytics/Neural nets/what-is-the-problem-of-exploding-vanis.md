## Note
nid: 1652348712806
model: Basic-02d89-e0e22
tags: bda::06_trainining_tuning
markdown: false

### Front
What is the <b>problem</b> of <b>exploding / vanishing
gradients</b> in <b>RNNs</b>?

### Back
Gradients become extremly large or close to 0, so that learning is no longer possible.

Evident for (very) deep Neural Networks and RNNs.
