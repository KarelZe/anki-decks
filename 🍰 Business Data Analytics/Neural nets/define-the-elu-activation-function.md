# Note
```
guid: pO_+(-j]?4
notetype: Basic-02d89-e0e22
```

### Tags
```
bda::05_neural_networks
```

## Front
Define the <b>ELU activation function</b>.

## Back
\[E L U_{\alpha}(z)=\left\{\begin{array}{c}\alpha(\exp (z)-1),
\quad z<0 \\ z, \quad z \geq 0\end{array}\right.\] Speeds up
gradient as no "bounce" happens at \(z=0\). <b>Visualization:</b>
<img src="paste-5be499d996a020494876216d173be3522c47b6b3.jpg">
