# Note
```
guid: :R8:Egcyd
notetype: Basic-02d89-e0e22
```

### Tags
```
bda::05_neural_networks
```

## Front
Define and sketch the <b>ReLU activation function</b>.

## Back
ReLU \(\quad f(z)= \begin{cases}0, & z<0 \\ z, & z \geq
0\end{cases}\) <img src="paste-f1fd8fcfc190b01eaafee8b02ef85a3673d331b4.jpg">
