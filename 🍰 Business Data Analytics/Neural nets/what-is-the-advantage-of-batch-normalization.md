# Note
```
guid: q;*J-8rj{l
notetype: Basic-02d89-e0e22
```

### Tags
```
bda::06_trainining_tuning
```

## Front
What is the <b>advantage</b> of <b>batch normalization</b>?

## Back
<ul>
  <li>Reduces sensitivity to intialization
  <li>Acts as regularizer, reducing need for dropout
  <li>No need to scale initial data if first layer is batch
  normalizing
</ul>
