## Note
nid: 1652348712806
model: Basic-02d89
tags: 06_trainining_tuning_bda
markdown: false

### Front
What is the <b>problem</b> of <b>exploding / vanishing
gradients</b> in <b>RNNs</b>?

### Back
Gradients become extremly large or close to 0, so that learning is no longer possible.

Evident for (very) deep Neural Networks and RNNs.
