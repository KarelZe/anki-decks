## Note
nid: 1652344509907
model: Basic-02d89
tags: 05_neural_networks_bda
markdown: false

### Front
Define the <b>ELU activation function</b>.

### Back
\[E L U_{\alpha}(z)=\left\{\begin{array}{c}\alpha(\exp (z)-1),
\quad z<0 \\ z, \quad z \geq 0\end{array}\right.\] Speeds up
gradient as no "bounce" happens at \(z=0\). <b>Visualization:</b>
<img src="paste-5be499d996a020494876216d173be3522c47b6b3.jpg">
