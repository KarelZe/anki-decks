# Note
```
guid: qcnm.qI|zu
notetype: Basic-02d89-e0e22
```

### Tags
```
bda::10_text_mining
```

## Front
How can the <b>cosine similarity</b> on <b>word embeddings</b> be
interpreted?

## Back
<b>Similar words:</b> \(\cos (\theta) \approx 1\) <b>Not very
similar words:</b> \(\cos (\theta) \approx 0\) <b>Two vectors, that
are similar but opposite:</b> e. g., France-Paris vs. Rome-Italy
\(\cos (\theta) \approx-1\)
