## Note
nid: 1656320906484
model: Basic-02d89
tags: 
markdown: false

### Front
How does Sequence-toSequence Video-to-Text work? What is novel about it?

### Back
The S2VT approach performs video description using
a sequence to sequence model. 

A stacked LSTM first encodes the frames one by one, taking as input the output of a CNN applied to each input frameâ€™s intensity values. Once
all frames are read, the model generates a sentence word
by word. The encoding and decoding of the frame and
word representations are learned jointly from a parallel corpus. 

To model the temporal aspects of activities typically
shown in videos, we also compute the optical flow between pairs of consecutive frames. The flow images are also
passed through a CNN and provided as input to the LSTM.

<b>Visualization:</b>
<img src="paste-15070d4dd24f4f0b66d89b5678205bd9820aeacd.jpg">
