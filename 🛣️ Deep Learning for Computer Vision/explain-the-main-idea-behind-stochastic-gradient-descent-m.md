# Note
```
guid: n)FrM*J-M,
notetype: Basic-02d89-e0e22
```

### Tags
```
dl_cv::02_basics_nn
```

## Front
Explain the main idea behind <b>Stochastic Gradient Descent /
mini-batch Gradient Descent</b>.

## Back
Approximate sum of loss with a mini-batch of examples (e. g. 3,2
64,...) Samples are randomly drawn from the entire set. <b>Pseudo
code:</b> <img src="paste-975ceed90a5f1e60224410fb45a1b43bc5d69b51.jpg">
