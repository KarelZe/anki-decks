# Note
```
guid: xp1w&fu]OD
notetype: Basic-02d89-e0e22
```

### Tags
```
dl_cv::02_basics_nn
```

## Front
Why is it necessary to use <b>non-linear activation functions</b>?

## Back
A composition of linear functions is still linear. A single-layer
perceptron cannont implement simple functions such as NOT or XOR.
(see example) Combining layers let us represent <b>non-linear
activation functions</b>. <b>Example:</b> <img src="paste-77a7a35ba27ffceadf57da561df83fe332cbbeb7.jpg">
