## Note
nid: 1651481199253
model: Basic-02d89
tags: 02_basics_nn_dlcv
markdown: false

### Front
What activation function should one use?

### Back
<ul>
  <li>Try ReLU
  <li>If dying gradients are a problem, try leaky ReLU or Maxout
  <li>Otherwise Tanh
</ul>
