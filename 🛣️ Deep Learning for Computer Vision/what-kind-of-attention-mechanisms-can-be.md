## Note
nid: 1656321935524
model: Basic-02d89
tags: 
markdown: false

### Front
What kind of Attention mechanisms can be differentiated? Explain them.

### Back
"Soft" attention -> weighted combination of softmax over \(L\)
locations "Hard" attention -> picks only the highest scoring
location (not very nice to train end-to-end) <b>Visualization:</b>
<img src="soft-vs-hard-attention.png">
