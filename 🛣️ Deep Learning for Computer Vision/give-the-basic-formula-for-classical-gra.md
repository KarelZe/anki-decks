## Note
nid: 1651473740511
model: Basic-02d89-e0e22
tags: dl_cv::02_basics_nn
markdown: false

### Front
Give the <b>basic formula</b> for classical <b>Gradient
Descen</b>t.

### Back
Modify the network weight \(\boldsymbol{\theta}\) based on gradient of the loss function:

\(\boldsymbol{\theta} \leftarrow \boldsymbol{\theta}-\eta \frac{\partial L}{\partial \boldsymbol{\theta}}\)

That is, increase or decrease weights if needed. Do nothing for opitmal weights.
