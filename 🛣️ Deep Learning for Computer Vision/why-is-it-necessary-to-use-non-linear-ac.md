## Note
nid: 1651470896072
model: Basic-02d89-e0e22
tags: dl_cv::02_basics_nn
markdown: false

### Front
Why is it necessary to use <b>non-linear activation functions</b>?

### Back
A composition of linear functions is still linear. A single-layer
perceptron cannont implement simple functions such as NOT or XOR.
(see example) Combining layers let us represent <b>non-linear
activation functions</b>. <b>Example:</b> <img src= 
"paste-77a7a35ba27ffceadf57da561df83fe332cbbeb7.jpg">
