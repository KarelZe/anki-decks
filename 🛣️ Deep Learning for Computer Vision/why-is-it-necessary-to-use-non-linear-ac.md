## Note
nid: 1651470896072
model: Basic-02d89
tags: 02_basics_nn_dlcv
markdown: false

### Front
Why is it necessary to use <b>non-linear activation functions</b>?

### Back
A composition of linear functions is still linear.

A single-layer perceptron cannont implement simple functions such as NOT or XOR. (see example)

Combining layers let us represent <b>non-linear activation functions</b>.

<b>Example:</b>
<img src="paste-77a7a35ba27ffceadf57da561df83fe332cbbeb7.jpg">
