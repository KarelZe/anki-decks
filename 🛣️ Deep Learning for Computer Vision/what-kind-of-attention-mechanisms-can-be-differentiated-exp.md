# Note
```
guid: KQM-*{!y;C
notetype: Basic-02d89-e0e22
```

### Tags
```
dl_cv::misc
```

## Front
What kind of Attention mechanisms can be differentiated? Explain them.

## Back
"Soft" attention -> weighted combination of softmax over \(L\)
locations "Hard" attention -> picks only the highest scoring
location (not very nice to train end-to-end) <b>Visualization:</b>
<img src="soft-vs-hard-attention.png">
