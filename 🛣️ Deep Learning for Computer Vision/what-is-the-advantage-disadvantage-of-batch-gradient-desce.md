# Note
```
guid: I>1Mg1KYYQ
notetype: Basic-02d89-e0e22
```

### Tags
```
dl_cv::02_basics_nn
```

## Front
What is the <b>advantage</b> / <b>disadvantage</b> of <b>Batch
Gradient Descent?</b>

## Back
<b>Pro:</b> Batch Gradient is guaranteed to converge to the global
minimum for a <b>convex error surfaces</b> and to a local minimum
for <b>non-convex surfaces</b>. <b>Con:</b> Full sum is expensive
when \(N\) the number of examples is large. Hard to hold in memory.
