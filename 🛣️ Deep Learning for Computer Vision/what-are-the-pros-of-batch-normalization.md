## Note
nid: 1652108968615
model: Basic-02d89
tags: 03_nn_basics_dlcv
markdown: false

### Front
What are the pros of <b>batch normalization</b>?

### Back
<ul>
  <li>Enables higher learning rates (thus faster training)
  <li>Less sensitive to weight initialization
  <li>Additional regularization, reduces overfitting
</ul>
