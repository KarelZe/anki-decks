## Note
nid: 1652108968615
model: Basic-02d89-e0e22
tags: dl_cv::03_nn_basics
markdown: false

### Front
What are the pros of <b>batch normalization</b>?

### Back
<ul>
  <li>Enables higher learning rates (thus faster training)
  <li>Less sensitive to weight initialization
  <li>Additional regularization, reduces overfitting
</ul>
