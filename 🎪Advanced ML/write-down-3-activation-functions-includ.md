## Note
nid: 1620552749097
model: Basic-b122e
tags: 10_lecture
markdown: false

### Front
Write down 3 <b>activation functions</b> including their formula.

### Back
<div>
<div><ul>
<li><b>Sigmoid:</b> \(\sigma(z)=\frac{1}{1+\exp (-z)}\)</li>
<li><b>Tanh:</b> \(\tanh (z)=\frac{\exp (z)-\exp (-z)}{\exp (z)+\exp (-z)}\)</li>
<li><b>ReLU (Rectified Linear Unit):</b> \(\operatorname{ReLU}(z)=\max (0, z)\)</li>
</ul>
</div></div><div>
</div><div><img src="paste-c71f6af1e42b08b1c8baeb599ce45d7c75036e25.jpg">
</div>
